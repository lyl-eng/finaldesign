"""
ç¿»è¯‘ä¸ä¼˜åŒ–Agent (Agent 2)
è´Ÿè´£ç¿»è¯‘ç”Ÿæˆä¸è¿­ä»£ä¼˜åŒ–
"""

import re
import json
import copy
from typing import Dict, Any, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from .BaseAgent import BaseAgent
from ModuleFolders.LLMRequester.LLMRequester import LLMRequester
from ModuleFolders.Cache.CacheProject import CacheProject
from ModuleFolders.Cache.CacheItem import CacheItem, TranslationStatus
from ModuleFolders.RequestLimiter.RequestLimiter import RequestLimiter
from ModuleFolders.ResponseExtractor.ResponseExtractor import ResponseExtractor


class TranslationRefinementAgent(BaseAgent):
    """
    Agent 2: ç¿»è¯‘ä¸ä¼˜åŒ–Agent
    åŠŸèƒ½ï¼š
    1. å¤šæ­¥éª¤å¼•å¯¼ç¿»è¯‘ï¼ˆç†è§£â€”åˆ†è§£â€”è½¬æ¢â€”æ¶¦è‰²ï¼‰
    2. å¤šç‰ˆæœ¬ç”Ÿæˆä¸èåˆï¼ˆç›´è¯‘ç‰ˆã€æ„è¯‘ç‰ˆã€é£æ ¼åŒ–ç‰ˆï¼‰
    3. å›è¯‘éªŒè¯ä¸è‡ªæˆ‘ä¿®æ­£ï¼ˆTEaRæ¡†æ¶ï¼‰
    """
    
    def __init__(self, config=None):
        super().__init__(
            name="TranslationRefinementAgent",
            description="ç¿»è¯‘ç”Ÿæˆä¸è¿­ä»£ä¼˜åŒ–Agent",
            config=config
        )
        
        self.llm_requester = LLMRequester()
        self.translation_versions = {}  # å­˜å‚¨å¤šç‰ˆæœ¬ç¿»è¯‘
        self.request_limiter = RequestLimiter()  # æ·»åŠ è¯·æ±‚é™åˆ¶å™¨
        self._current_cache_project = None  # å½“å‰å¤„ç†çš„cache_project
        
        # åˆå§‹åŒ– DB Manager
        try:
            from ModuleFolders.Cache.DatabaseManager import DatabaseManager
            self.db_manager = DatabaseManager()
        except ImportError:
            self.db_manager = None

        # é…ç½®è¯·æ±‚é™åˆ¶å™¨
        if self.config:
            rpm_limit = getattr(self.config, 'rpm_limit', 60)
            tpm_limit = getattr(self.config, 'tpm_limit', 10000)
            self.request_limiter.set_limit(tpm_limit, rpm_limit)
    
    def _get_atom_id(self, cache_project, file_path, row_index) -> Optional[int]:
        """[DB] è·å–ç¼“å­˜é¡¹å¯¹åº”çš„æ•°æ®åº“åŸå­ID"""
        if hasattr(cache_project, 'db_atom_map'):
            file_map = cache_project.db_atom_map.get(file_path)
            if file_map:
                return file_map.get(row_index)
        return None
    
    def _update_stage_progress(self, cache_project: CacheProject, stage: str, current: int, total: int):
        """æ›´æ–°å½“å‰é˜¶æ®µçš„è¿›åº¦ä¿¡æ¯ï¼ˆç”¨äºé¢„ä¼°æ—¶é—´ï¼‰"""
        import time
        
        if not cache_project.stats_data:
            return
        
        with cache_project.stats_data.atomic_scope():
            # å¦‚æœæ˜¯æ–°é˜¶æ®µï¼Œé‡ç½®é˜¶æ®µå¼€å§‹æ—¶é—´
            if cache_project.stats_data.current_stage != stage:
                cache_project.stats_data.current_stage = stage
                cache_project.stats_data.stage_start_time = time.time()
                self.debug(f"[TranslationAgent] è¿›å…¥æ–°é˜¶æ®µ: {stage}, æ€»è¿›åº¦={total}")
            
            # æ›´æ–°è¿›åº¦
            cache_project.stats_data.stage_progress_current = current
            cache_project.stats_data.stage_progress_total = total
    
    def _publish_stage_with_stats(self, cache_project: CacheProject, stage: str, batch_info: str):
        """å‘é€åŒ…å«ç»Ÿè®¡æ•°æ®çš„é˜¶æ®µæ›´æ–°ï¼ˆä¸WorkflowManagerä¿æŒä¸€è‡´ï¼‰"""
        from Base.Base import Base
        import time
        
        # ğŸ”¥ ä½¿ç”¨atomic_scopeç¡®ä¿è¯»å–æœ€æ–°çš„ç»Ÿè®¡æ•°æ®
        if cache_project.stats_data:
            with cache_project.stats_data.atomic_scope():
                # ğŸ”¥ æ›´æ–°å·²æ¶ˆè€—æ—¶é—´ï¼ˆç¡®ä¿é˜¶æ®µæ›´æ–°æ—¶ä¹ŸåŒæ­¥æ—¶é—´ï¼‰
                cache_project.stats_data.time = time.time() - cache_project.stats_data.start_time
            update_data = cache_project.stats_data.to_dict()
        else:
            update_data = {}
        
        # æ·»åŠ é˜¶æ®µä¿¡æ¯
        update_data["agent_stage"] = {
            "stage": stage,
            "batch_info": batch_info
        }
        
        start_time_val = update_data.get('start_time', 0)
        time_val = update_data.get('time', 0)
        self.debug(f"[TranslationAgent] å‘é€é˜¶æ®µæ›´æ–°: stage={stage}, batch_info={batch_info}, line={update_data.get('line', 0)}/{update_data.get('total_line', 0)}, time={time_val:.1f}s, start_time={start_time_val:.0f}, active_llm={update_data.get('active_llm_calls', 0)}")
        self.emit(Base.EVENT.TASK_UPDATE, update_data)
    
    def _update_token_stats(self, prompt_tokens: int, completion_tokens: int):
        """æ›´æ–°tokenç»Ÿè®¡å¹¶å‘é€UIæ›´æ–°äº‹ä»¶ï¼ˆä¸åŸTaskExecutorä¿æŒä¸€è‡´ï¼‰"""
        if not self._current_cache_project or not self._current_cache_project.stats_data:
            return
        
        from Base.Base import Base
        import time
        
        # ğŸ”¥ ä½¿ç”¨atomic_scopeç¡®ä¿çº¿ç¨‹å®‰å…¨
        with self._current_cache_project.stats_data.atomic_scope():
            # ğŸ”¥ æ›´æ–°æ€»tokenæ•°ï¼ˆprompt + completionï¼‰
            if prompt_tokens or completion_tokens:
                self._current_cache_project.stats_data.token += (prompt_tokens or 0) + (completion_tokens or 0)
            
            # ğŸ”¥ æ›´æ–°completion_tokensï¼ˆç”¨äºæˆæœ¬è®¡ç®—ï¼‰
            if completion_tokens:
                self._current_cache_project.stats_data.total_completion_tokens += completion_tokens
            
            # æ›´æ–°è¯·æ±‚è®¡æ•°
            self._current_cache_project.stats_data.total_requests += 1
            
            # ğŸ”¥ æ›´æ–°å·²æ¶ˆè€—æ—¶é—´ï¼ˆä¸åŸTaskExecutorä¿æŒä¸€è‡´ï¼‰
            self._current_cache_project.stats_data.time = time.time() - self._current_cache_project.stats_data.start_time
            
            # ğŸ”¥ ç«‹å³å‘é€UIæ›´æ–°äº‹ä»¶ï¼Œç¡®ä¿tokenç»Ÿè®¡å®æ—¶æ›´æ–°
            stats_dict = self._current_cache_project.stats_data.to_dict()
        
        # åœ¨atomic_scopeå¤–å‘é€äº‹ä»¶
        self.emit(Base.EVENT.TASK_UPDATE, stats_dict)
    
    def _update_line_stats(self, row_count: int):
        """æ›´æ–°å·²ç¿»è¯‘è¡Œæ•°ç»Ÿè®¡å¹¶å‘é€UIæ›´æ–°äº‹ä»¶ï¼ˆä¸åŸTaskExecutorä¿æŒä¸€è‡´ï¼‰"""
        if not self._current_cache_project or not self._current_cache_project.stats_data:
            return
        
        from Base.Base import Base
        import time
        
        # ğŸ”¥ ä½¿ç”¨atomic_scopeç¡®ä¿çº¿ç¨‹å®‰å…¨
        with self._current_cache_project.stats_data.atomic_scope():
            # ğŸ”¥ æ›´æ–°å·²ç¿»è¯‘è¡Œæ•°
            self._current_cache_project.stats_data.line += row_count
            
            # ğŸ”¥ æ›´æ–°å·²æ¶ˆè€—æ—¶é—´ï¼ˆä¸åŸTaskExecutorä¿æŒä¸€è‡´ï¼‰
            self._current_cache_project.stats_data.time = time.time() - self._current_cache_project.stats_data.start_time
            
            # ğŸ”¥ ç«‹å³å‘é€UIæ›´æ–°äº‹ä»¶ï¼Œç¡®ä¿è¡Œæ•°ç»Ÿè®¡å®æ—¶æ›´æ–°
            stats_dict = self._current_cache_project.stats_data.to_dict()
        
        # åœ¨atomic_scopeå¤–å‘é€äº‹ä»¶
        # æ¯5æ¬¡æ›´æ–°æ‰“å°ä¸€æ¬¡æ—¥å¿—ï¼Œé¿å…åˆ·å±
        if not hasattr(self, '_line_update_count'):
            self._line_update_count = 0
        self._line_update_count += 1
        if self._line_update_count % 5 == 1:
            self.debug(f"[TranslationAgent] æ›´æ–°è¡Œæ•°ç»Ÿè®¡: +{row_count}, total={stats_dict.get('line', 0)}/{stats_dict.get('total_line', 0)}, time={stats_dict.get('time', 0):.1f}s, active_llm={stats_dict.get('active_llm_calls', 0)}")
        self.emit(Base.EVENT.TASK_UPDATE, stats_dict)
    
    def _llm_request_with_tracking(self, messages, system_prompt, platform_config):
        """
        åŒ…è£…LLMè¯·æ±‚ï¼Œè‡ªåŠ¨è¿½è¸ªæ´»è·ƒè°ƒç”¨æ•°
        
        Returns:
            (skip, response_think, response_content, prompt_tokens, completion_tokens)
        """
        if not self._current_cache_project or not self._current_cache_project.stats_data:
            return self.llm_requester.sent_request(messages, system_prompt, platform_config)
        
        from Base.Base import Base
        
        try:
            # ğŸ”¥ è°ƒç”¨å‰ï¼šå¢åŠ æ´»è·ƒLLMè°ƒç”¨è®¡æ•°å¹¶ç«‹å³å‘é€äº‹ä»¶
            with self._current_cache_project.stats_data.atomic_scope():
                self._current_cache_project.stats_data.active_llm_calls += 1
                stats_dict = self._current_cache_project.stats_data.to_dict()
            self.emit(Base.EVENT.TASK_UPDATE, stats_dict)
            
            # æ‰§è¡ŒLLMè¯·æ±‚
            result = self.llm_requester.sent_request(messages, system_prompt, platform_config)
            
            return result
        finally:
            # ğŸ”¥ è°ƒç”¨åï¼šå‡å°‘æ´»è·ƒLLMè°ƒç”¨è®¡æ•°å¹¶ç«‹å³å‘é€äº‹ä»¶
            with self._current_cache_project.stats_data.atomic_scope():
                self._current_cache_project.stats_data.active_llm_calls = max(0, self._current_cache_project.stats_data.active_llm_calls - 1)
                stats_dict = self._current_cache_project.stats_data.to_dict()
            self.emit(Base.EVENT.TASK_UPDATE, stats_dict)
    
    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œç¿»è¯‘å’Œä¼˜åŒ–ä»»åŠ¡
        
        Args:
            input_data: åŒ…å«cache_projectã€terminology_databaseç­‰çš„å­—å…¸
            
        Returns:
            åŒ…å«ç¿»è¯‘ç»“æœå’Œä¼˜åŒ–ä¿¡æ¯çš„å­—å…¸
        """
        self.log_agent_action("å¼€å§‹æ‰§è¡Œç¿»è¯‘ä¸ä¼˜åŒ–")
        
        cache_project: CacheProject = input_data.get("cache_project")
        terminology_db = input_data.get("terminology_database", {})
        memory_storage = input_data.get("memory_storage", {})
        progress_callback = input_data.get("progress_callback")  # è·å–è¿›åº¦å›è°ƒ
        planning_result = input_data.get("planning_result", {})  # è·å–è§„åˆ’ç»“æœ
        task_memory = input_data.get("task_memory", {})  # è·å–ä»»åŠ¡å…ƒæ•°æ®ï¼ˆåŒ…å«chunkç­–ç•¥å’Œå®ä½“æ•°æ®åº“ï¼‰
        human_intervention_callback = input_data.get("human_intervention_callback", None)  # ğŸ”¥ è·å–äººå·¥ä»‹å…¥å›è°ƒ
        
        # ğŸ”¥ ä¿å­˜cache_projectå¼•ç”¨ï¼Œç”¨äºtokenç»Ÿè®¡
        self._current_cache_project = cache_project
        
        # ğŸ”¥ ä¿å­˜human_intervention_callbackå¼•ç”¨ï¼Œç”¨äºäººå·¥å®¡æ ¸
        self._human_intervention_callback = human_intervention_callback
        
        if not cache_project:
            self.error("æœªæ‰¾åˆ°cache_projectæ•°æ®")
            return {"success": False, "error": "ç¼ºå°‘cache_project"}
        
        # ğŸ”¥ ä½¿ç”¨ä¸åŸTaskExecutorç›¸åŒçš„æ‰¹é‡ç¿»è¯‘ç­–ç•¥
        # ç”Ÿæˆchunksï¼ˆæ¯ä¸ªchunkåŒ…å«å¤šè¡Œæ–‡æœ¬ï¼Œè€Œä¸æ˜¯å•è¡Œï¼‰
        translation_chunks, context_chunks, file_paths = self._prepare_translation_chunks(cache_project)
        
        total_chunks = len(translation_chunks)
        
        # ç»Ÿè®¡æ€»æ–‡æœ¬å•å…ƒæ•°ï¼ˆç”¨äºè¿›åº¦æ˜¾ç¤ºï¼‰
        total_units = sum(len(chunk) for chunk in translation_chunks)
        self.info(f"æ‰¹é‡ç¿»è¯‘æ¨¡å¼ï¼š{total_units} ä¸ªæ–‡æœ¬å•å…ƒï¼Œåˆ†ä¸º {total_chunks} ä¸ªæ‰¹æ¬¡")
        
        # ğŸ”¥ ä½¿ç”¨ä¸åŸTaskExecutorç›¸åŒçš„å¹¶å‘æ•°è®¡ç®—ç­–ç•¥
        # ä¸éœ€è¦é™çº§ç³»æ•°ï¼Œå› ä¸ºæˆ‘ä»¬ç°åœ¨ä½¿ç”¨æ‰¹é‡ç¿»è¯‘ï¼ˆå¤šè¡Œåˆå¹¶ä¸ºä¸€ä¸ªchunkï¼‰
        if planning_result and "execution_plan" in planning_result:
            max_workers = planning_result["execution_plan"].get("max_workers", 10)
        elif self.config and hasattr(self.config, 'actual_thread_counts'):
            # ç›´æ¥ä½¿ç”¨é…ç½®çš„çº¿ç¨‹æ•°ï¼ˆä¸åŸTaskExecutorä¸€è‡´ï¼‰
            max_workers = self.config.actual_thread_counts
            self.info(f"ä½¿ç”¨é…ç½®çš„çº¿ç¨‹æ•°: {max_workers} (ä¸åŸç¿»è¯‘æ–¹æ³•ä¸€è‡´)")
        else:
            max_workers = 10  # é»˜è®¤å€¼
        
        # æ‰¹é‡ç¿»è¯‘æ¨¡å¼ä¸‹ï¼Œå¹¶å‘æ•°ç­‰äºchunkæ•°é‡
        max_workers = min(max_workers, total_units)  # ä¸è¶…è¿‡æ€»chunkæ•°
        
        self.info("=" * 60)
        self.info(f"å¼€å§‹æ‰¹é‡ç¿»è¯‘ï¼š{total_units} ä¸ªæ–‡æœ¬å•å…ƒï¼Œåˆ†ä¸º {total_chunks} ä¸ªæ‰¹æ¬¡")
        self.info(f"å¹¶å‘çº¿ç¨‹æ•°: {max_workers}")
        self.info("=" * 60)
        
        # ğŸ”¥ å‘é€UIé˜¶æ®µæ›´æ–°ï¼šå¼€å§‹æ‰¹é‡ç¿»è¯‘
        self._update_stage_progress(cache_project, "translating", 0, total_chunks)  # ä½¿ç”¨chunkæ•°é‡è€Œä¸æ˜¯unitæ•°é‡
        self._publish_stage_with_stats(cache_project, "translating", "ç¿»è¯‘ä¸­")
        
        # å‘é€åˆå§‹è¿›åº¦
        if progress_callback:
            progress_callback(0, total_units, "translation", "å¼€å§‹æ‰¹é‡ç¿»è¯‘")
        
        results = []
        completed_units = 0  # å·²å®Œæˆçš„æ–‡æœ¬å•å…ƒæ•°
        all_chunks_data = []  # ğŸ”¥ æ”¶é›†æ‰€æœ‰æ‰¹æ¬¡çš„æ•°æ®ï¼ˆç”¨äºç»Ÿä¸€äººå·¥å®¡æ ¸å’Œå®ä½“æ£€æŸ¥ï¼‰
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æ‰¹é‡ç¿»è¯‘ä»»åŠ¡
            future_to_chunk = {
                executor.submit(
                    self._translate_chunk,  # æ”¹ä¸ºæ‰¹é‡ç¿»è¯‘
                    chunk, 
                    context_chunk,
                    file_path,
                    idx, 
                    total_chunks, 
                    terminology_db, 
                    memory_storage,
                    task_memory,  # ä¼ é€’ä»»åŠ¡å…ƒæ•°æ®ï¼ˆåŒ…å«chunkç­–ç•¥ï¼‰
                    progress_callback,
                    completed_units  # ä¼ é€’å·²å®Œæˆæ•°ç”¨äºè¿›åº¦æ›´æ–°
                ): (chunk, idx) 
                for idx, (chunk, context_chunk, file_path) in enumerate(zip(translation_chunks, context_chunks, file_paths), 1)
            }
            
            # æŒ‰å®Œæˆé¡ºåºæ”¶é›†ç»“æœ
            completed_chunks = 0  # ğŸ”¥ è¿½è¸ªå·²å®Œæˆçš„chunkæ•°é‡
            for future in as_completed(future_to_chunk):
                chunk, chunk_idx = future_to_chunk[future]
                try:
                    result = future.result()
                    if result and result.get("success"):
                        results.extend(result.get("translated_items", []))
                        all_chunks_data.append(result)  # ğŸ”¥ ä¿å­˜æ‰¹æ¬¡æ•°æ®
                        chunk_size = len(chunk)
                        completed_units += chunk_size
                        completed_chunks += 1  # ğŸ”¥ å¢åŠ å·²å®Œæˆchunkæ•°
                        
                        # ğŸ”¥ æ›´æ–°é˜¶æ®µè¿›åº¦ï¼ˆåŸºäºchunkæ•°é‡ï¼‰
                        self._update_stage_progress(cache_project, "translating", completed_chunks, total_chunks)
                        
                        # æ›´æ–°è¿›åº¦
                        if progress_callback:
                            progress_callback(
                                completed_units, 
                                total_units, 
                                "translation", 
                                f"å·²ç¿»è¯‘ {completed_units}/{total_units} ä¸ªå•å…ƒ (æ‰¹æ¬¡ {completed_chunks}/{total_chunks})"
                            )
                except Exception as exc:
                    self.error(f"ç¿»è¯‘æ‰¹æ¬¡ {chunk_idx} å¤±è´¥: {exc}", exc)
        
        # ğŸ”¥ ========== ç»Ÿä¸€äººå·¥å®¡æ ¸ï¼ˆæ‰€æœ‰æ‰¹æ¬¡å®Œæˆåï¼‰ ==========
        self.info("\n" + "="*60)
        self.info("æ‰€æœ‰æ‰¹æ¬¡å›è¯‘éªŒè¯å®Œæˆï¼Œå¼€å§‹ç»Ÿä¸€äººå·¥å®¡æ ¸...")
        self.info("="*60)
        
        # è·å–å·¥ä½œæµé…ç½®
        workflow_config = planning_result.get("workflow_config", {})
        
        all_chunks_data = self._unified_human_review(all_chunks_data, terminology_db, task_memory, workflow_config)
        
        # ğŸ”¥ ========== ç»Ÿä¸€å®ä½“ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆäººå·¥å®¡æ ¸åï¼‰ ==========
        self.info("\n" + "="*60)
        self.info("å¼€å§‹ç»Ÿä¸€å®ä½“ä¸€è‡´æ€§æ£€æŸ¥...")
        self.info("="*60)
        entity_database = task_memory.get("entity_database", {})
        all_chunks_data = self._unified_entity_check(all_chunks_data, terminology_db, entity_database)
        
        # ğŸ”¥ ========== æ›´æ–°ç¼“å­˜ï¼ˆæ‰€æœ‰æ£€æŸ¥å®Œæˆåï¼‰ ==========
        for chunk_data in all_chunks_data:
            chunk_items = chunk_data.get("chunk_items", [])
            translated_texts = chunk_data.get("translated_texts", [])
            for item, translated_text in zip(chunk_items, translated_texts):
                if translated_text:
                    self._update_cache_item(item, translated_text)
        
        self.log_agent_action("ç¿»è¯‘ä¸ä¼˜åŒ–å®Œæˆ", f"æˆåŠŸç¿»è¯‘ {len(results)} ä¸ªå•å…ƒ")
        
        return {
            "success": True,
            "cache_project": cache_project,
            "translation_results": results
        }
    
    def _prepare_translation_chunks(self, cache_project: CacheProject):
        """
        å‡†å¤‡ç¿»è¯‘æ‰¹æ¬¡ï¼ˆchunksï¼‰- ä½¿ç”¨ä¸åŸTaskExecutorç›¸åŒçš„æ‰¹é‡ç­–ç•¥
        
        Returns:
            chunks: List[List[CacheItem]] - æ‰¹æ¬¡åˆ—è¡¨ï¼Œæ¯ä¸ªæ‰¹æ¬¡åŒ…å«å¤šä¸ªæ–‡æœ¬å•å…ƒ
            context_chunks: List[List[CacheItem]] - ä¸Šä¸‹æ–‡æ‰¹æ¬¡
            file_paths: List[str] - æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        from ModuleFolders.TaskConfig.TaskType import TaskType
        
        # è·å–æ‰¹é‡ç¿»è¯‘é…ç½®ï¼ˆä¸åŸTaskExecutorä¸€è‡´ï¼‰
        if self.config:
            limit_type = "token" if getattr(self.config, 'tokens_limit_switch', False) else "line"
            limit_count = getattr(self.config, 'tokens_limit', 500) if limit_type == "token" else getattr(self.config, 'lines_limit', 15)
            previous_line_count = getattr(self.config, 'pre_line_counts', 3)
        else:
            limit_type = "line"
            limit_count = 15  # é»˜è®¤æ¯æ‰¹15è¡Œ
            previous_line_count = 3
        
        chunks, context_chunks, file_paths = [], [], []
        
        for file_path, cache_file in cache_project.files.items():
            # ç­›é€‰æœªç¿»è¯‘çš„æ¡ç›®
            items = [item for item in cache_file.items if item.translation_status == TranslationStatus.UNTRANSLATED]
            
            if not items:
                continue
            
            current_chunk, current_length = [], 0
            chunk_start_idx = 0
            
            for i, item in enumerate(items):
                # è®¡ç®—itemé•¿åº¦ï¼ˆæŒ‰è¡Œæˆ–æŒ‰tokenï¼‰
                item_length = item.token_count if limit_type == "token" else 1
                source_text_length = len(item.source_text)
                
                # ğŸ”¥ ã€æ™ºèƒ½åˆ†å—ç­–ç•¥ã€‘
                # ç­–ç•¥ï¼šæŒ‰æ€»å­—ç¬¦æ•°åˆ†å—ï¼Œè€Œä¸æ˜¯å›ºå®šè¡Œæ•°
                # - æç«¯è¶…é•¿æ–‡æœ¬ï¼ˆ>6000å­—ç¬¦ï¼‰ï¼šå•ç‹¬æˆchunk
                # - æ™®é€šæ–‡æœ¬ï¼šç´¯è®¡ä¸è¶…è¿‡6000å­—ç¬¦/chunk
                MAX_CHUNK_CHARS = 6000
                is_extreme_long = source_text_length > MAX_CHUNK_CHARS
                
                # è®°å½•chunkèµ·å§‹ç´¢å¼•
                if not current_chunk:
                    chunk_start_idx = i
                    chunk_chars = 0  # è·Ÿè¸ªå½“å‰chunkçš„æ€»å­—ç¬¦æ•°
                
                # ğŸ”¥ æç«¯è¶…é•¿æ–‡æœ¬ï¼ˆ>6000å­—ç¬¦ï¼‰å•ç‹¬æˆchunk
                if is_extreme_long:
                    # å…ˆæäº¤å½“å‰chunkï¼ˆå¦‚æœæœ‰ï¼‰
                    if current_chunk:
                        chunks.append(current_chunk)
                        context_chunk = self._generate_context_chunk(items, previous_line_count, chunk_start_idx)
                        context_chunks.append(context_chunk)
                        file_paths.append(file_path)
                    
                    # æç«¯è¶…é•¿æ–‡æœ¬å•ç‹¬æˆchunk
                    chunks.append([item])
                    context_chunk = self._generate_context_chunk(items, previous_line_count, i)
                    context_chunks.append(context_chunk)
                    file_paths.append(file_path)
                    
                    self.debug(f"  âš¡ æç«¯è¶…é•¿æ–‡æœ¬ (ç¬¬{i+1}é¡¹, {source_text_length}å­—ç¬¦)ï¼Œå•ç‹¬æˆchunk")
                    
                    # é‡ç½®
                    current_chunk, current_length, chunk_chars = [], 0, 0
                    chunk_start_idx = -1
                    continue
                
                # ğŸ”¥ æ™ºèƒ½æ‰“åŒ…ï¼šæŒ‰æ€»å­—ç¬¦æ•°é™åˆ¶
                # å¦‚æœåŠ å…¥å½“å‰itemä¼šè¶…è¿‡MAX_CHUNK_CHARSï¼Œå…ˆæäº¤å½“å‰chunk
                if current_chunk and (chunk_chars + source_text_length > MAX_CHUNK_CHARS):
                    chunks.append(current_chunk)
                    context_chunk = self._generate_context_chunk(items, previous_line_count, chunk_start_idx)
                    context_chunks.append(context_chunk)
                    file_paths.append(file_path)
                    
                    # é‡ç½®
                    current_chunk, current_length, chunk_chars = [], 0, 0
                    chunk_start_idx = i
                
                # æ·»åŠ å½“å‰itemåˆ°chunk
                current_chunk.append(item)
                current_length += item_length
                chunk_chars += source_text_length
            
            # å¤„ç†æœ€åä¸€ä¸ªchunk
            if current_chunk:
                chunks.append(current_chunk)
                context_chunk = self._generate_context_chunk(items, previous_line_count, chunk_start_idx)
                context_chunks.append(context_chunk)
                file_paths.append(file_path)
        
        return chunks, context_chunks, file_paths
    
    def _generate_context_chunk(self, all_items: List[CacheItem], previous_count: int, start_idx: int) -> List[CacheItem]:
        """ç”Ÿæˆä¸Šä¸‹æ–‡chunkï¼ˆä¸åŸCacheManager.generate_previous_chunksä¸€è‡´ï¼‰"""
        if previous_count <= 0 or start_idx <= 0:
            return []
        
        from_idx = max(0, start_idx - previous_count)
        to_idx = start_idx
        
        return all_items[from_idx:to_idx]
    
    def _translate_chunk(self, chunk: List[CacheItem], context_chunk: List[CacheItem], 
                         file_path: str, chunk_idx: int, total_chunks: int,
                         terminology_db: Dict, memory_storage: Dict, task_memory: Dict,
                         progress_callback=None, completed_units: int = 0) -> Optional[Dict[str, Any]]:
        """
        æ‰¹é‡ç¿»è¯‘ä¸€ä¸ªchunkï¼ˆå¤šè¡Œæ–‡æœ¬ï¼‰- åŸºäºPlanningAgentç­–ç•¥çš„æ™ºèƒ½ç¿»è¯‘
        
        Args:
            chunk: å¾…ç¿»è¯‘çš„æ–‡æœ¬å•å…ƒåˆ—è¡¨ï¼ˆ10-15ä¸ªCacheItemï¼‰
            context_chunk: ä¸Šä¸‹æ–‡å•å…ƒåˆ—è¡¨
            file_path: æ–‡ä»¶è·¯å¾„
            chunk_idx: å½“å‰æ‰¹æ¬¡åºå·
            total_chunks: æ€»æ‰¹æ¬¡æ•°
            terminology_db: æœ¯è¯­åº“
            memory_storage: è®°å¿†å­˜å‚¨
            task_memory: ä»»åŠ¡å…ƒæ•°æ®ï¼ˆåŒ…å«chunkç­–ç•¥å’Œå®ä½“æ•°æ®åº“ï¼‰
            progress_callback: è¿›åº¦å›è°ƒ
            completed_units: å·²å®Œæˆçš„å•å…ƒæ•°
            
        Returns:
            ç¿»è¯‘ç»“æœå­—å…¸
        """
        try:
            chunk_size = len(chunk)
            self.info(f"\n{'='*60}")
            self.info(f"[{chunk_idx}/{total_chunks}] æ­£åœ¨æ‰¹é‡ç¿»è¯‘ {chunk_size} ä¸ªæ–‡æœ¬å•å…ƒ...")
            self.info(f"{'='*60}")
            
            # æ„å»ºæ‰¹é‡ç¿»è¯‘çš„promptï¼ˆåˆå¹¶æ‰€æœ‰æ–‡æœ¬ï¼‰
            source_texts = [item.source_text for item in chunk]
            context_texts = [item.source_text for item in context_chunk] if context_chunk else []
            
            # è·å–å½“å‰chunkçš„ç¿»è¯‘ç­–ç•¥ï¼ˆä»PlanningAgentçš„åˆ†æç»“æœï¼‰
            chunk_strategies = task_memory.get("chunk_strategies", [])
            chunk_strategy_info = chunk_strategies[chunk_idx - 1] if chunk_idx - 1 < len(chunk_strategies) else None
            strategy = chunk_strategy_info["strategy"] if chunk_strategy_info else "free"  # é»˜è®¤æ„è¯‘
            
            self.info(f"  ğŸ“‹ ç¿»è¯‘ç­–ç•¥: {strategy} ({chunk_strategy_info['reason'] if chunk_strategy_info else 'é»˜è®¤æ„è¯‘'})")
            
            # ========== æ­¥éª¤1+2åˆå¹¶: åŸºäºç­–ç•¥çš„æ‰¹é‡ç¿»è¯‘ï¼ˆå¤šæ­¥éª¤å¼•å¯¼ï¼‰ ==========
            # æ ¹æ®PlanningAgentçš„ç­–ç•¥ï¼Œç›´æ¥æ‰§è¡Œå¯¹åº”çš„ç¿»è¯‘æ–¹å¼ï¼Œä¸éœ€è¦ç”Ÿæˆå¤šä¸ªç‰ˆæœ¬
            self.info(f"  â†’ æ­¥éª¤1: æ‰¹é‡{strategy}ç¿»è¯‘ï¼ˆå¤šæ­¥éª¤å¼•å¯¼: ç†è§£â†’åˆ†è§£â†’è½¬æ¢â†’æ¶¦è‰²ï¼‰...")
            
            translated_texts = self._strategy_based_batch_translation(
                source_texts, context_texts, strategy, terminology_db, memory_storage
            )
            
            # ğŸ”¥ ã€å…³é”®æ£€æŸ¥ã€‘ä¸¥æ ¼éªŒè¯è¿”å›è¡Œæ•°
            if translated_texts and len(translated_texts) != chunk_size:
                self.error(f"  âŒ è‡´å‘½é”™è¯¯ï¼šè¿”å›è¡Œæ•°ä¸åŒ¹é…ï¼åŸæ–‡{chunk_size}è¡Œï¼Œè¯‘æ–‡{len(translated_texts)}è¡Œ")
                self.error(f"  â†’ è¿™ä¼šå¯¼è‡´åç»­æ‰€æœ‰å†…å®¹é”™ä½ï¼Œå¿…é¡»é‡æ–°ç¿»è¯‘æ•´ä¸ªbatch")
                self.warning(f"  â†’ è§¦å‘å®Œå…¨é‡è¯‘...")
                # å¼ºåˆ¶é€è¡Œé‡è¯‘æ•´ä¸ªbatch
                translated_texts = self._fallback_translate_one_by_one(
                    source_texts, context_texts, strategy, terminology_db, memory_storage
                )
            
            # ğŸ”¥ Fallbackæœºåˆ¶ï¼šå¦‚æœæ‰¹é‡ç¿»è¯‘å¤±è´¥æˆ–éƒ¨åˆ†å¤±è´¥ï¼Œå¯¹ç¼ºå¤±çš„è¡Œè¿›è¡Œå•ç‹¬é‡è¯•
            if not translated_texts:
                self.warning(f"  âš  æ‰¹é‡ç¿»è¯‘å®Œå…¨å¤±è´¥ï¼Œå°è¯•é€è¡Œç¿»è¯‘...")
                translated_texts = self._fallback_translate_one_by_one(
                    source_texts, context_texts, strategy, terminology_db, memory_storage
                )
            else:
                # ğŸ”¥ è¡¥é½åˆ—è¡¨é•¿åº¦ï¼ˆå¦‚æœè¿”å›æ•°é‡ä¸è¶³ï¼‰
                while len(translated_texts) < chunk_size:
                    translated_texts.append("")
                
                # ğŸ”¥ æ£€æŸ¥æ¯ä¸€è¡Œï¼Œå¯¹ä»¥ä¸‹æƒ…å†µè¿›è¡Œè¡¥å……ç¿»è¯‘ï¼š
                # 1. ç©ºå­—ç¬¦ä¸²
                # 2. ä¸¥é‡æˆªæ–­ï¼ˆè¯‘æ–‡é•¿åº¦ < åŸæ–‡é•¿åº¦ * 0.3 ä¸”åŸæ–‡é•¿åº¦ > 100ï¼‰
                problem_indices = []
                for i, (src, trans) in enumerate(zip(source_texts[:chunk_size], translated_texts[:chunk_size])):
                    if not trans or trans.strip() == "":
                        problem_indices.append((i, "ç©º"))
                    elif len(src) > 100 and len(trans) < len(src) * 0.3:
                        # è¯‘æ–‡ä¸¥é‡æˆªæ–­ï¼ˆé•¿åŸæ–‡ä½†è¯‘æ–‡å¤ªçŸ­ï¼‰
                        problem_indices.append((i, f"æˆªæ–­({len(trans)}/{len(src)})"))
                
                if problem_indices:
                    self.warning(f"  âš  æ‰¹é‡ç¿»è¯‘éƒ¨åˆ†å¤±è´¥: {len(problem_indices)} è¡Œéœ€è¦é‡è¯•...")
                    
                    for i, reason in problem_indices:
                        self.warning(f"    â†’ æ­£åœ¨å•ç‹¬ç¿»è¯‘ç¬¬ {i+1} è¡Œï¼ˆ{reason}ï¼‰...")
                        single_translation = self._translate_single_line(
                            source_texts[i], context_texts, strategy, terminology_db, memory_storage
                        )
                        if single_translation and single_translation.strip():
                            # æ£€æŸ¥å•è¡Œç¿»è¯‘æ˜¯å¦ä¹Ÿè¢«æˆªæ–­
                            if len(source_texts[i]) > 100 and len(single_translation) < len(source_texts[i]) * 0.3:
                                self.warning(f"    âš  ç¬¬ {i+1} è¡Œå•ç‹¬ç¿»è¯‘ä¹Ÿè¢«æˆªæ–­ ({len(single_translation)}/{len(source_texts[i])})")
                            translated_texts[i] = single_translation
                            self.info(f"    âœ“ ç¬¬ {i+1} è¡Œç¿»è¯‘å®Œæˆ (é•¿åº¦: {len(single_translation)})")
                        else:
                            # å¦‚æœå•ç‹¬ç¿»è¯‘ä¹Ÿå¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡æ ‡è®°
                            translated_texts[i] = f"[ç¿»è¯‘å¤±è´¥]{source_texts[i]}"
                            self.error(f"    âœ— ç¬¬ {i+1} è¡Œç¿»è¯‘å¤±è´¥ï¼Œä¿ç•™åŸæ–‡")
                    
                    self.info(f"  âœ“ è¡¥å……ç¿»è¯‘å®Œæˆ")
            
            # æœ€ç»ˆæ£€æŸ¥
            if len(translated_texts) != chunk_size:
                self.error(f"  âœ— æ‰¹æ¬¡ {chunk_idx} ç¿»è¯‘å¤±è´¥ï¼šæ— æ³•å®Œæˆæ‰€æœ‰è¡Œçš„ç¿»è¯‘")
                return {"success": False}
            
            self.info(f"  âœ“ ç­–ç•¥ç¿»è¯‘å®Œæˆ: {len(translated_texts)} è¡Œ")
            
            # ========== æ­¥éª¤2: æ‰¹é‡å›è¯‘éªŒè¯ï¼ˆä¸è¿›è¡Œäººå·¥å®¡æ ¸ï¼Œåªè¿”å›è¯„åˆ†æ•°æ®ï¼‰ ==========
            self.info(f"  â†’ æ­¥éª¤2: æ‰¹é‡å›è¯‘éªŒè¯ï¼ˆTEaR: æ‰¹é‡å›è¯‘â†’æ‰¹é‡è¯„ä¼°â†’æ‰¹é‡ä¿®æ­£ï¼‰...")
            verified_texts, quality_scores, back_translations = self._batch_tear_verification_with_scores(
                source_texts, translated_texts, terminology_db
            )
            
            if not verified_texts or len(verified_texts) != chunk_size:
                self.warning(f"  âš  å›è¯‘éªŒè¯å¤±è´¥ï¼Œä½¿ç”¨åŸè¯‘æ–‡")
                verified_texts = translated_texts
                quality_scores = [8.0] * chunk_size  # é»˜è®¤è¯„åˆ†
                back_translations = [""] * chunk_size
            
            self.info(f"  âœ“ å›è¯‘éªŒè¯å®Œæˆ: {len(verified_texts)} è¡Œ")
            translated_texts = verified_texts
            
            # [DB] Phase 3: è®°å½•è¯„ä¼°è½¨è¿¹ (Evaluate Trace) + æ›´æ–° examination å­—æ®µ
            if self.db_manager and self._current_cache_project:
                try:
                    for idx, (score, back_trans) in enumerate(zip(quality_scores, back_translations)):
                        if idx < len(chunk):
                            item = chunk[idx]
                            atom_id = self._get_atom_id(self._current_cache_project, file_path, item.row_index)
                            if atom_id:
                                # ç¡®å®šè­¦å‘Šçº§åˆ«
                                if score >= 8.0:
                                    warning_level = "low"
                                elif score >= 6.0:
                                    warning_level = "medium"
                                else:
                                    warning_level = "high"
                                
                                # è®°å½•è¯„ä¼°è½¨è¿¹
                                self.db_manager.add_trace(
                                    atom_id=atom_id,
                                    agent_role="QualityAssessor",
                                    action_type="evaluate",
                                    content=f"Quality Score: {score}",
                                    quality_report={
                                        "score": score,
                                        "back_translation": back_trans,
                                        "status": "pass" if score >= 8.0 else "needs_refinement"
                                    }
                                )
                                
                                # æ›´æ–°åŸå­çš„ examination å­—æ®µï¼ˆè´¨é‡æ£€æŸ¥ä¿¡æ¯ï¼‰
                                examination = {
                                    "back_translation": back_trans,
                                    "warning_level": warning_level,
                                    "semantic_similarity": score / 10.0,  # è½¬æ¢ä¸º0-1èŒƒå›´
                                    "issues": [] if score >= 8.0 else ["éœ€è¦æ¶¦è‰²"],
                                    "algorithm": "backtranslation"
                                }
                                self.db_manager.update_atom_examination(atom_id, examination)
                except Exception as e:
                    self.error(f"[DB] è®°å½•è¯„ä¼°è½¨è¿¹å¤±è´¥: {e}")

            # æ›´æ–°ç¼“å­˜
            translated_items = []
            for item, translated_text in zip(chunk, translated_texts):
                if translated_text:
                    self._update_cache_item(item, translated_text)
                    translated_items.append({
                        "source": item.source_text,
                        "translated": translated_text,
                        "status": "success"
                    })

                    # [DB] Phase 3: è®°å½•åˆç¿»è½¨è¿¹ (Draft Trace)
                    if self.db_manager and self._current_cache_project:
                        atom_id = self._get_atom_id(self._current_cache_project, file_path, item.row_index)
                        if atom_id:
                            self.db_manager.add_trace(
                                atom_id=atom_id,
                                agent_role="Translator",
                                action_type="draft",
                                content=translated_text,
                                meta_data={"strategy": strategy}
                            )
                            # åŒæ—¶æ›´æ–°åŸå­çš„ç¿»è¯‘ç»“æœ
                            self.db_manager.update_atom_translation(
                                atom_id=atom_id,
                                translated_text=translated_text,
                                status_code=1  # å·²åˆç¿»
                            )
            
            # ğŸ”¥ æ›´æ–°è¡Œæ•°ç»Ÿè®¡ï¼ˆæ¯å®Œæˆä¸€ä¸ªchunkå°±æ›´æ–°ï¼‰
            self._update_line_stats(chunk_size)
            
            self.info(f"âœ“ æ‰¹æ¬¡ {chunk_idx} å®Œæ•´ç¿»è¯‘æµç¨‹å®Œæˆ: {chunk_size} ä¸ªå•å…ƒ")
            self.info(f"{'='*60}\n")
        
            return {
                "success": True,
                "translated_items": translated_items,
                "chunk_size": chunk_size,
                "source_texts": source_texts,  # ğŸ”¥ è¿”å›åŸæ–‡
                "translated_texts": translated_texts,  # ğŸ”¥ è¿”å›è¯‘æ–‡
                "quality_scores": quality_scores,  # ğŸ”¥ è¿”å›è¯„åˆ†
                "back_translations": back_translations,  # ğŸ”¥ è¿”å›å›è¯‘
                "chunk_items": chunk,  # ğŸ”¥ è¿”å›CacheItemç”¨äºåç»­æ›´æ–°
                "file_path": file_path # ğŸ”¥ è®°å½•æ–‡ä»¶è·¯å¾„
            }
                
        except Exception as e:
            self.error(f"ç¿»è¯‘æ‰¹æ¬¡ {chunk_idx} å¤±è´¥: {e}", e)
            return {"success": False}
    
    def _translate_single_unit(self, unit: Dict, idx: int, total_units: int, terminology_db: Dict, memory_storage: Dict) -> Optional[Dict[str, Any]]:
        """
        ç¿»è¯‘å•ä¸ªæ–‡æœ¬å•å…ƒï¼ˆç”¨äºå¹¶è¡Œè°ƒç”¨ï¼‰
        
        Args:
            unit: ç¿»è¯‘å•å…ƒ
            idx: å½“å‰åºå·
            total_units: æ€»æ•°
            terminology_db: æœ¯è¯­åº“
            memory_storage: è®°å¿†å­˜å‚¨
            
        Returns:
            ç¿»è¯‘ç»“æœå­—å…¸
        """
        try:
            self.info(f"\n{'='*60}")
            self.info(f"[{idx}/{total_units}] æ­£åœ¨ç¿»è¯‘...")
            self.info(f"{'='*60}")
            self.info(f"åŸæ–‡: {unit['source_text'][:200]}{'...' if len(unit['source_text']) > 200 else ''}")
            self.info(f"-" * 60)
            
            # 1. å¤šæ­¥éª¤å¼•å¯¼ç¿»è¯‘
            translated_text = self._multi_step_translation(unit, terminology_db, memory_storage)
            
            # 2. å¤šç‰ˆæœ¬ç”Ÿæˆä¸èåˆ
            if translated_text:
                optimized_text = self._multi_version_fusion(unit, translated_text, terminology_db, memory_storage)
            else:
                optimized_text = translated_text
            
            # 3. å›è¯‘éªŒè¯ä¸è‡ªæˆ‘ä¿®æ­£ï¼ˆTEaRï¼‰
            if optimized_text:
                final_text = self._tear_verification(unit, optimized_text, terminology_db)
            else:
                final_text = optimized_text
            
            # æ›´æ–°ç¼“å­˜
            if final_text:
                self._update_cache_item(unit["item"], final_text)
                
                # è¾“å‡ºç¿»è¯‘ç»“æœ
                self.info(f"è¯‘æ–‡: {final_text[:200]}{'...' if len(final_text) > 200 else ''}")
                self.info(f"âœ“ ç¿»è¯‘å®Œæˆ [{idx}/{total_units}]")
                self.info(f"{'='*60}\n")
                
                return {
                    "item_id": unit["item_id"],
                    "source": unit["source_text"],
                    "translated": final_text,
                    "status": "success"
                }
            else:
                self.warning(f"ç¿»è¯‘å•å…ƒ {unit['item_id']} è¿”å›ç©ºç»“æœ")
                return None
                
        except Exception as e:
            self.error(f"ç¿»è¯‘å•å…ƒ {unit['item_id']} å¤±è´¥: {e}", e)
            return None
    
    def _multi_step_batch_translation(self, source_texts: List[str], context_texts: List[str],
                                      terminology_db: Dict, memory_storage: Dict) -> Optional[List[str]]:
        """
        æ‰¹é‡å¤šæ­¥éª¤ç¿»è¯‘ï¼ˆä¸€æ¬¡APIè°ƒç”¨ç¿»è¯‘å¤šè¡Œï¼‰
        ä½¿ç”¨ä¸åŸTranslatorTaskç›¸åŒçš„textareaæ ¼å¼å’ŒResponseExtractorè§£æ
        
        Args:
            source_texts: å¾…ç¿»è¯‘æ–‡æœ¬åˆ—è¡¨
            context_texts: ä¸Šä¸‹æ–‡æ–‡æœ¬åˆ—è¡¨
            terminology_db: æœ¯è¯­åº“
            memory_storage: è®°å¿†å­˜å‚¨
            
        Returns:
            ç¿»è¯‘ç»“æœåˆ—è¡¨
        """
        self.info("  â†’ æ­¥éª¤1: æ‰¹é‡å¤šæ­¥éª¤ç¿»è¯‘ï¼ˆç†è§£â†’åˆ†è§£â†’è½¬æ¢â†’æ¶¦è‰²ï¼‰...")
        
        # æ„å»ºæ‰¹é‡ç¿»è¯‘æç¤ºè¯ï¼ˆâœ… ä¼ é€’source_textsç”¨äºåŠ¨æ€ç­›é€‰ï¼‰
        terminology_prompt = self._build_terminology_prompt(terminology_db, source_texts)
        memory_context = self._build_memory_context(memory_storage)
        
        # ã€å…³é”®ã€‘ä½¿ç”¨ä¸åŸTranslatorTaskç›¸åŒçš„system_promptæ ¼å¼
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘ä¸“å®¶ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œç¿»è¯‘ï¼š

æ­¥éª¤1 - ç†è§£ï¼šåˆ†æåŸæ–‡çš„è¯­ä¹‰ã€è¯­å¢ƒå’Œé£æ ¼
æ­¥éª¤2 - åˆ†è§£ï¼šå¯¹äºé•¿éš¾å¥ï¼Œå…ˆè¯†åˆ«ä¸»å¹²æˆåˆ†å’Œä»å¥å±‚çº§
æ­¥éª¤3 - è½¬æ¢ï¼šå°†åŸæ–‡è½¬æ¢ä¸ºç›®æ ‡è¯­è¨€ï¼Œä¿æŒè¯­ä¹‰å‡†ç¡®
æ­¥éª¤4 - æ¶¦è‰²ï¼šä¼˜åŒ–è¯‘æ–‡ï¼Œç¡®ä¿æµç•…è‡ªç„¶

{terminology_prompt}
{memory_context}

ã€é‡è¦ã€‘è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- å¿…é¡»ä½¿ç”¨<textarea>æ ‡ç­¾åŒ…è£¹æ‰€æœ‰è¯‘æ–‡
- æ¯è¡Œè¯‘æ–‡å‰å¿…é¡»åŠ ä¸Šåºå·ï¼ˆå¦‚1. 2. 3.ï¼‰
- ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„æ ‡é¢˜ã€å‰ç¼€æˆ–è¯´æ˜æ–‡å­—
- æ ¼å¼ç¤ºä¾‹ï¼š
<textarea>
1.ç¬¬ä¸€è¡Œè¯‘æ–‡
2.ç¬¬äºŒè¡Œè¯‘æ–‡
3.ç¬¬ä¸‰è¡Œè¯‘æ–‡
</textarea>"""
        
        # ã€å…³é”®ã€‘æ„å»ºsource_text_dictï¼ˆä¸åŸTranslatorTaskå®Œå…¨ç›¸åŒï¼‰
        source_text_dict = {str(i): text for i, text in enumerate(source_texts)}
        
        # ã€å…³é”®ã€‘ä½¿ç”¨ä¸åŸPromptBuilder.build_source_textç›¸åŒçš„é€»è¾‘æ„å»ºåŸæ–‡
        numbered_lines = []
        for index, line in enumerate(source_texts):
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šè¡Œæ–‡æœ¬
            if "\n" in line:
                lines = line.split("\n")
                numbered_text = f"{index + 1}.[\n"
                total_lines = len(lines)
                for sub_index, sub_line in enumerate(lines):
                    # ä»…å½“åªæœ‰ä¸€ä¸ªå°¾éšç©ºæ ¼æ—¶æ‰å»é™¤
                    sub_line = sub_line[:-1] if re.match(r'.*[^ ] $', sub_line) else sub_line
                    numbered_text += f'"{index + 1}.{total_lines - sub_index}.,{sub_line}",\n'
                numbered_text = numbered_text.rstrip('\n').rstrip(',')
                numbered_text += f"\n]"
                numbered_lines.append(numbered_text)
            else:
                # å•è¡Œæ–‡æœ¬ç›´æ¥æ·»åŠ åºå·
                numbered_lines.append(f"{index + 1}.{line}")
        
        source_text = "\n".join(numbered_lines)
        
        # ã€å…³é”®ã€‘æ„å»ºä¸Šä¸‹æ–‡ï¼ˆä¸åŸæ–¹æ³•ç›¸åŒï¼‰
        context_str = "\n".join(context_texts[-3:]) if context_texts else ""
        context_prefix = f"###ä¸Šæ–‡å†…å®¹\n{context_str}\n" if context_str else ""
        
        # ã€å…³é”®ã€‘ä½¿ç”¨ä¸åŸæ–¹æ³•ç›¸åŒçš„textareaæ ‡ç­¾æ ¼å¼
        user_prompt = f"""{context_prefix}###å¾…ç¿»è¯‘æ–‡æœ¬
<textarea>
{source_text}
</textarea>

###è¯‘æ–‡è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
<textarea>
ï¼ˆåœ¨è¿™é‡Œè¾“å‡ºå¸¦åºå·çš„è¯‘æ–‡ï¼Œæ¯è¡Œä¸€ä¸ªåºå·ï¼‰
</textarea>"""
        
        messages = [{"role": "user", "content": user_prompt}]
        
        try:
            # ç­‰å¾…RequestLimiterå…è®¸å‘é€è¯·æ±‚
            if not self._wait_for_limiter(messages, system_prompt):
                self.warning("  âš  RequestLimiteræ£€æŸ¥å¤±è´¥")
                return None
            
            platform_config = self.config.get_platform_configuration("translationReq") if self.config else {}
            
            skip, _, response_content, _, _ = self.llm_requester.sent_request(
                messages=messages,
                system_prompt=system_prompt,
                platform_config=platform_config
            )
            
            if not skip and response_content:
                # ã€è°ƒè¯•ã€‘æ‰“å°LLMåŸå§‹å“åº”ï¼ˆå‰1000å­—ç¬¦ï¼‰
                self.debug(f"  [è°ƒè¯•] LLMåŸå§‹å“åº”ï¼ˆå‰1000å­—ç¬¦ï¼‰ï¼š\n{response_content[:1000]}")
                
                # ã€å…³é”®ã€‘ä½¿ç”¨ResponseExtractoræå–ç¿»è¯‘ç»“æœï¼ˆä¸åŸTranslatorTaskå®Œå…¨ç›¸åŒï¼‰
                response_dict = ResponseExtractor.text_extraction(self, source_text_dict, response_content)
                
                # ã€è°ƒè¯•ã€‘æ‰“å°è§£æåçš„å­—å…¸
                if response_dict:
                    self.debug(f"  [è°ƒè¯•] ResponseExtractorè§£æåå­—å…¸é”®: {list(response_dict.keys())}")
                    self.debug(f"  [è°ƒè¯•] ç¬¬ä¸€ä¸ªè¯‘æ–‡ç¤ºä¾‹: {list(response_dict.values())[0][:100] if response_dict else 'None'}...")
                else:
                    self.warning(f"  [è°ƒè¯•] ResponseExtractorè§£æè¿”å›Noneæˆ–ç©ºå­—å…¸")
                
                # ã€å…³é”®ã€‘å»é™¤æ•°å­—åºå·å‰ç¼€ï¼ˆä¸åŸæ–¹æ³•ç›¸åŒï¼‰
                response_dict = ResponseExtractor.remove_numbered_prefix(self, response_dict)
                
                # ã€å…³é”®ã€‘ä¸åŸæ–¹æ³•ä¸€è‡´ï¼šåªå–æˆ‘ä»¬éœ€è¦çš„é”®ï¼Œå¿½ç•¥å¤šä½™çš„é”®
                # ResponseExtractorçš„generate_text_by_newlinesä¼šè‡ªåŠ¨å¤„ç†å¤šä½™çš„è¯‘æ–‡
                if response_dict:
                    translated_texts = []
                    missing_keys = []
                    
                    for i in range(len(source_texts)):
                        key = str(i)
                        if key in response_dict:
                            translated_texts.append(response_dict[key])
                        else:
                            missing_keys.append(key)
                            translated_texts.append("")  # ç¼ºå¤±çš„é”®ç”¨ç©ºå­—ç¬¦ä¸²å¡«å……
                    
                    # ã€è°ƒè¯•ã€‘å¦‚æœè¯‘æ–‡æ•°é‡ä¸ç­‰äºåŸæ–‡æ•°é‡ï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯
                    if len(response_dict) != len(source_texts):
                        self.debug(f"  [è°ƒè¯•] è¯‘æ–‡æ•°é‡({len(response_dict)})â‰ åŸæ–‡æ•°é‡({len(source_texts)})ï¼Œå·²è‡ªåŠ¨å¤„ç†")
                        self.debug(f"  [è°ƒè¯•] response_dicté”®: {list(response_dict.keys())}")
                        if len(response_dict) > len(source_texts):
                            extra_keys = [k for k in response_dict.keys() if int(k) >= len(source_texts)]
                            self.debug(f"  [è°ƒè¯•] å¤šä½™çš„é”®(å·²å¿½ç•¥): {extra_keys}")
                    
                    if missing_keys:
                        self.warning(f"  âš  éƒ¨åˆ†è¯‘æ–‡ç¼ºå¤±: é”®{missing_keys}ä¸å­˜åœ¨")
                    
                    # åªè¦è·å–åˆ°äº†éƒ¨åˆ†è¯‘æ–‡å°±è¿”å›ï¼ˆä¸åŸæ–¹æ³•ä¸€è‡´ï¼‰
                    if any(translated_texts):
                        self.info(f"  âœ“ æ‰¹é‡ç¿»è¯‘æˆåŠŸ: {len([t for t in translated_texts if t])} è¡Œ")
                        return translated_texts
                    else:
                        self.warning(f"  âš  æ‰€æœ‰è¯‘æ–‡ä¸ºç©º")
                        return None
                else:
                    self.warning(f"  âš  ResponseExtractorè¿”å›ç©ºå­—å…¸")
                    return None
            else:
                self.warning("  âš  LLMè¿”å›ä¸ºç©ºæˆ–è¢«è·³è¿‡")
                return None
                
        except Exception as e:
            self.error(f"  âœ— æ‰¹é‡ç¿»è¯‘å¤±è´¥: {e}", e)
            return None
    
    def _multi_step_translation(self, unit: Dict, terminology_db: Dict, memory_storage: Dict) -> Optional[str]:
        """
        å¤šæ­¥éª¤å¼•å¯¼ç¿»è¯‘
        å°†ç¿»è¯‘ä»»åŠ¡æ‹†åˆ†ä¸º"ç†è§£â€”åˆ†è§£â€”è½¬æ¢â€”æ¶¦è‰²"é˜¶æ®µ
        """
        self.info("  â†’ æ­¥éª¤1: å¤šæ­¥éª¤å¼•å¯¼ç¿»è¯‘...")
        
        source_text = unit["source_text"]
        context = unit.get("context", [])
        
        # æ„å»ºå¤šæ­¥éª¤æç¤ºè¯ï¼ˆâœ… ä¼ é€’å•ä¸ªsource_textä½œä¸ºåˆ—è¡¨ï¼‰
        terminology_prompt = self._build_terminology_prompt(terminology_db, [source_text])
        memory_context = self._build_memory_context(memory_storage)
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘ä¸“å®¶ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œç¿»è¯‘ï¼š

æ­¥éª¤1 - ç†è§£ï¼šåˆ†æåŸæ–‡çš„è¯­ä¹‰ã€è¯­å¢ƒå’Œé£æ ¼
æ­¥éª¤2 - åˆ†è§£ï¼šå¯¹äºé•¿éš¾å¥ï¼Œå…ˆè¯†åˆ«ä¸»å¹²æˆåˆ†å’Œä»å¥å±‚çº§
æ­¥éª¤3 - è½¬æ¢ï¼šå°†åŸæ–‡è½¬æ¢ä¸ºç›®æ ‡è¯­è¨€ï¼Œä¿æŒè¯­ä¹‰å‡†ç¡®
æ­¥éª¤4 - æ¶¦è‰²ï¼šä¼˜åŒ–è¯‘æ–‡ï¼Œç¡®ä¿æµç•…è‡ªç„¶

{terminology_prompt}
{memory_context}

è¯·ç›´æ¥è¾“å‡ºæœ€ç»ˆè¯‘æ–‡ï¼Œä¸è¦è¾“å‡ºä¸­é—´æ­¥éª¤ã€‚"""
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_text = "\n".join(context[-3:]) if context else ""
        user_content = f"""è¯·ç¿»è¯‘ä»¥ä¸‹æ–‡æœ¬ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context_text}

å¾…ç¿»è¯‘æ–‡æœ¬ï¼š
{source_text}"""
        
        messages = [{"role": "user", "content": user_content}]
        
        try:
            # ğŸ”¥ ç­‰å¾…RequestLimiterå…è®¸å‘é€è¯·æ±‚
            if not self._wait_for_limiter(messages, system_prompt):
                self.warning("RequestLimiteræ£€æŸ¥å¤±è´¥æˆ–è¶…æ—¶ï¼Œè·³è¿‡å¤šæ­¥éª¤ç¿»è¯‘")
                return None
            
            platform_config = self.config.get_platform_configuration("translationReq") if self.config else {}
            
            # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºAgentç¿»è¯‘ä½¿ç”¨çš„é…ç½®ï¼ˆä»…é¦–æ¬¡ï¼‰
            if not hasattr(self, '_logged_config'):
                self._logged_config = True
                self.info("=" * 60)
                self.info("[Agent ç¿»è¯‘é…ç½®]")
                self.info(f"å¹³å°: {platform_config.get('target_platform', 'unknown')}")
                self.info(f"API URL: {platform_config.get('api_url', 'N/A')}")
                self.info(f"æ¨¡å‹: {platform_config.get('model_name', 'N/A')}")
                self.info("=" * 60)
            
            skip, response_think, response_content, _, _ = self.llm_requester.sent_request(
                messages=messages,
                system_prompt=system_prompt,
                platform_config=platform_config
            )
            
            if not skip and response_content:
                # æå–è¯‘æ–‡ï¼ˆå»é™¤å¯èƒ½çš„æ­¥éª¤æ ‡è®°ï¼‰
                translated = self._extract_translation(response_content)
                self.info(f"  âœ“ åˆæ­¥è¯‘æ–‡: {translated[:100]}{'...' if len(translated) > 100 else ''}")
                return translated
        except Exception as e:
            self.error(f"å¤šæ­¥éª¤ç¿»è¯‘å¤±è´¥: {e}", e)
        
        return None
    
    def _multi_version_fusion(self, unit: Dict, initial_translation: str, 
                             terminology_db: Dict, memory_storage: Dict) -> Optional[str]:
        """
        å¤šç‰ˆæœ¬ç”Ÿæˆä¸èåˆ
        ç”Ÿæˆå¤šç§é£æ ¼çš„è¯‘æ–‡ç‰ˆæœ¬ï¼Œç„¶åæ™ºèƒ½è¯„é€‰ä¸èåˆ
        """
        self.info("  â†’ æ­¥éª¤2: å¤šç‰ˆæœ¬ç”Ÿæˆä¸èåˆ...")
        
        source_text = unit["source_text"]
        
        # ç”Ÿæˆå¤šä¸ªç‰ˆæœ¬
        versions = {}
        
        # ç‰ˆæœ¬1ï¼šç›´è¯‘ç‰ˆ
        versions["literal"] = self._generate_version(source_text, initial_translation, "literal", terminology_db)
        
        # ç‰ˆæœ¬2ï¼šæ„è¯‘ç‰ˆ
        versions["free"] = self._generate_version(source_text, initial_translation, "free", terminology_db)
        
        # ç‰ˆæœ¬3ï¼šé£æ ¼åŒ–ç‰ˆï¼ˆæ ¹æ®memoryä¸­çš„é£æ ¼ï¼‰
        style = memory_storage.get("style", "neutral")
        versions["stylized"] = self._generate_version(source_text, initial_translation, f"stylized_{style}", terminology_db)
        
        # æ™ºèƒ½è¯„é€‰ä¸èåˆ
        best_version = self._select_and_fuse_versions(source_text, versions, terminology_db)
        
        # ä¿å­˜ç‰ˆæœ¬ä¿¡æ¯
        self.translation_versions[unit["item_id"]] = versions
        
        if best_version:
            self.info(f"  âœ“ èåˆåè¯‘æ–‡: {best_version[:100]}{'...' if len(best_version) > 100 else ''}")
        
        return best_version
    
    def _generate_version(self, source_text: str, initial_translation: str, 
                        version_type: str, terminology_db: Dict) -> Optional[str]:
        """
        ç”Ÿæˆç‰¹å®šç‰ˆæœ¬çš„ç¿»è¯‘
        ä½¿ç”¨textareaæ ¼å¼å’ŒResponseExtractorï¼ˆä¸åŸæ–¹æ³•ç›¸åŒï¼‰
        """
        version_prompts = {
            "literal": "è¯·æä¾›ç›´è¯‘ç‰ˆæœ¬ï¼Œå°½å¯èƒ½è´´è¿‘åŸæ–‡ç»“æ„",
            "free": "è¯·æä¾›æ„è¯‘ç‰ˆæœ¬ï¼Œæ³¨é‡æµç•…æ€§å’Œè‡ªç„¶åº¦",
            "stylized_formal": "è¯·æä¾›æ­£å¼é£æ ¼çš„ç¿»è¯‘ç‰ˆæœ¬",
            "stylized_informal": "è¯·æä¾›éæ­£å¼é£æ ¼çš„ç¿»è¯‘ç‰ˆæœ¬"
        }
        
        prompt_instruction = version_prompts.get(version_type, "è¯·æä¾›ç¿»è¯‘ç‰ˆæœ¬")
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘ä¸“å®¶ã€‚{prompt_instruction}ã€‚

{self._build_terminology_prompt(terminology_db, [source_text])}

ã€é‡è¦ã€‘è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- å¿…é¡»ä½¿ç”¨<textarea>æ ‡ç­¾åŒ…è£¹è¯‘æ–‡
- è¯‘æ–‡å‰å¿…é¡»åŠ ä¸Šåºå·"1."
- ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„æ ‡é¢˜ã€å‰ç¼€æˆ–è¯´æ˜æ–‡å­—
- æ ¼å¼ç¤ºä¾‹ï¼š<textarea>
1.è¯‘æ–‡å†…å®¹
</textarea>"""
        
        # ã€å…³é”®ã€‘ä½¿ç”¨textareaæ ¼å¼ï¼ˆå•è¡Œï¼‰
        source_text_dict = {"0": source_text}
        user_prompt = f"""###å¾…ç¿»è¯‘æ–‡æœ¬
<textarea>
1.{source_text}
</textarea>

###è¯‘æ–‡è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
<textarea>
1.
</textarea>"""
        
        messages = [{"role": "user", "content": user_prompt}]
        
        try:
            if not self._wait_for_limiter(messages, system_prompt):
                return initial_translation
            
            platform_config = self.config.get_platform_configuration("translationReq") if self.config else {}
            skip, _, response_content, _, _ = self.llm_requester.sent_request(
                messages=messages,
                system_prompt=system_prompt,
                platform_config=platform_config
            )
            
            if not skip and response_content:
                # ã€å…³é”®ã€‘ä½¿ç”¨ResponseExtractorè§£æ
                response_dict = ResponseExtractor.text_extraction(self, source_text_dict, response_content)
                response_dict = ResponseExtractor.remove_numbered_prefix(self, response_dict)
                
                if response_dict and "0" in response_dict:
                    return response_dict["0"]
                else:
                    # é™çº§ä¸ºç®€å•æå–
                    return self._extract_translation(response_content)
        except Exception as e:
            self.debug(f"ç”Ÿæˆ{version_type}ç‰ˆæœ¬å¤±è´¥: {e}")
        
        return initial_translation
    
    def _select_and_fuse_versions(self, source_text: str, versions: Dict[str, str], 
                                  terminology_db: Dict) -> str:
        """
        æ™ºèƒ½è¯„é€‰ä¸èåˆå¤šä¸ªç‰ˆæœ¬
        ä½¿ç”¨textareaæ ¼å¼å’ŒResponseExtractorï¼ˆä¸åŸæ–¹æ³•ç›¸åŒï¼‰
        """
        versions_text = "\n".join([f"{k}: {v}" for k, v in versions.items() if v])
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹å¤šä¸ªç¿»è¯‘ç‰ˆæœ¬ï¼Œå¹¶èåˆç”Ÿæˆæœ€ä½³è¯‘æ–‡ã€‚

è¯„ä¼°æ ‡å‡†ï¼š
1. è¯­ä¹‰å‡†ç¡®æ€§
2. æµç•…æ€§
3. é£æ ¼ä¸€è‡´æ€§
4. æœ¯è¯­ä½¿ç”¨è§„èŒƒæ€§

ã€é‡è¦ã€‘è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- å¿…é¡»ä½¿ç”¨<textarea>æ ‡ç­¾åŒ…è£¹è¯‘æ–‡
- è¯‘æ–‡å‰å¿…é¡»åŠ ä¸Šåºå·"1."
- ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„æ ‡é¢˜ã€å‰ç¼€æˆ–è¯´æ˜æ–‡å­—"""
        
        # ã€å…³é”®ã€‘ä½¿ç”¨textareaæ ¼å¼ï¼ˆå•è¡Œï¼‰
        source_text_dict = {"0": source_text}
        user_prompt = f"""åŸæ–‡ï¼š
<textarea>
1.{source_text}
</textarea>

ç¿»è¯‘ç‰ˆæœ¬ï¼š
{versions_text}

è¯·è¯„ä¼°å¹¶èåˆç”Ÿæˆæœ€ä½³è¯‘æ–‡ï¼š
<textarea>
1.
</textarea>"""
        
        messages = [{"role": "user", "content": user_prompt}]
        
        try:
            if not self._wait_for_limiter(messages, system_prompt):
                return list(versions.values())[0] if versions else ""
            
            platform_config = self.config.get_platform_configuration("translationReq") if self.config else {}
            skip, _, response_content, _, _ = self.llm_requester.sent_request(
                messages=messages,
                system_prompt=system_prompt,
                platform_config=platform_config
            )
            
            if not skip and response_content:
                # ã€å…³é”®ã€‘ä½¿ç”¨ResponseExtractorè§£æ
                response_dict = ResponseExtractor.text_extraction(self, source_text_dict, response_content)
                response_dict = ResponseExtractor.remove_numbered_prefix(self, response_dict)
                
                if response_dict and "0" in response_dict:
                    return response_dict["0"]
                else:
                    fused = self._extract_translation(response_content)
                    return fused if fused else list(versions.values())[0]
        except Exception as e:
            self.debug(f"ç‰ˆæœ¬èåˆå¤±è´¥: {e}")
        
        return list(versions.values())[0] if versions else ""
    
    def _tear_verification(self, unit: Dict, translated_text: str, 
                          terminology_db: Dict) -> str:
        """
        å›è¯‘éªŒè¯ä¸è‡ªæˆ‘ä¿®æ­£ï¼ˆTEaRæ¡†æ¶ï¼‰
        TEaR: Translate, Estimate, and Refine
        """
        self.info("  â†’ æ­¥éª¤3: å›è¯‘éªŒè¯ä¸è‡ªæˆ‘ä¿®æ­£...")
        
        source_text = unit["source_text"]
        
        # æ­¥éª¤1: Estimate - å›è¯‘å¹¶è¯„ä¼°
        back_translation = self._back_translate(translated_text)
        if back_translation:
            self.info(f"  âœ“ å›è¯‘ç»“æœ: {back_translation[:100]}{'...' if len(back_translation) > 100 else ''}")
        
        estimate_result = self._estimate_quality(source_text, translated_text, back_translation)
        
        # æ­¥éª¤2: Refine - å¦‚æœå‘ç°é—®é¢˜ï¼Œè¿›è¡Œä¿®æ­£
        if estimate_result.get("needs_refinement", False):
            issues = estimate_result.get("issues", [])
            self.info(f"  âš  å‘ç°è´¨é‡é—®é¢˜: {', '.join(issues[:3])}")
            refined_text = self._refine_translation(source_text, translated_text, 
                                                   estimate_result, terminology_db)
            if refined_text:
                self.info(f"  âœ“ ä¿®æ­£åè¯‘æ–‡: {refined_text[:100]}{'...' if len(refined_text) > 100 else ''}")
            return refined_text
        else:
            score = estimate_result.get("score", 0)
            self.info(f"  âœ“ è´¨é‡è¯„åˆ†: {score}/100 (æ— éœ€ä¿®æ­£)")
        
        return translated_text
    
    def _back_translate(self, translated_text: str) -> Optional[str]:
        """
        å°†è¯‘æ–‡å›è¯‘åˆ°æºè¯­è¨€
        ä½¿ç”¨textareaæ ¼å¼å’ŒResponseExtractorï¼ˆä¸åŸæ–¹æ³•ç›¸åŒï¼‰
        """
        source_lang = self.config.source_language if self.config else "chinese"
        target_lang = self.config.target_language if self.config else "english"
        
        system_prompt = f"""è¯·å°†ä»¥ä¸‹{target_lang}æ–‡æœ¬å›è¯‘ä¸º{source_lang}ã€‚

ã€é‡è¦ã€‘è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- å¿…é¡»ä½¿ç”¨<textarea>æ ‡ç­¾åŒ…è£¹å›è¯‘ç»“æœ
- å›è¯‘ç»“æœå‰å¿…é¡»åŠ ä¸Šåºå·"1."
- ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„æ ‡é¢˜ã€å‰ç¼€æˆ–è¯´æ˜æ–‡å­—"""
        
        # ã€å…³é”®ã€‘ä½¿ç”¨textareaæ ¼å¼ï¼ˆå•è¡Œï¼‰
        source_text_dict = {"0": translated_text}
        user_prompt = f"""è¯·å›è¯‘ä»¥ä¸‹æ–‡æœ¬ï¼š
<textarea>
1.{translated_text}
</textarea>

###å›è¯‘è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
<textarea>
1.
</textarea>"""
        
        messages = [{"role": "user", "content": user_prompt}]
        
        try:
            if not self._wait_for_limiter(messages, system_prompt):
                return None
            
            platform_config = self.config.get_platform_configuration("translationReq") if self.config else {}
            skip, _, response_content, _, _ = self.llm_requester.sent_request(
                messages=messages,
                system_prompt=system_prompt,
                platform_config=platform_config
            )
            
            if not skip and response_content:
                # ã€å…³é”®ã€‘ä½¿ç”¨ResponseExtractorè§£æ
                response_dict = ResponseExtractor.text_extraction(self, source_text_dict, response_content)
                response_dict = ResponseExtractor.remove_numbered_prefix(self, response_dict)
                
                if response_dict and "0" in response_dict:
                    return response_dict["0"]
                else:
                    return self._extract_translation(response_content)
        except Exception as e:
            self.debug(f"å›è¯‘å¤±è´¥: {e}")
        
        return None
    
    def _estimate_quality(self, source_text: str, translated_text: str, 
                         back_translation: Optional[str]) -> Dict[str, Any]:
        """
        è¯„ä¼°ç¿»è¯‘è´¨é‡ï¼ˆTEaRçš„Estimateæ­¥éª¤ï¼‰
        """
        if not back_translation:
            return {"needs_refinement": False, "issues": []}
        
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·æ¯”è¾ƒåŸæ–‡å’Œå›è¯‘æ–‡ï¼Œè¯„ä¼°ç¿»è¯‘è´¨é‡ã€‚

è¯„ä¼°ç»´åº¦ï¼š
1. è¯­ä¹‰åå·®
2. é€»è¾‘é”™è¯¯
3. ä¿¡æ¯é—æ¼
4. æœ¯è¯­ä¸€è‡´æ€§

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
{
    "needs_refinement": true/false,
    "issues": ["é—®é¢˜1", "é—®é¢˜2"],
    "score": 0-100
}"""
        
        messages = [{
            "role": "user",
            "content": f"åŸæ–‡ï¼š{source_text}\n\nè¯‘æ–‡ï¼š{translated_text}\n\nå›è¯‘æ–‡ï¼š{back_translation}\n\nè¯·è¯„ä¼°ç¿»è¯‘è´¨é‡ï¼š"
        }]
        
        try:
            # ğŸ”¥ ç­‰å¾…RequestLimiterå…è®¸å‘é€è¯·æ±‚
            if not self._wait_for_limiter(messages, system_prompt):
                self.debug("RequestLimiteræ£€æŸ¥å¤±è´¥ï¼Œè·³è¿‡è´¨é‡è¯„ä¼°")
                return {"needs_refinement": False, "issues": [], "score": 80}
            
            platform_config = self.config.get_platform_configuration("translationReq") if self.config else {}
            skip, _, response_content, _, _ = self.llm_requester.sent_request(
                messages=messages,
                system_prompt=system_prompt,
                platform_config=platform_config
            )
            
            if not skip and response_content:
                # å°è¯•è§£æJSON
                try:
                    json_start = response_content.find("{")
                    json_end = response_content.rfind("}") + 1
                    if json_start >= 0 and json_end > json_start:
                        json_str = response_content[json_start:json_end]
                        result = json.loads(json_str)
                        return result
                except json.JSONDecodeError:
                    pass
        except Exception as e:
            self.debug(f"è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
        
        return {"needs_refinement": False, "issues": [], "score": 80}
    
    def _refine_translation(self, source_text: str, translated_text: str, 
                           estimate_result: Dict, terminology_db: Dict) -> str:
        """
        ä¿®æ­£ç¿»è¯‘ï¼ˆTEaRçš„Refineæ­¥éª¤ï¼‰
        ä½¿ç”¨textareaæ ¼å¼å’ŒResponseExtractorï¼ˆä¸åŸæ–¹æ³•ç›¸åŒï¼‰
        """
        issues = estimate_result.get("issues", [])
        issues_text = "\n".join(issues) if issues else "æ— æ˜æ˜¾é—®é¢˜"
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘ä¿®æ­£ä¸“å®¶ã€‚è¯·æ ¹æ®è¯„ä¼°ç»“æœä¿®æ­£ä»¥ä¸‹è¯‘æ–‡ã€‚

è¯„ä¼°å‘ç°çš„é—®é¢˜ï¼š
{issues_text}

{self._build_terminology_prompt(terminology_db, [source_text])}

ã€é‡è¦ã€‘è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- å¿…é¡»ä½¿ç”¨<textarea>æ ‡ç­¾åŒ…è£¹ä¿®æ­£åçš„è¯‘æ–‡
- è¯‘æ–‡å‰å¿…é¡»åŠ ä¸Šåºå·"1."
- ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„æ ‡é¢˜ã€å‰ç¼€æˆ–è¯´æ˜æ–‡å­—"""
        
        # ã€å…³é”®ã€‘ä½¿ç”¨textareaæ ¼å¼ï¼ˆå•è¡Œï¼‰
        source_text_dict = {"0": source_text}
        user_prompt = f"""åŸæ–‡ï¼š
<textarea>
1.{source_text}
</textarea>

åŸè¯‘æ–‡ï¼š{translated_text}

è¯·ä¿®æ­£è¯‘æ–‡ï¼š
<textarea>
1.
</textarea>"""
        
        messages = [{"role": "user", "content": user_prompt}]
        
        try:
            if not self._wait_for_limiter(messages, system_prompt):
                return translated_text
            
            platform_config = self.config.get_platform_configuration("translationReq") if self.config else {}
            skip, _, response_content, _, _ = self.llm_requester.sent_request(
                messages=messages,
                system_prompt=system_prompt,
                platform_config=platform_config
            )
            
            if not skip and response_content:
                # ã€å…³é”®ã€‘ä½¿ç”¨ResponseExtractorè§£æ
                response_dict = ResponseExtractor.text_extraction(self, source_text_dict, response_content)
                response_dict = ResponseExtractor.remove_numbered_prefix(self, response_dict)
                
                if response_dict and "0" in response_dict:
                    return response_dict["0"]
                else:
                    refined = self._extract_translation(response_content)
                return refined if refined else translated_text
        except Exception as e:
            self.error(f"ç¿»è¯‘ä¿®æ­£å¤±è´¥: {e}")
        
        return translated_text
    
    def _build_terminology_prompt(self, terminology_db: Dict, source_texts: List[str] = None) -> str:
        """
        æ„å»ºæœ¯è¯­è¡¨æç¤ºè¯ï¼ˆåŒ…å«å®ä½“å’Œä¸“ä¸šæœ¯è¯­ï¼‰
        âœ… ã€å…³é”®æ”¹è¿›ã€‘é‡‡ç”¨åŸæ–¹æ³•çš„åŠ¨æ€ç­›é€‰ç­–ç•¥ï¼ŒåªåŒ…å«æœ¬æ‰¹æ¬¡æ–‡æœ¬ä¸­å‡ºç°çš„æœ¯è¯­
        
        Args:
            terminology_db: å®Œæ•´æœ¯è¯­åº“
            source_texts: æœ¬æ‰¹æ¬¡çš„åŸæ–‡åˆ—è¡¨ï¼ˆç”¨äºç­›é€‰ï¼‰
        
        Returns:
            æœ¯è¯­è¡¨æç¤ºè¯
        """
        if not terminology_db:
            return ""
        
        # âœ… ã€å…³é”®ã€‘åŠ¨æ€ç­›é€‰ï¼šåªåŒ…å«æœ¬æ‰¹æ¬¡æ–‡æœ¬ä¸­å®é™…å‡ºç°çš„æœ¯è¯­
        if source_texts:
            # å°†æ‰€æœ‰åŸæ–‡åˆå¹¶å¹¶è½¬ä¸ºå°å†™ï¼ˆç”¨äºåŒ¹é…ï¼‰
            combined_text = " ".join(source_texts).lower()
            
            # ç­›é€‰å‡ºç°åœ¨æœ¬æ‰¹æ¬¡ä¸­çš„æœ¯è¯­
            filtered_terms = []
            for term, info in terminology_db.items():
                term_lower = term.lower()
                # å¦‚æœæœ¯è¯­å‡ºç°åœ¨æœ¬æ‰¹æ¬¡æ–‡æœ¬ä¸­
                if term_lower in combined_text:
                    translation = info.get("translation", "")
                    if translation:
                        filtered_terms.append({
                            "term": term,
                            "translation": translation,
                            "type": info.get("type", "term"),
                            "info": info.get("info", "")
                        })
            
            # å¦‚æœæ²¡æœ‰ç­›é€‰åˆ°ä»»ä½•æœ¯è¯­ï¼Œè¿”å›ç©º
            if not filtered_terms:
                return ""
        else:
            # å¦‚æœæ²¡æœ‰æä¾›source_textsï¼Œä½¿ç”¨æ‰€æœ‰æœ¯è¯­ï¼ˆå‘åå…¼å®¹ï¼‰
            filtered_terms = []
            for term, info in list(terminology_db.items())[:50]:
                translation = info.get("translation", "")
                if translation:
                    filtered_terms.append({
                        "term": term,
                        "translation": translation,
                        "type": info.get("type", "term"),
                        "info": info.get("info", "")
                    })
        
        # âœ… ä½¿ç”¨åŸæ–¹æ³•çš„è¡¨æ ¼æ ¼å¼ï¼ˆæ›´æ¸…æ™°ï¼‰
        if self.config and self.config.target_language in ("chinese_simplified", "chinese_traditional"):
            prompt = "\n###æœ¯è¯­è¡¨\nåŸæ–‡|è¯‘æ–‡|å¤‡æ³¨\n"
        else:
            prompt = "\n###Glossary\nOriginal Text|Translation|Remarks\n"
        
        # æ·»åŠ ç­›é€‰åçš„æœ¯è¯­
        for item in filtered_terms:
            info_text = item["info"] if item["info"] else " "
            prompt += f"{item['term']}|{item['translation']}|{info_text}\n"
        
        return prompt
    
    def _build_terminology_prompt_for_backtranslation(self, terminology_db: Dict, translated_texts: List[str]) -> str:
        """
        æ„å»ºå›è¯‘ç”¨çš„æœ¯è¯­è¡¨æç¤ºè¯
        âœ… ç­›é€‰æ ‡å‡†ï¼šæ£€æŸ¥æœ¯è¯­çš„**è¯‘æ–‡**æ˜¯å¦åœ¨translated_textsä¸­å‡ºç°
        
        Args:
            terminology_db: å®Œæ•´æœ¯è¯­åº“
            translated_texts: æœ¬æ‰¹æ¬¡çš„è¯‘æ–‡åˆ—è¡¨ï¼ˆç”¨äºç­›é€‰ï¼‰
        
        Returns:
            æœ¯è¯­è¡¨æç¤ºè¯ï¼ˆåå‘ï¼šè¯‘æ–‡â†’åŸæ–‡ï¼‰
        """
        if not terminology_db or not translated_texts:
            return ""
        
        # å°†æ‰€æœ‰è¯‘æ–‡åˆå¹¶å¹¶è½¬ä¸ºå°å†™ï¼ˆç”¨äºåŒ¹é…ï¼‰
        combined_text = " ".join(translated_texts).lower()
        
        # ç­›é€‰ï¼šæ£€æŸ¥æœ¯è¯­çš„translationæ˜¯å¦å‡ºç°åœ¨è¯‘æ–‡ä¸­
        filtered_terms = []
        for term, info in terminology_db.items():
            translation = info.get("translation", "")
            if translation and translation.lower() in combined_text:
                filtered_terms.append({
                    "term": term,
                    "translation": translation,
                    "info": info.get("info", "")
                })
        
        if not filtered_terms:
            return ""
        
        # âœ… ä½¿ç”¨è¡¨æ ¼æ ¼å¼ï¼ˆåå‘ï¼šè¯‘æ–‡â†’åŸæ–‡ï¼Œç”¨äºå›è¯‘ï¼‰
        if self.config and self.config.target_language in ("chinese_simplified", "chinese_traditional"):
            prompt = "\n###æœ¯è¯­è¡¨ï¼ˆå›è¯‘å‚è€ƒï¼‰\nè¯‘æ–‡|åŸæ–‡|å¤‡æ³¨\n"
        else:
            prompt = "\n###Glossary (Back-translation Reference)\nTranslation|Original Text|Remarks\n"
        
        # æ·»åŠ ç­›é€‰åçš„æœ¯è¯­ï¼ˆæ³¨æ„é¡ºåºï¼šè¯‘æ–‡åœ¨å‰ï¼ŒåŸæ–‡åœ¨åï¼‰
        for item in filtered_terms:
            info_text = item["info"] if item["info"] else " "
            prompt += f"{item['translation']}|{item['term']}|{info_text}\n"
        
        return prompt
    
    def _build_memory_context(self, memory_storage: Dict) -> str:
        """æ„å»ºMemoryä¸Šä¸‹æ–‡"""
        context_parts = []
        
        domain = memory_storage.get("domain", "")
        style = memory_storage.get("style", "")
        
        if domain:
            context_parts.append(f"æ–‡æœ¬é¢†åŸŸï¼š{domain}")
        if style:
            context_parts.append(f"æ–‡æœ¬é£æ ¼ï¼š{style}")
        
        return "\n".join(context_parts) if context_parts else ""
    
    def _extract_translation(self, response: str) -> str:
        """ä»LLMå“åº”ä¸­æå–è¯‘æ–‡"""
        # å»é™¤å¯èƒ½çš„æ ‡è®°å’Œè¯´æ˜
        lines = response.strip().split("\n")
        # å–ç¬¬ä¸€è¡Œæˆ–æœ€é•¿çš„è¡Œä½œä¸ºè¯‘æ–‡
        translation = max(lines, key=len).strip()
        # å»é™¤å¯èƒ½çš„å¼•å·
        translation = translation.strip('"').strip("'")
        return translation
    
    def _update_cache_item(self, item: CacheItem, translated_text: str) -> None:
        """æ›´æ–°ç¼“å­˜é¡¹"""
        item.translated_text = translated_text
        item.translation_status = TranslationStatus.TRANSLATED
    
    def _wait_for_limiter(self, messages: list, system_prompt: str, timeout: int = 300) -> bool:
        """
        ç­‰å¾…RequestLimiterå…è®¸å‘é€è¯·æ±‚ï¼ˆå‚è€ƒåŸTaskExecutorçš„å®ç°ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            True if å¯ä»¥å‘é€è¯·æ±‚, False if è¶…æ—¶
        """
        import time
        from Base.Base import Base
        
        # è®¡ç®—Tokenæ¶ˆè€—
        tokens_consume = self.request_limiter.calculate_tokens(messages, system_prompt)
        
        # ç­‰å¾…é™åˆ¶å™¨å…è®¸
        start_time = time.time()
        while True:
            # æ£€æµ‹æ˜¯å¦æ”¶åˆ°åœæ­¢ç¿»è¯‘äº‹ä»¶
            if Base.work_status == Base.STATUS.STOPING:
                return False
            
            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            if time.time() - start_time >= timeout:
                self.warning(f"ç­‰å¾…RequestLimiterè¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰ï¼Œè·³è¿‡å½“å‰è¯·æ±‚")
                return False
            
            # æ£€æŸ¥RPMå’ŒTPMé™åˆ¶
            if self.request_limiter.check_limiter(tokens_consume):
                return True
            
            # å¦‚æœä»¥ä¸Šæ¡ä»¶éƒ½ä¸ç¬¦åˆï¼Œåˆ™é—´éš”1ç§’å†æ¬¡æ£€æŸ¥
            time.sleep(1)
    
    def _batch_multi_version_fusion(self, source_texts: List[str], initial_translations: List[str],
                                   terminology_db: Dict, memory_storage: Dict) -> Optional[List[str]]:
        """
        æ‰¹é‡å¤šç‰ˆæœ¬ç”Ÿæˆä¸èåˆ
        ä½¿ç”¨ä¸æ‰¹é‡ç¿»è¯‘ç›¸åŒçš„textareaæ ¼å¼ï¼Œåˆ†åˆ«æ‰¹é‡ç”Ÿæˆ3ä¸ªç‰ˆæœ¬ï¼Œç„¶åæ‰¹é‡èåˆ
        """
        self.info(f"    â†’ æ‰¹é‡ç”Ÿæˆ3ä¸ªç‰ˆæœ¬ï¼ˆç›´è¯‘/æ„è¯‘/é£æ ¼åŒ–ï¼‰...")
        
        # ===== æ‰¹é‡ç”Ÿæˆç‰ˆæœ¬1ï¼šç›´è¯‘ç‰ˆ =====
        literal_versions = self._batch_generate_version(source_texts, initial_translations, "literal", terminology_db)
        if not literal_versions:
            self.warning("    âš  ç›´è¯‘ç‰ˆæœ¬æ‰¹é‡ç”Ÿæˆå¤±è´¥")
            literal_versions = initial_translations
        
        # ===== æ‰¹é‡ç”Ÿæˆç‰ˆæœ¬2ï¼šæ„è¯‘ç‰ˆ =====
        free_versions = self._batch_generate_version(source_texts, initial_translations, "free", terminology_db)
        if not free_versions:
            self.warning("    âš  æ„è¯‘ç‰ˆæœ¬æ‰¹é‡ç”Ÿæˆå¤±è´¥")
            free_versions = initial_translations
        
        # ===== æ‰¹é‡ç”Ÿæˆç‰ˆæœ¬3ï¼šé£æ ¼åŒ–ç‰ˆ =====
        style = memory_storage.get("style", "neutral")
        stylized_versions = self._batch_generate_version(source_texts, initial_translations, f"stylized_{style}", terminology_db)
        if not stylized_versions:
            self.warning("    âš  é£æ ¼åŒ–ç‰ˆæœ¬æ‰¹é‡ç”Ÿæˆå¤±è´¥")
            stylized_versions = initial_translations
        
        self.info(f"    âœ“ 3ä¸ªç‰ˆæœ¬æ‰¹é‡ç”Ÿæˆå®Œæˆ")
        
        # ===== æ‰¹é‡æ™ºèƒ½èåˆ =====
        self.info(f"    â†’ æ‰¹é‡æ™ºèƒ½èåˆ3ä¸ªç‰ˆæœ¬...")
        fused_texts = self._batch_fuse_versions(
            source_texts, literal_versions, free_versions, stylized_versions, terminology_db
        )
        
        if not fused_texts:
            self.warning("    âš  æ‰¹é‡èåˆå¤±è´¥ï¼Œä½¿ç”¨åˆå§‹è¯‘æ–‡")
            return initial_translations
        
        self.info(f"    âœ“ æ‰¹é‡èåˆå®Œæˆ: {len(fused_texts)} è¡Œ")
        return fused_texts
    
    def _batch_generate_version(self, source_texts: List[str], initial_translations: List[str],
                               version_type: str, terminology_db: Dict) -> Optional[List[str]]:
        """
        æ‰¹é‡ç”Ÿæˆç‰¹å®šç‰ˆæœ¬çš„ç¿»è¯‘ï¼ˆä½¿ç”¨textareaæ ¼å¼ï¼‰
        """
        version_prompts = {
            "literal": "è¯·æä¾›ç›´è¯‘ç‰ˆæœ¬ï¼Œå°½å¯èƒ½è´´è¿‘åŸæ–‡ç»“æ„",
            "free": "è¯·æä¾›æ„è¯‘ç‰ˆæœ¬ï¼Œæ³¨é‡æµç•…æ€§å’Œè‡ªç„¶åº¦",
            "stylized_formal": "è¯·æä¾›æ­£å¼é£æ ¼çš„ç¿»è¯‘ç‰ˆæœ¬",
            "stylized_informal": "è¯·æä¾›éæ­£å¼é£æ ¼çš„ç¿»è¯‘ç‰ˆæœ¬",
            "stylized_neutral": "è¯·æä¾›ä¸­æ€§é£æ ¼çš„ç¿»è¯‘ç‰ˆæœ¬"
        }
        
        prompt_instruction = version_prompts.get(version_type, "è¯·æä¾›ç¿»è¯‘ç‰ˆæœ¬")
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘ä¸“å®¶ã€‚{prompt_instruction}ã€‚

{self._build_terminology_prompt(terminology_db, [source_text])}

ã€é‡è¦ã€‘è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- å¿…é¡»ä½¿ç”¨<textarea>æ ‡ç­¾åŒ…è£¹æ‰€æœ‰è¯‘æ–‡
- æ¯è¡Œè¯‘æ–‡å‰å¿…é¡»åŠ ä¸Šåºå·ï¼ˆå¦‚1. 2. 3.ï¼‰
- ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„æ ‡é¢˜ã€å‰ç¼€æˆ–è¯´æ˜æ–‡å­—"""
        
        # æ„å»ºsource_text_dictï¼ˆä¸æ‰¹é‡ç¿»è¯‘ç›¸åŒï¼‰
        source_text_dict = {str(i): text for i, text in enumerate(source_texts)}
        
        # æ„å»ºå¸¦åºå·çš„åŸæ–‡ï¼ˆä¸æ‰¹é‡ç¿»è¯‘ç›¸åŒï¼‰
        numbered_lines = []
        for index, line in enumerate(source_texts):
            if "\n" in line:
                lines = line.split("\n")
                numbered_text = f"{index + 1}.[\n"
                total_lines = len(lines)
                for sub_index, sub_line in enumerate(lines):
                    sub_line = sub_line[:-1] if re.match(r'.*[^ ] $', sub_line) else sub_line
                    numbered_text += f'"{index + 1}.{total_lines - sub_index}.,{sub_line}",\n'
                numbered_text = numbered_text.rstrip('\n').rstrip(',')
                numbered_text += f"\n]"
                numbered_lines.append(numbered_text)
            else:
                numbered_lines.append(f"{index + 1}.{line}")
        
        source_text = "\n".join(numbered_lines)
        
        # æ„å»ºuser_prompt
        user_prompt = f"""###å¾…ç¿»è¯‘æ–‡æœ¬
<textarea>
{source_text}
</textarea>

###è¯‘æ–‡è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
<textarea>
ï¼ˆåœ¨è¿™é‡Œè¾“å‡ºå¸¦åºå·çš„è¯‘æ–‡ï¼‰
</textarea>"""
        
        messages = [{"role": "user", "content": user_prompt}]
        
        try:
            if not self._wait_for_limiter(messages, system_prompt):
                return None
            
            platform_config = self.config.get_platform_configuration("translationReq") if self.config else {}
            skip, _, response_content, _, _ = self.llm_requester.sent_request(
                messages=messages,
                system_prompt=system_prompt,
                platform_config=platform_config
            )
            
            if not skip and response_content:
                response_dict = ResponseExtractor.text_extraction(self, source_text_dict, response_content)
                response_dict = ResponseExtractor.remove_numbered_prefix(self, response_dict)
                
                if response_dict:
                    translated_texts = []
                    for i in range(len(source_texts)):
                        key = str(i)
                        if key in response_dict:
                            translated_texts.append(response_dict[key])
                        else:
                            translated_texts.append("")
                    
                    if any(translated_texts):
                        return translated_texts
        except Exception as e:
            self.debug(f"æ‰¹é‡ç”Ÿæˆ{version_type}ç‰ˆæœ¬å¤±è´¥: {e}")
        
        return None
    
    def _batch_fuse_versions(self, source_texts: List[str], literal_versions: List[str],
                           free_versions: List[str], stylized_versions: List[str],
                           terminology_db: Dict) -> Optional[List[str]]:
        """
        æ‰¹é‡æ™ºèƒ½èåˆå¤šä¸ªç‰ˆæœ¬ï¼ˆä½¿ç”¨textareaæ ¼å¼ï¼‰
        """
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹å¤šä¸ªç¿»è¯‘ç‰ˆæœ¬ï¼Œå¹¶èåˆç”Ÿæˆæœ€ä½³è¯‘æ–‡ã€‚

è¯„ä¼°æ ‡å‡†ï¼š
1. è¯­ä¹‰å‡†ç¡®æ€§
2. æµç•…æ€§
3. é£æ ¼ä¸€è‡´æ€§
4. æœ¯è¯­ä½¿ç”¨è§„èŒƒæ€§

ã€é‡è¦ã€‘è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- å¿…é¡»ä½¿ç”¨<textarea>æ ‡ç­¾åŒ…è£¹æ‰€æœ‰è¯‘æ–‡
- æ¯è¡Œè¯‘æ–‡å‰å¿…é¡»åŠ ä¸Šåºå·ï¼ˆå¦‚1. 2. 3.ï¼‰
- ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„æ ‡é¢˜ã€å‰ç¼€æˆ–è¯´æ˜æ–‡å­—"""
        
        # æ„å»ºæ‰¹é‡èåˆçš„è¾“å…¥
        source_text_dict = {str(i): text for i, text in enumerate(source_texts)}
        
        # æ„å»ºå¸¦ç‰ˆæœ¬çš„åŸæ–‡
        numbered_blocks = []
        for i, (src, lit, free, sty) in enumerate(zip(source_texts, literal_versions, free_versions, stylized_versions)):
            block = f"""{i + 1}.åŸæ–‡: {src}
   ç›´è¯‘ç‰ˆ: {lit}
   æ„è¯‘ç‰ˆ: {free}
   é£æ ¼åŒ–ç‰ˆ: {sty}"""
            numbered_blocks.append(block)
        
        versions_text = "\n\n".join(numbered_blocks)
        
        user_prompt = f"""###å¤šç‰ˆæœ¬ç¿»è¯‘ç»“æœ
{versions_text}

###è¯·è¯„ä¼°å¹¶èåˆï¼Œè¾“å‡ºæœ€ä½³è¯‘æ–‡
<textarea>
ï¼ˆåœ¨è¿™é‡Œè¾“å‡ºå¸¦åºå·çš„æœ€ä½³è¯‘æ–‡ï¼‰
</textarea>"""
        
        messages = [{"role": "user", "content": user_prompt}]
        
        try:
            if not self._wait_for_limiter(messages, system_prompt):
                return None
            
            platform_config = self.config.get_platform_configuration("translationReq") if self.config else {}
            skip, _, response_content, _, _ = self.llm_requester.sent_request(
                messages=messages,
                system_prompt=system_prompt,
                platform_config=platform_config
            )
            
            if not skip and response_content:
                response_dict = ResponseExtractor.text_extraction(self, source_text_dict, response_content)
                response_dict = ResponseExtractor.remove_numbered_prefix(self, response_dict)
                
                if response_dict:
                    fused_texts = []
                    for i in range(len(source_texts)):
                        key = str(i)
                        if key in response_dict:
                            fused_texts.append(response_dict[key])
                        else:
                            fused_texts.append("")
                    
                    if any(fused_texts):
                        return fused_texts
        except Exception as e:
            self.debug(f"æ‰¹é‡èåˆå¤±è´¥: {e}")
        
        return None
    
    def _unified_human_review(self, all_chunks_data: List[Dict], terminology_db: Dict, task_memory: Dict, workflow_config: Dict = None) -> List[Dict]:
        """
        ç»Ÿä¸€äººå·¥å®¡æ ¸ï¼ˆæ‰€æœ‰æ‰¹æ¬¡å®Œæˆåï¼‰
        
        Args:
            all_chunks_data: æ‰€æœ‰æ‰¹æ¬¡çš„æ•°æ®åˆ—è¡¨
            terminology_db: æœ¯è¯­åº“
            task_memory: ä»»åŠ¡å…ƒæ•°æ®
            workflow_config: å·¥ä½œæµé…ç½®
            
        Returns:
            æ›´æ–°åçš„æ‰¹æ¬¡æ•°æ®åˆ—è¡¨
        """
        if workflow_config is None:
            workflow_config = {}
            
        enable_human_review = workflow_config.get("enable_human_review", False)
        review_threshold = workflow_config.get("review_threshold", 0.8)
        
        self.info(f"äººå·¥å®¡æ ¸æ£€æŸ¥: enable_human_review={enable_human_review}, callback={self._human_intervention_callback is not None}")
        
        if not enable_human_review or not self._human_intervention_callback:
            self.info("    âš  äººå·¥å®¡æ ¸æœªå¯ç”¨æˆ–å›è°ƒä¸å¯ç”¨ï¼Œè·³è¿‡äººå·¥å®¡æ ¸")
            return all_chunks_data
        
        # ğŸ”¥ æ”¶é›†å…¨æ–‡çš„è¯„åˆ†æ•°æ®
        all_scores_with_index = []  # [(global_index, chunk_idx, local_idx, score, source, translated, back)]
        global_index = 0
        
        for chunk_idx, chunk_data in enumerate(all_chunks_data):
            source_texts = chunk_data.get("source_texts", [])
            translated_texts = chunk_data.get("translated_texts", [])
            quality_scores = chunk_data.get("quality_scores", [])
            back_translations = chunk_data.get("back_translations", [])
            chunk_items = chunk_data.get("chunk_items", []) # è·å–åŸå§‹CacheItemåˆ—è¡¨
            
            file_path = chunk_data.get("file_path", "") # ğŸ”¥ ä» chunk_data è·å– file_path
            
            for local_idx, (src, trans, score, back) in enumerate(zip(source_texts, translated_texts, quality_scores, back_translations)):
                # è·å–å¯¹åº”çš„CacheItemä¿¡æ¯
                row_index = -1
                if local_idx < len(chunk_items):
                    # file_path = chunk_items[local_idx].file_path # CacheItem æ²¡æœ‰ file_path
                    row_index = chunk_items[local_idx].row_index

                all_scores_with_index.append({
                    "global_index": global_index,
                    "chunk_idx": chunk_idx,
                    "local_idx": local_idx,
                    "score": score,
                    "source_text": src,
                    "translated_text": trans,
                    "back_translation": back,
                    "file_path": file_path,
                    "row_index": row_index
                })
                global_index += 1
        
        # ğŸ”¥ æŒ‰è¯„åˆ†æ’åºï¼Œæ‰¾å‡ºæœ€ä½çš„3ä¸ª
        all_scores_with_index.sort(key=lambda x: x["score"])
        lowest_3 = all_scores_with_index[:min(3, len(all_scores_with_index))]
        
        self.info(f"    ğŸ”” å¯ç”¨äººå·¥å®¡æ ¸ï¼šå±•ç¤ºè¯„åˆ†æœ€ä½çš„3ä¸ªä¾›å®¡æ ¸ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
        
        # å‡†å¤‡å®¡æ ¸é¡¹ç›®
        review_items = []
        for item in lowest_3:
            # å‡†å¤‡ä¸Šä¸‹æ–‡ï¼ˆå‰åå„1è¡Œï¼‰
            global_idx = item["global_index"]
            context_before = all_scores_with_index[global_idx - 1]["source_text"] if global_idx > 0 else ""
            context_after = all_scores_with_index[global_idx + 1]["source_text"] if global_idx < len(all_scores_with_index) - 1 else ""
            
            review_items.append({
                "index": global_idx,  # å…¨å±€ç´¢å¼•
                "chunk_idx": item["chunk_idx"],
                "local_idx": item["local_idx"],
                "source_text": item["source_text"],
                "translated_text": item["translated_text"],
                "back_translation": item["back_translation"],
                "score": item["score"],
                "context_before": context_before,
                "context_after": context_after,
                "file_path": item.get("file_path"),
                "row_index": item.get("row_index")
            })
            self.info(f"      å…¨å±€è¡Œ{global_idx+1}: è¯„åˆ† {item['score']:.1f}/10ï¼ˆæœ€ä½3ä¸ªä¹‹ä¸€ï¼‰")
        
        # è°ƒç”¨äººå·¥å®¡æ ¸
        review_result = self._human_intervention_callback(
            "batch_translation_review",
            {
                "review_items": review_items,
                "review_threshold": review_threshold
            }
        )
        
        # å¤„ç†äººå·¥å®¡æ ¸ç»“æœ
        if review_result and review_result.get("review_results"):
            self.info(f"    âœ“ äººå·¥å®¡æ ¸å®Œæˆï¼Œåº”ç”¨ç”¨æˆ·å†³ç­–...")
            
            for user_decision in review_result["review_results"]:
                global_idx = user_decision["index"]
                action = user_decision["action"]
                
                # æ‰¾åˆ°å¯¹åº”çš„æ‰¹æ¬¡å’Œæœ¬åœ°ç´¢å¼•
                target_item = None
                for item in review_items:
                    if item["index"] == global_idx:
                        target_item = item
                        break
                
                if not target_item:
                    continue
                
                chunk_idx = target_item["chunk_idx"]
                local_idx = target_item["local_idx"]
                chunk_data = all_chunks_data[chunk_idx]
                
                if action == "accept":
                    self.info(f"      å…¨å±€è¡Œ{global_idx+1}: ç”¨æˆ·æ¥å— âœ…")
                    # æ— éœ€ä¿®æ”¹
                    
                elif action == "custom":
                    # ä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰ç¿»è¯‘
                    custom_translation = user_decision["translation"]
                    chunk_data["translated_texts"][local_idx] = custom_translation
                    self.info(f"      å…¨å±€è¡Œ{global_idx+1}: ä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰ç¿»è¯‘ âœï¸")
                    
                    # [DB] è®°å½•äººå·¥ä¿®æ”¹ (Human Edit Trace)
                    if self.db_manager and self._current_cache_project:
                        atom_id = self._get_atom_id(self._current_cache_project, target_item["file_path"], target_item["row_index"])
                        if atom_id:
                            self.db_manager.add_trace(
                                atom_id=atom_id,
                                agent_role="Human",
                                action_type="human_edit",
                                content=custom_translation,
                                meta_data={"note": "User custom translation in review dialog"}
                            )
                            # æ›´æ–°åŸå­ç¿»è¯‘ç»“æœ
                            self.db_manager.update_atom_translation(
                                atom_id=atom_id,
                                translated_text=custom_translation,
                                status_code=3  # å·²äººå·¥å®¡æ ¸
                            )
                    
                elif action == "retranslate":
                    # éœ€è¦LLMé‡æ–°ç¿»è¯‘
                    self.info(f"      å…¨å±€è¡Œ{global_idx+1}: æ ‡è®°ä¸ºéœ€è¦é‡æ–°ç¿»è¯‘ ğŸ”„")
                    source_text = target_item["source_text"]
                    # ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨å•è¡Œç¿»è¯‘
                    new_translation = self._translate_single_line(
                        source_text, [], "literal", terminology_db, {}
                    )
                    if new_translation:
                        chunk_data["translated_texts"][local_idx] = new_translation
                        self.info(f"      å…¨å±€è¡Œ{global_idx+1}: é‡æ–°ç¿»è¯‘å®Œæˆ")
                        
                        # [DB] è®°å½•é‡æ–°ç¿»è¯‘ (Refine Trace)
                        if self.db_manager and self._current_cache_project:
                            atom_id = self._get_atom_id(self._current_cache_project, target_item["file_path"], target_item["row_index"])
                            if atom_id:
                                self.db_manager.add_trace(
                                    atom_id=atom_id,
                                    agent_role="Translator",
                                    action_type="refine",
                                    content=new_translation,
                                    meta_data={"reason": "User requested retranslation"}
                                )
                                # æ›´æ–°åŸå­ç¿»è¯‘ç»“æœ
                                self.db_manager.update_atom_translation(
                                    atom_id=atom_id,
                                    translated_text=new_translation,
                                    status_code=2  # å·²æ¶¦è‰²
                                )
                    else:
                        self.warning(f"      å…¨å±€è¡Œ{global_idx+1}: é‡æ–°ç¿»è¯‘å¤±è´¥ï¼Œä¿ç•™åŸè¯‘æ–‡")
        else:
            self.info(f"    âš  ç”¨æˆ·å–æ¶ˆå®¡æ ¸ï¼Œç»§ç»­è‡ªåŠ¨æµç¨‹")
        
        return all_chunks_data
    
    def _unified_entity_check(self, all_chunks_data: List[Dict], terminology_db: Dict, entity_database: Dict) -> List[Dict]:
        """
        ç»Ÿä¸€å®ä½“ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆäººå·¥å®¡æ ¸åï¼‰
        
        Args:
            all_chunks_data: æ‰€æœ‰æ‰¹æ¬¡çš„æ•°æ®åˆ—è¡¨
            terminology_db: æœ¯è¯­åº“
            entity_database: å®ä½“æ•°æ®åº“
            
        Returns:
            æ›´æ–°åçš„æ‰¹æ¬¡æ•°æ®åˆ—è¡¨
        """
        for chunk_idx, chunk_data in enumerate(all_chunks_data):
            source_texts = chunk_data.get("source_texts", [])
            translated_texts = chunk_data.get("translated_texts", [])
            
            self.info(f"  â†’ æ£€æŸ¥æ‰¹æ¬¡ {chunk_idx+1}/{len(all_chunks_data)}...")
            
            # ğŸ”¥ å‘é€UIé˜¶æ®µæ›´æ–°ï¼šä¸€è‡´æ€§æ£€æŸ¥é˜¶æ®µï¼ˆä»…ç¬¬ä¸€æ¬¡ï¼‰
            if chunk_idx == 0 and self._current_cache_project and not hasattr(self, '_entity_check_stage_sent'):
                self._update_stage_progress(self._current_cache_project, "entity_check", 0, 1)
                self._publish_stage_with_stats(self._current_cache_project, "entity_check", "æ£€æŸ¥ä¸­")
                self._entity_check_stage_sent = True
            
            checked_texts = self._check_entity_consistency(
                source_texts, translated_texts, terminology_db, entity_database
            )
            
            # [DB] Phase 3: è®°å½•æœ€ç»ˆä¸€è‡´æ€§æ£€æŸ¥è½¨è¿¹ (Final Trace)
            # ä»…è®°å½•è¢«å®ä½“æ£€æŸ¥ä¿®æ”¹è¿‡çš„è¡Œ
            if self.db_manager and self._current_cache_project:
                chunk_items = chunk_data.get("chunk_items", [])
                file_path = chunk_data.get("file_path", "")
                
                for idx, (old_text, new_text) in enumerate(zip(translated_texts, checked_texts)):
                    if old_text != new_text and idx < len(chunk_items):
                        item = chunk_items[idx]
                        atom_id = self._get_atom_id(self._current_cache_project, file_path, item.row_index)
                        if atom_id:
                            self.db_manager.add_trace(
                                atom_id=atom_id,
                                agent_role="ConsistencyChecker",
                                action_type="final",
                                content=new_text,
                                meta_data={"reason": "Entity consistency check", "before": old_text}
                            )
                            # æ›´æ–°åŸå­ä¸ºæœ€ç»ˆè¯‘æ–‡
                            self.db_manager.update_atom_translation(
                                atom_id=atom_id,
                                translated_text=new_text,
                                status_code=4  # å·²å®Œæˆ
                            )

            chunk_data["translated_texts"] = checked_texts
        
        # å®Œæˆä¸€è‡´æ€§æ£€æŸ¥
        if self._current_cache_project:
            self._update_stage_progress(self._current_cache_project, "entity_check", 1, 1)
        
        self.info(f"  âœ“ æ‰€æœ‰æ‰¹æ¬¡å®ä½“ä¸€è‡´æ€§æ£€æŸ¥å®Œæˆ")
        return all_chunks_data
    
    def _batch_tear_verification_with_scores(self, source_texts: List[str], translated_texts: List[str],
                                terminology_db: Dict) -> tuple[Optional[List[str]], List[float], List[str]]:
        """
        æ‰¹é‡å›è¯‘éªŒè¯ï¼ˆè¿”å›è¯„åˆ†æ•°æ®ï¼Œä¸è¿›è¡Œäººå·¥å®¡æ ¸ï¼‰
        
        Returns:
            (verified_texts, quality_scores, back_translations)
        """
        # ğŸ”¥ å‘é€UIé˜¶æ®µæ›´æ–°ï¼šå›è¯‘è¯„ä¼°é˜¶æ®µï¼ˆä»…ç¬¬ä¸€æ¬¡ï¼‰
        if self._current_cache_project and not hasattr(self, '_backtranslation_stage_sent'):
            self._update_stage_progress(self._current_cache_project, "backtranslation", 0, 3)
            self._publish_stage_with_stats(self._current_cache_project, "backtranslation", "å›è¯‘ä¸­")
            self._backtranslation_stage_sent = True
        
        # ===== æ‰¹é‡å›è¯‘ =====
        self.info(f"    â†’ æ‰¹é‡å›è¯‘...")
        self._update_stage_progress(self._current_cache_project, "backtranslation", 1, 3)
        back_translations = self._batch_back_translate(translated_texts, terminology_db)
        
        if not back_translations:
            self.warning("    âš  æ‰¹é‡å›è¯‘å¤±è´¥ï¼Œè·³è¿‡TEaRéªŒè¯")
            return translated_texts, [8.0] * len(translated_texts), [""] * len(translated_texts)
        
        self.info(f"    âœ“ æ‰¹é‡å›è¯‘å®Œæˆ: {len(back_translations)} è¡Œ")
        
        # ===== æ‰¹é‡è´¨é‡è¯„ä¼° =====
        self.info(f"    â†’ æ‰¹é‡è´¨é‡è¯„ä¼°...")
        self._update_stage_progress(self._current_cache_project, "backtranslation", 2, 3)
        needs_refinement, quality_scores = self._batch_estimate_quality(source_texts, translated_texts, back_translations)
        
        # ç»Ÿè®¡éœ€è¦ä¿®æ­£çš„æ•°é‡
        refine_count = sum(1 for need in needs_refinement if need)
        
        # ===== æ‰¹é‡ä¿®æ­£ï¼ˆä»…å¯¹éœ€è¦ä¿®æ­£çš„ï¼‰ =====
        self._update_stage_progress(self._current_cache_project, "backtranslation", 3, 3)
        if refine_count == 0:
            self.info(f"    âœ“ æ‰€æœ‰è¯‘æ–‡è´¨é‡è‰¯å¥½ï¼Œæ— éœ€ä¿®æ­£")
            return translated_texts, quality_scores, back_translations
        
        self.info(f"    â†’ æ‰¹é‡ä¿®æ­£ {refine_count} è¡Œ...")
        refined_texts = self._batch_refine_translation(
            source_texts, translated_texts, back_translations, needs_refinement, terminology_db
        )
        
        if not refined_texts:
            self.warning("    âš  æ‰¹é‡ä¿®æ­£å¤±è´¥ï¼Œä½¿ç”¨åŸè¯‘æ–‡")
            return translated_texts, quality_scores, back_translations
        
        self.info(f"    âœ“ æ‰¹é‡ä¿®æ­£å®Œæˆ")
        return refined_texts, quality_scores, back_translations
    
    def _batch_tear_verification(self, source_texts: List[str], translated_texts: List[str],
                                terminology_db: Dict, human_intervention_callback=None) -> Optional[List[str]]:
        """
        æ‰¹é‡å›è¯‘éªŒè¯ï¼ˆTEaR: Translate, Estimate, and Refineï¼‰
        
        Args:
            source_texts: åŸæ–‡åˆ—è¡¨
            translated_texts: è¯‘æ–‡åˆ—è¡¨
            terminology_db: æœ¯è¯­åº“
            human_intervention_callback: äººå·¥ä»‹å…¥å›è°ƒå‡½æ•°
        """
        # ğŸ”¥ å‘é€UIé˜¶æ®µæ›´æ–°ï¼šå›è¯‘è¯„ä¼°é˜¶æ®µï¼ˆä»…ç¬¬ä¸€æ¬¡ï¼‰
        if self._current_cache_project and not hasattr(self, '_backtranslation_stage_sent'):
            # å›è¯‘é˜¶æ®µï¼šæ€»å…±3æ­¥ï¼ˆå›è¯‘ã€è¯„ä¼°ã€ä¿®æ­£ï¼‰
            self._update_stage_progress(self._current_cache_project, "backtranslation", 0, 3)
            self._publish_stage_with_stats(self._current_cache_project, "backtranslation", "å›è¯‘ä¸­")
            self._backtranslation_stage_sent = True
        
        # ===== æ‰¹é‡å›è¯‘ =====
        self.info(f"    â†’ æ‰¹é‡å›è¯‘...")
        self._update_stage_progress(self._current_cache_project, "backtranslation", 1, 3)  # ç¬¬1æ­¥
        back_translations = self._batch_back_translate(translated_texts, terminology_db)
        
        if not back_translations:
            self.warning("    âš  æ‰¹é‡å›è¯‘å¤±è´¥ï¼Œè·³è¿‡TEaRéªŒè¯")
            return translated_texts
        
        self.info(f"    âœ“ æ‰¹é‡å›è¯‘å®Œæˆ: {len(back_translations)} è¡Œ")
        
        # ===== æ‰¹é‡è´¨é‡è¯„ä¼° =====
        self.info(f"    â†’ æ‰¹é‡è´¨é‡è¯„ä¼°...")
        self._update_stage_progress(self._current_cache_project, "backtranslation", 2, 3)  # ç¬¬2æ­¥
        needs_refinement, quality_scores = self._batch_estimate_quality(source_texts, translated_texts, back_translations)
        
        # ç»Ÿè®¡éœ€è¦ä¿®æ­£çš„æ•°é‡
        refine_count = sum(1 for need in needs_refinement if need)
        # æ³¨æ„ï¼šè¯¦ç»†è¯„åˆ†å·²åœ¨ _batch_estimate_quality ä¸­æ˜¾ç¤º
        
        # ===== äººåœ¨å›è·¯ï¼šäººå·¥å®¡æ ¸ï¼ˆå¦‚æœå¯ç”¨ï¼‰ =====
        if human_intervention_callback:
            # è·å–å·¥ä½œæµé…ç½®
            workflow_config = {}
            if hasattr(self, '_workflow_state') and self._workflow_state:
                workflow_config = self._workflow_state.get("workflow_config", {})
            
            enable_human_review = workflow_config.get("enable_human_review", False)
            review_threshold = workflow_config.get("review_threshold", 0.8)
            
            if enable_human_review:
                # å‡†å¤‡éœ€è¦å®¡æ ¸çš„é¡¹ç›®
                review_items = []
                
                if refine_count > 0:
                    # æƒ…å†µ1ï¼šæœ‰è¯„åˆ†ä½äº7.0çš„è¡Œï¼Œå®¡æ ¸è¿™äº›è¡Œ
                    self.info(f"    ğŸ”” å¯ç”¨äººå·¥å®¡æ ¸ï¼š{refine_count} è¡Œè¯„åˆ†ä½äºé˜ˆå€¼ {review_threshold*10:.0f}/10")
                    
                    for i, (need, score) in enumerate(zip(needs_refinement, quality_scores)):
                        if need:  # score < 7.0
                            # å‡†å¤‡ä¸Šä¸‹æ–‡ï¼ˆå‰åå„1è¡Œï¼‰
                            context_before = source_texts[i-1] if i > 0 else ""
                            context_after = source_texts[i+1] if i < len(source_texts) - 1 else ""
                            
                            review_items.append({
                                "index": i,
                                "source_text": source_texts[i],
                                "translated_text": translated_texts[i],
                                "back_translation": back_translations[i],
                                "score": score,
                                "context_before": context_before,
                                "context_after": context_after
                            })
                else:
                    # æƒ…å†µ2ï¼šæ‰€æœ‰è¯„åˆ†éƒ½å¾ˆé«˜ï¼Œä½†ä¸ºäº†æµ‹è¯•/å±•ç¤ºï¼Œé€‰æ‹©è¯„åˆ†æœ€ä½çš„3ä¸ª
                    self.info(f"    ğŸ”” å¯ç”¨äººå·¥å®¡æ ¸ï¼šæ‰€æœ‰è¯‘æ–‡è¯„åˆ†è‰¯å¥½ï¼Œå±•ç¤ºè¯„åˆ†æœ€ä½çš„3ä¸ªä¾›å®¡æ ¸ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
                    
                    # æŒ‰è¯„åˆ†æ’åºï¼Œæ‰¾å‡ºæœ€ä½çš„3ä¸ª
                    scored_items = [(i, score) for i, score in enumerate(quality_scores)]
                    scored_items.sort(key=lambda x: x[1])  # æŒ‰è¯„åˆ†ä»ä½åˆ°é«˜æ’åº
                    lowest_3 = scored_items[:min(3, len(scored_items))]
                    
                    for i, score in lowest_3:
                        context_before = source_texts[i-1] if i > 0 else ""
                        context_after = source_texts[i+1] if i < len(source_texts) - 1 else ""
                        
                        review_items.append({
                            "index": i,
                            "source_text": source_texts[i],
                            "translated_text": translated_texts[i],
                            "back_translation": back_translations[i],
                            "score": score,
                            "context_before": context_before,
                            "context_after": context_after
                        })
                        self.info(f"      è¡Œ{i+1}: è¯„åˆ† {score:.1f}/10ï¼ˆæœ€ä½3ä¸ªä¹‹ä¸€ï¼‰")
                
                if review_items:
                    # è°ƒç”¨äººå·¥å®¡æ ¸
                    review_result = human_intervention_callback(
                        "batch_translation_review",
                        {
                            "review_items": review_items,
                            "review_threshold": review_threshold
                        }
                    )
                    
                    # å¤„ç†äººå·¥å®¡æ ¸ç»“æœ
                    if review_result and review_result.get("review_results"):
                        translated_texts = self._apply_human_review_results(
                            source_texts, translated_texts, back_translations, 
                            needs_refinement, quality_scores, review_result["review_results"],
                            terminology_db
                        )
                        self.info(f"    âœ“ äººå·¥å®¡æ ¸å®Œæˆï¼Œå·²åº”ç”¨ç”¨æˆ·å†³ç­–")
                        self._update_stage_progress(self._current_cache_project, "backtranslation", 3, 3)
                        return translated_texts
                    else:
                        self.info(f"    âš  ç”¨æˆ·å–æ¶ˆå®¡æ ¸ï¼Œç»§ç»­è‡ªåŠ¨æµç¨‹")
        
        # ===== æ‰¹é‡ä¿®æ­£ï¼ˆä»…å¯¹éœ€è¦ä¿®æ­£çš„ï¼‰ =====
        self._update_stage_progress(self._current_cache_project, "backtranslation", 3, 3)  # ç¬¬3æ­¥
        if refine_count == 0:
            self.info(f"    âœ“ æ‰€æœ‰è¯‘æ–‡è´¨é‡è‰¯å¥½ï¼Œæ— éœ€ä¿®æ­£")
            return translated_texts
        
        self.info(f"    â†’ æ‰¹é‡ä¿®æ­£ {refine_count} è¡Œ...")
        refined_texts = self._batch_refine_translation(
            source_texts, translated_texts, back_translations, needs_refinement, terminology_db
        )
        
        if not refined_texts:
            self.warning("    âš  æ‰¹é‡ä¿®æ­£å¤±è´¥ï¼Œä½¿ç”¨åŸè¯‘æ–‡")
            return translated_texts
        
        self.info(f"    âœ“ æ‰¹é‡ä¿®æ­£å®Œæˆ")
        return refined_texts
    
    def _apply_human_review_results(self, source_texts: List[str], translated_texts: List[str],
                                    back_translations: List[str], needs_refinement: List[bool],
                                    quality_scores: List[float], review_results: List[Dict],
                                    terminology_db: Dict) -> List[str]:
        """
        åº”ç”¨äººå·¥å®¡æ ¸ç»“æœ
        
        Args:
            source_texts: åŸæ–‡åˆ—è¡¨
            translated_texts: è¯‘æ–‡åˆ—è¡¨
            back_translations: å›è¯‘åˆ—è¡¨
            needs_refinement: éœ€è¦ä¿®æ­£çš„æ ‡è®°
            quality_scores: è´¨é‡è¯„åˆ†åˆ—è¡¨
            review_results: ç”¨æˆ·å®¡æ ¸ç»“æœ [{"index": int, "action": str, "translation": str/None}]
            terminology_db: æœ¯è¯­åº“
            
        Returns:
            æ›´æ–°åçš„è¯‘æ–‡åˆ—è¡¨
        """
        result_texts = list(translated_texts)  # å¤åˆ¶è¯‘æ–‡åˆ—è¡¨
        
        # æ„å»ºç´¢å¼•åˆ°å®¡æ ¸ç»“æœçš„æ˜ å°„
        review_map = {item["index"]: item for item in review_results}
        
        # æ”¶é›†éœ€è¦LLMé‡æ–°ç¿»è¯‘çš„è¡Œ
        to_retranslate_indices = []
        
        for i, need in enumerate(needs_refinement):
            if not need:  # ä¸éœ€è¦ä¿®æ­£çš„è¡Œï¼Œä¿æŒåŸæ ·
                continue
            
            if i in review_map:
                review = review_map[i]
                action = review["action"]
                
                if action == "accept":
                    # ç”¨æˆ·æ¥å—å½“å‰è¯‘æ–‡ï¼Œæ— éœ€ä¿®æ”¹
                    self.info(f"      è¡Œ{i+1}: ç”¨æˆ·æ¥å— âœ…")
                    pass
                
                elif action == "custom":
                    # ç”¨æˆ·æä¾›äº†è‡ªå®šä¹‰ç¿»è¯‘
                    custom_translation = review["translation"]
                    result_texts[i] = custom_translation
                    self.info(f"      è¡Œ{i+1}: ä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰ç¿»è¯‘ âœï¸")
                
                elif action == "retranslate":
                    # éœ€è¦LLMé‡æ–°ç¿»è¯‘
                    to_retranslate_indices.append(i)
                    self.info(f"      è¡Œ{i+1}: æ ‡è®°ä¸ºéœ€è¦é‡æ–°ç¿»è¯‘ ğŸ”„")
        
        # æ‰¹é‡é‡æ–°ç¿»è¯‘
        if to_retranslate_indices:
            self.info(f"    â†’ æ­£åœ¨é‡æ–°ç¿»è¯‘ {len(to_retranslate_indices)} è¡Œ...")
            
            to_retranslate_sources = [source_texts[i] for i in to_retranslate_indices]
            to_retranslate_translations = [translated_texts[i] for i in to_retranslate_indices]
            to_retranslate_backs = [back_translations[i] for i in to_retranslate_indices]
            
            # ä½¿ç”¨_batch_refine_translationé‡æ–°ç¿»è¯‘
            needs_refinement_subset = [True] * len(to_retranslate_indices)
            refined_subset = self._batch_refine_translation(
                to_retranslate_sources,
                to_retranslate_translations,
                to_retranslate_backs,
                needs_refinement_subset,
                terminology_db
            )
            
            if refined_subset:
                # å°†é‡æ–°ç¿»è¯‘çš„ç»“æœæ”¾å›åŸä½ç½®
                for idx, refined_text in zip(to_retranslate_indices, refined_subset):
                    result_texts[idx] = refined_text
                self.info(f"    âœ“ é‡æ–°ç¿»è¯‘å®Œæˆ")
            else:
                self.warning(f"    âš  é‡æ–°ç¿»è¯‘å¤±è´¥ï¼Œä¿ç•™åŸè¯‘æ–‡")
        
        return result_texts
    
    def _extract_by_line_number(self, text: str) -> Dict[int, str]:
        """
        é²æ£’çš„æŒ‰è¡Œå·æå–æ–‡æœ¬ (è¾…åŠ©æ–¹æ³•)
        è§£å†³ResponseExtractoræŒ‰é¡ºåºæå–å¯¼è‡´çš„é”™ä½é—®é¢˜
        """
        results = {}
        # ç§»é™¤textareaæ ‡ç­¾
        text = re.sub(r'<textarea.*?>', '', text, flags=re.IGNORECASE)
        text = re.sub(r'</textarea>', '', text, flags=re.IGNORECASE)
        text = text.strip()
        
        # æŒ‰ "æ¢è¡Œ+æ•°å­—+ç‚¹/é¡¿å·" åˆ†å‰²
        # ä½¿ç”¨splitä¿ç•™åˆ†éš”ç¬¦ä¸­çš„æ•°å­—
        parts = re.split(r'(?:^|\n)\s*(\d+)[.ã€]\s*', text)
        
        # splitç»“æœ: [å‰ç¼€, æ•°å­—1, å†…å®¹1, æ•°å­—2, å†…å®¹2, ...]
        for i in range(1, len(parts), 2):
            if i + 1 < len(parts):
                try:
                    num_str = parts[i]
                    content = parts[i+1].strip()
                    idx = int(num_str) - 1 # è½¬ä¸º0-basedç´¢å¼•
                    if idx >= 0:
                        results[idx] = content
                except:
                    pass
        return results
    
    def _batch_back_translate(self, translated_texts: List[str], terminology_db: Dict) -> Optional[List[str]]:
        """
        æ‰¹é‡å›è¯‘ï¼ˆä½¿ç”¨textareaæ ¼å¼ï¼‰
        
        Args:
            translated_texts: è¯‘æ–‡åˆ—è¡¨
            terminology_db: æœ¯è¯­åº“ï¼ˆç¡®ä¿å›è¯‘æ—¶ä½¿ç”¨ç›¸åŒçš„å®ä½“ç¿»è¯‘ï¼‰
        
        Returns:
            å›è¯‘ç»“æœåˆ—è¡¨
        """
        source_lang = self.config.source_language if self.config else "chinese"
        target_lang = self.config.target_language if self.config else "english"
        
        # æ„å»ºæœ¯è¯­æç¤ºï¼ˆâœ… ä¼ é€’translated_textsç”¨äºç­›é€‰ï¼Œæ£€æŸ¥è¯‘æ–‡ä¸­æ˜¯å¦åŒ…å«æœ¯è¯­çš„ç¿»è¯‘ï¼‰
        terminology_prompt = self._build_terminology_prompt_for_backtranslation(terminology_db, translated_texts)
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å›è¯‘ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹{target_lang}æ–‡æœ¬ç²¾ç¡®å›è¯‘ä¸º{source_lang}ã€‚

{terminology_prompt}

ğŸ”¥ã€å¼ºåˆ¶è¦æ±‚-æœ¯è¯­è¡¨ä¸¥æ ¼éµå®ˆã€‘ğŸ”¥
- æœ¯è¯­è¡¨ä¸­åˆ—å‡ºçš„æ‰€æœ‰è¯‘æ–‡ï¼Œå¿…é¡»å›è¯‘ä¸ºå¯¹åº”çš„åŸæ–‡æœ¯è¯­ï¼Œç»ä¸å…è®¸æ›¿æ¢æˆ–æ”¹å†™
- è¿™æ˜¯å¼ºåˆ¶æ€§è¦æ±‚ï¼Œä¸å¯è¿å
- ä¾‹å¦‚ï¼šå¦‚æœæœ¯è¯­è¡¨è§„å®š"ç£·è„‚é…°è‚Œé†‡"å¿…é¡»å›è¯‘ä¸º"phosphatidylinositol"ï¼Œåˆ™ç»å¯¹ä¸èƒ½å›è¯‘ä¸º"phospholipid inositol"æˆ–å…¶ä»–ä»»ä½•å˜ä½“
- ä¾‹å¦‚ï¼šå¦‚æœæœ¯è¯­è¡¨è§„å®š"Beclin"å¿…é¡»å›è¯‘ä¸º"Beclin"ï¼Œåˆ™å¿…é¡»ä¿æŒä¸å˜
- ä¾‹å¦‚ï¼šå¦‚æœæœ¯è¯­è¡¨è§„å®š"è‡ªå™¬"å¿…é¡»å›è¯‘ä¸º"autophagy"ï¼Œåˆ™ç»å¯¹ä¸èƒ½å›è¯‘ä¸º"self-phagocytosis"æˆ–å…¶ä»–ä»»ä½•æ›¿ä»£è¯
- æœ¯è¯­è¡¨çš„å›è¯‘è§„åˆ™ä¼˜å…ˆçº§æœ€é«˜ï¼Œé«˜äºä»»ä½•è¯­è¨€ä¹ æƒ¯æˆ–åŒä¹‰è¯

ã€å›è¯‘ç›®çš„ã€‘
- å›è¯‘æ˜¯ä¸ºäº†éªŒè¯æ­£å‘ç¿»è¯‘çš„å‡†ç¡®æ€§
- å¦‚æœå›è¯‘æ— æ³•è¿˜åŸåŸæ–‡æœ¯è¯­ï¼Œè¯´æ˜æ­£å‘ç¿»è¯‘å¯èƒ½æœ‰è¯¯
- å› æ­¤ï¼Œæœ¯è¯­çš„å›è¯‘å¿…é¡»100%å‡†ç¡®

ã€é‡è¦ã€‘è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- å¿…é¡»ä½¿ç”¨<textarea>æ ‡ç­¾åŒ…è£¹æ‰€æœ‰å›è¯‘ç»“æœ
- æ¯è¡Œå›è¯‘å‰å¿…é¡»åŠ ä¸Šåºå·ï¼ˆå¦‚1. 2. 3.ï¼‰
- ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„æ ‡é¢˜ã€å‰ç¼€æˆ–è¯´æ˜æ–‡å­—"""
        
        source_text_dict = {str(i): text for i, text in enumerate(translated_texts)}
        
        # æ„å»ºå¸¦åºå·çš„è¯‘æ–‡
        numbered_lines = []
        for index, line in enumerate(translated_texts):
            if "\n" in line:
                lines = line.split("\n")
                numbered_text = f"{index + 1}.[\n"
                total_lines = len(lines)
                for sub_index, sub_line in enumerate(lines):
                    sub_line = sub_line[:-1] if re.match(r'.*[^ ] $', sub_line) else sub_line
                    numbered_text += f'"{index + 1}.{total_lines - sub_index}.,{sub_line}",\n'
                numbered_text = numbered_text.rstrip('\n').rstrip(',')
                numbered_text += f"\n]"
                numbered_lines.append(numbered_text)
            else:
                numbered_lines.append(f"{index + 1}.{line}")
        
        translated_text = "\n".join(numbered_lines)
        
        user_prompt = f"""###è¯·å›è¯‘ä»¥ä¸‹æ–‡æœ¬
<textarea>
{translated_text}
</textarea>

###å›è¯‘è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
<textarea>
ï¼ˆåœ¨è¿™é‡Œè¾“å‡ºå¸¦åºå·çš„å›è¯‘ç»“æœï¼‰
</textarea>"""
        
        messages = [{"role": "user", "content": user_prompt}]
        
        try:
            if not self._wait_for_limiter(messages, system_prompt):
                return None
            
            platform_config = self.config.get_platform_configuration("translationReq") if self.config else {}
            skip, _, response_content, prompt_tokens, completion_tokens = self._llm_request_with_tracking(
                messages=messages,
                system_prompt=system_prompt,
                platform_config=platform_config
            )
            
            # ğŸ”¥ æ›´æ–°tokenç»Ÿè®¡
            self._update_token_stats(prompt_tokens, completion_tokens)
            
            if not skip and response_content:
                # ğŸ”¥ ä½¿ç”¨é²æ£’çš„æŒ‰è¡Œå·æå–ï¼Œè§£å†³é”™ä½é—®é¢˜
                extracted_map = self._extract_by_line_number(response_content)
                
                if extracted_map:
                    back_translations = []
                    for i in range(len(translated_texts)):
                        # æŒ‰ç´¢å¼•è·å–ï¼Œå¦‚æœç¼ºå¤±åˆ™ä¸ºç©º
                        back_translations.append(extracted_map.get(i, ""))
                    
                    if any(back_translations):
                        return back_translations
        except Exception as e:
            self.debug(f"æ‰¹é‡å›è¯‘å¤±è´¥: {e}")
        
        return None
    
    def _batch_estimate_quality(self, source_texts: List[str], translated_texts: List[str],
                               back_translations: List[str]) -> tuple[List[bool], List[float]]:
        """
        ğŸ”¥ æ‰¹é‡è¯„ä¼°ç¿»è¯‘è´¨é‡ï¼ˆå¸¦è¯¦ç»†è¯„åˆ†ï¼‰
        è¿”å›: (needs_refinement, quality_scores)
            - needs_refinement: List[bool]ï¼Œæ¯ä¸ªå…ƒç´ è¡¨ç¤ºå¯¹åº”è¡Œæ˜¯å¦éœ€è¦ä¿®æ­£
            - quality_scores: List[float]ï¼Œæ¯ä¸ªå…ƒç´ è¡¨ç¤ºå¯¹åº”è¡Œçš„è´¨é‡è¯„åˆ†(1-10)
        """
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·æ¯”è¾ƒåŸæ–‡å’Œå›è¯‘æ–‡ï¼Œä¸ºæ¯è¡Œç¿»è¯‘æ‰“åˆ†ï¼ˆ1-10åˆ†ï¼‰ã€‚

è¯„ä¼°ç»´åº¦ï¼š
1. è¯­ä¹‰å‡†ç¡®æ€§ï¼ˆ40%ï¼‰ï¼šå›è¯‘æ˜¯å¦å‡†ç¡®è¿˜åŸäº†åŸæ–‡çš„å«ä¹‰
2. ä¿¡æ¯å®Œæ•´æ€§ï¼ˆ30%ï¼‰ï¼šå›è¯‘æ˜¯å¦ä¿ç•™äº†åŸæ–‡çš„æ‰€æœ‰å…³é”®ä¿¡æ¯
3. é€»è¾‘ä¸€è‡´æ€§ï¼ˆ20%ï¼‰ï¼šå›è¯‘æ˜¯å¦é€»è¾‘é€šé¡ºï¼Œæ— çŸ›ç›¾
4. æ•´ä½“æµç•…æ€§ï¼ˆ10%ï¼‰ï¼šå›è¯‘æ˜¯å¦è‡ªç„¶æµç•…

ğŸ”¥ã€é‡è¦è¯„ä¼°åŸåˆ™-æœ¯è¯­å®¹å¿ã€‘ğŸ”¥
- å¦‚æœå›è¯‘ä¸åŸæ–‡çš„å·®å¼‚ä»…åœ¨äºä¸“æœ‰åè¯ã€æœ¯è¯­çš„è¡¨è¿°ä¸åŒï¼Œä½†è¯­ä¹‰ç­‰ä»·ï¼Œåº”ç»™äºˆé«˜åˆ†
- ä¾‹å¦‚ï¼šåŸæ–‡"phosphatidylinositol"ï¼Œå›è¯‘ä¸º"phospholipid inositol"ï¼Œè™½ç„¶è¯ä¸åŒï¼Œä½†è¯­ä¹‰ç›¸è¿‘ï¼Œåº”ç»™8-9åˆ†
- ä¾‹å¦‚ï¼šåŸæ–‡"autophagy"ï¼Œå›è¯‘ä¸º"self-eating"ï¼Œè™½ç„¶è¯ä¸åŒï¼Œä½†éƒ½æŒ‡ä»£è‡ªå™¬ï¼Œåº”ç»™8-9åˆ†
- æœ¯è¯­çš„ç²¾ç¡®æ€§ç”±æ­£å‘ç¿»è¯‘çš„æœ¯è¯­è¡¨ä¿è¯ï¼Œå›è¯‘åªéœ€éªŒè¯è¯­ä¹‰å‡†ç¡®æ€§

ã€è¯„åˆ†æ ‡å‡†ã€‘
- 9-10åˆ†ï¼šå®Œç¾ï¼Œè¯­ä¹‰å‡†ç¡®ï¼Œä¿¡æ¯å®Œæ•´
- 8åˆ†ï¼šä¼˜ç§€ï¼Œä»…æœ‰å¾®å°çš„æœ¯è¯­è¡¨è¿°å·®å¼‚
- 7åˆ†ï¼šè‰¯å¥½ï¼Œè¯­ä¹‰åŸºæœ¬å‡†ç¡®ï¼Œæœ‰å°ç‘•ç–µ
- 6åˆ†ï¼šåŠæ ¼ï¼Œè¯­ä¹‰å¤§è‡´æ­£ç¡®ï¼Œä½†æœ‰æ˜æ˜¾ä¸è¶³
- 5åˆ†ä»¥ä¸‹ï¼šéœ€è¦ä¿®æ­£ï¼Œå­˜åœ¨è¯­ä¹‰åå·®ã€ä¿¡æ¯é—æ¼æˆ–é€»è¾‘é”™è¯¯

ã€é‡è¦ã€‘è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- å¿…é¡»ä½¿ç”¨<textarea>æ ‡ç­¾åŒ…è£¹æ‰€æœ‰è¯„ä¼°ç»“æœ
- æ¯è¡Œæ ¼å¼ï¼šåºå·. è¯„åˆ†ï¼šX.Xï¼ˆå¦‚ï¼š1. è¯„åˆ†ï¼š9.5 æˆ– 2. è¯„åˆ†ï¼š7.0ï¼‰
- è¯„åˆ†å¿…é¡»æ˜¯1.0åˆ°10.0ä¹‹é—´çš„æ•°å­—ï¼Œå¿…é¡»åŒ…å«å°æ•°ç‚¹
- ä¸è¦è¾“å‡º"0"æˆ–"0.0"è¿™æ ·çš„æ— æ•ˆè¯„åˆ†
- ä¸è¦æ·»åŠ "åˆ†"å­—æˆ–å…¶ä»–è¯´æ˜æ–‡å­—"""
        
        # æ„å»ºæ‰¹é‡è¯„ä¼°çš„è¾“å…¥
        comparison_blocks = []
        for i, (src, trans, back) in enumerate(zip(source_texts, translated_texts, back_translations)):
            block = f"{i + 1}.åŸæ–‡: {src[:100]}{'...' if len(src) > 100 else ''}\n   å›è¯‘: {back[:100]}{'...' if len(back) > 100 else ''}"
            comparison_blocks.append(block)
        
        comparison_text = "\n\n".join(comparison_blocks)
        
        user_prompt = f"""###ç¿»è¯‘è´¨é‡è¯„ä¼°ï¼ˆä¸ºæ¯è¡Œæ‰“åˆ†1-10åˆ†ï¼‰
{comparison_text}

###è¯„ä¼°ç»“æœè¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
<textarea>
1. è¯„åˆ†ï¼š9.5
2. è¯„åˆ†ï¼š8.0
3. è¯„åˆ†ï¼š7.5
4. è¯„åˆ†ï¼š6.0
5. è¯„åˆ†ï¼š9.0
...ï¼ˆæŒ‰æ­¤æ ¼å¼è¾“å‡ºæ‰€æœ‰è¡Œçš„è¯„åˆ†ï¼‰
</textarea>

ã€é‡è¦æç¤ºã€‘
- æ¯è¡Œå¿…é¡»åŒ…å«"è¯„åˆ†ï¼š"ä¸¤ä¸ªå­—
- è¯„åˆ†å¿…é¡»æ˜¯1.0-10.0ä¹‹é—´çš„æ•°å­—
- ä¸è¦è¾“å‡º0æˆ–0.0
- ç¤ºä¾‹æ­£ç¡®æ ¼å¼ï¼š1. è¯„åˆ†ï¼š9.5ï¼ˆæ­£ç¡®ï¼‰
- ç¤ºä¾‹é”™è¯¯æ ¼å¼ï¼š1. 9.5ï¼ˆé”™è¯¯ï¼‰ã€1. è¯„åˆ†ï¼š0ï¼ˆé”™è¯¯ï¼‰ã€1. è¯„åˆ†ï¼š9.5åˆ†ï¼ˆé”™è¯¯ï¼‰"""
        
        messages = [{"role": "user", "content": user_prompt}]
        
        try:
            if not self._wait_for_limiter(messages, system_prompt):
                return [False] * len(source_texts), [8.0] * len(source_texts)  # é»˜è®¤éƒ½ä¸éœ€è¦ä¿®æ­£ï¼Œé»˜è®¤8åˆ†
            
            platform_config = self.config.get_platform_configuration("translationReq") if self.config else {}
            skip, _, response_content, prompt_tokens, completion_tokens = self._llm_request_with_tracking(
                messages=messages,
                system_prompt=system_prompt,
                platform_config=platform_config
            )
            
            # ğŸ”¥ æ›´æ–°tokenç»Ÿè®¡
            self._update_token_stats(prompt_tokens, completion_tokens)
            
            if not skip and response_content:
                # ğŸ”¥ ä½¿ç”¨é²æ£’çš„æŒ‰è¡Œå·æå–ï¼Œè§£å†³é”™ä½é—®é¢˜
                extracted_map = self._extract_by_line_number(response_content)
                
                if extracted_map:
                    needs_refinement = []
                    quality_scores = []  # å­˜å‚¨è´¨é‡åˆ†æ•°
                    
                    for i in range(len(source_texts)):
                        if i in extracted_map:
                            # ğŸ”¥ ä¼˜å…ˆè§£æ"è¯„åˆ†ï¼š"æ ¼å¼
                            raw_response = extracted_map[i].strip()
                            score_str = ""
                            
                            try:
                                # æ–¹æ³•1ï¼šæŸ¥æ‰¾"è¯„åˆ†ï¼š"æˆ–"è¯„åˆ†:"åé¢çš„æ•°å­—
                                if 'è¯„åˆ†ï¼š' in raw_response:
                                    score_str = raw_response.split('è¯„åˆ†ï¼š')[-1].strip()
                                elif 'è¯„åˆ†:' in raw_response:
                                    score_str = raw_response.split('è¯„åˆ†:')[-1].strip()
                                # æ–¹æ³•2ï¼šæŸ¥æ‰¾è‹±æ–‡"score:"æˆ–"Score:"
                                elif 'scoreï¼š' in raw_response.lower():
                                    score_str = raw_response.lower().split('scoreï¼š')[-1].strip()
                                elif 'score:' in raw_response.lower():
                                    score_str = raw_response.lower().split('score:')[-1].strip()
                                # æ–¹æ³•3ï¼šå¦‚æœæ²¡æœ‰å‰ç¼€ï¼Œç›´æ¥å½“ä½œæ•°å­—
                                else:
                                    score_str = raw_response
                                
                                # æ¸…ç†å¯èƒ½çš„å¹²æ‰°å­—ç¬¦
                                score_str = score_str.replace('åˆ†', '').replace('/10', '').replace(' ', '').strip()
                                
                                # å¦‚æœæ˜¯ç©ºçš„æˆ–ä»¥"."å¼€å¤´ï¼Œè¯´æ˜è§£æå¤±è´¥
                                if not score_str or score_str.startswith('.'):
                                    raise ValueError(f"Invalid score format: '{score_str}'")
                                
                                # ğŸ”¥ å¦‚æœåªæœ‰æ•´æ•°ï¼ˆå¦‚"9"ï¼‰ï¼Œè‡ªåŠ¨æ·»åŠ ".0"
                                if '.' not in score_str:
                                    score_str = score_str + '.0'
                                
                                score = float(score_str)
                                
                                # ğŸ”¥ åˆ†æ•°èŒƒå›´æ£€æŸ¥ï¼šå¿…é¡»åœ¨1-10ä¹‹é—´
                                if score < 1.0 or score > 10.0:
                                    self.warning(f"    âš  è¡Œ{i+1}è¯„åˆ†å¼‚å¸¸({score})ï¼Œè¶…å‡º1-10èŒƒå›´ï¼Œä½¿ç”¨é»˜è®¤å€¼8.0")
                                    score = 8.0
                                
                                quality_scores.append(score)
                                # 7åˆ†ä»¥ä¸‹éœ€è¦ä¿®æ­£
                                needs_refinement.append(score < 7.0)
                            except (ValueError, Exception) as e:
                                # æ˜¾ç¤ºåŸå§‹å“åº”å†…å®¹ä»¥ä¾¿è°ƒè¯•
                                self.warning(f"    âš  æ— æ³•è§£æè¡Œ{i+1}çš„è¯„åˆ†'{raw_response[:100]}'ï¼Œä½¿ç”¨é»˜è®¤å€¼8.0")
                                quality_scores.append(8.0)  # é»˜è®¤8åˆ†
                                needs_refinement.append(False)
                        else:
                            self.warning(f"    âš  è¡Œ{i+1}æœªæ‰¾åˆ°è¯„åˆ†ï¼Œä½¿ç”¨é»˜è®¤å€¼8.0")
                            quality_scores.append(8.0)  # é»˜è®¤8åˆ†
                            needs_refinement.append(False)
                    
                    # ğŸ”¥ æ˜¾ç¤ºè¯¦ç»†è¯„åˆ†
                    need_refine_count = sum(needs_refinement)
                    self.info(f"    âœ“ è¯„ä¼°å®Œæˆ: {need_refine_count}/{len(source_texts)} è¡Œéœ€è¦ä¿®æ­£")
                    
                    # ğŸ”¥ æ˜¾ç¤ºç­–ç•¥ï¼š
                    # - å¦‚æœ<=10è¡Œï¼šæ˜¾ç¤ºæ‰€æœ‰è¡Œçš„è¯„åˆ†
                    # - å¦‚æœ>10è¡Œï¼šæ˜¾ç¤ºéœ€è¦ä¿®æ­£çš„è¡Œ + å‰3è¡Œç¤ºä¾‹ï¼ˆè®©ç”¨æˆ·çŸ¥é“ç¡®å®æ‰“åˆ†äº†ï¼‰
                    show_all = len(source_texts) <= 10
                    
                    if show_all:
                        # æ˜¾ç¤ºæ‰€æœ‰è¯„åˆ†
                        for i, (score, needs_refine) in enumerate(zip(quality_scores, needs_refinement)):
                            status_icon = "âš " if needs_refine else "âœ…"
                            status_text = "éœ€ä¿®æ­£" if needs_refine else "è‰¯å¥½"
                            self.info(f"      è¡Œ{i+1}: è¯„åˆ†ï¼š{score:.1f}/10 {status_icon} {status_text}")
                    else:
                        # æ˜¾ç¤ºéœ€è¦ä¿®æ­£çš„è¡Œ + å‰3è¡Œç¤ºä¾‹
                        shown_count = 0
                        for i, (score, needs_refine) in enumerate(zip(quality_scores, needs_refinement)):
                            should_show = needs_refine or (shown_count < 3 and not needs_refine)
                            
                            if should_show:
                                status_icon = "âš " if needs_refine else "âœ…"
                                status_text = "éœ€ä¿®æ­£" if needs_refine else "è‰¯å¥½"
                                self.info(f"      è¡Œ{i+1}: è¯„åˆ†ï¼š{score:.1f}/10 {status_icon} {status_text}")
                                if not needs_refine:
                                    shown_count += 1
                        
                        if need_refine_count == 0 and len(source_texts) > 10:
                            self.info(f"      ... (å…¶ä½™{len(source_texts)-3}è¡Œè¯„åˆ†å‡ä¸ºè‰¯å¥½ï¼Œå·²çœç•¥)")
                    
                    return needs_refinement, quality_scores
        except Exception as e:
            self.debug(f"æ‰¹é‡è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
        
        return [False] * len(source_texts), [8.0] * len(source_texts)
    
    def _batch_refine_translation(self, source_texts: List[str], translated_texts: List[str],
                                 back_translations: List[str], needs_refinement: List[bool],
                                 terminology_db: Dict) -> Optional[List[str]]:
        """
        æ‰¹é‡ä¿®æ­£ç¿»è¯‘ï¼ˆä»…ä¿®æ­£éœ€è¦ä¿®æ­£çš„è¡Œï¼‰
        """
        # æ”¶é›†éœ€è¦ä¿®æ­£çš„è¡Œ
        to_refine_indices = [i for i, need in enumerate(needs_refinement) if need]
        
        if not to_refine_indices:
            return translated_texts
        
        to_refine_sources = [source_texts[i] for i in to_refine_indices]
        to_refine_translations = [translated_texts[i] for i in to_refine_indices]
        to_refine_backs = [back_translations[i] for i in to_refine_indices]
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘ä¿®æ­£ä¸“å®¶ã€‚è¯·æ ¹æ®åŸæ–‡å’Œå›è¯‘ç»“æœä¿®æ­£ä»¥ä¸‹è¯‘æ–‡ã€‚

{self._build_terminology_prompt(terminology_db, to_refine_sources)}

ğŸ”¥ã€å¼ºåˆ¶è¦æ±‚-æœ¯è¯­è¡¨å¿…é¡»ä¸¥æ ¼éµå®ˆã€‘ğŸ”¥
- å¦‚æœåŸæ–‡ä¸­å‡ºç°æœ¯è¯­è¡¨ä¸­çš„ä»»ä½•æœ¯è¯­ï¼Œä¿®æ­£åçš„è¯‘æ–‡å¿…é¡»ä½¿ç”¨æœ¯è¯­è¡¨ä¸­æŒ‡å®šçš„ç¿»è¯‘
- ç»å¯¹ä¸å…è®¸ç”¨å…¶ä»–ç¿»è¯‘æ›¿ä»£æœ¯è¯­è¡¨ä¸­çš„æœ¯è¯­
- è¿™æ˜¯å¼ºåˆ¶æ€§è¦æ±‚ï¼Œä¸å¯è¿å
- ä¾‹å¦‚ï¼šå¦‚æœæœ¯è¯­è¡¨è§„å®š"phosphatidylinositol"å¿…é¡»ç¿»è¯‘ä¸º"ç£·è„‚é…°è‚Œé†‡"ï¼Œåˆ™ä¿®æ­£æ—¶å¿…é¡»ä½¿ç”¨è¿™ä¸ªç¿»è¯‘
- ä¾‹å¦‚ï¼šå¦‚æœæœ¯è¯­è¡¨è§„å®š"Beclin"å¿…é¡»ç¿»è¯‘ä¸º"Beclin"ï¼Œåˆ™ä¿®æ­£æ—¶ä¸èƒ½æ”¹æˆ"è´å…‹æ—"
- æœ¯è¯­è¡¨çš„ç¿»è¯‘ä¼˜å…ˆçº§æœ€é«˜ï¼Œå³ä½¿å›è¯‘ç»“æœæ˜¾ç¤ºæœ‰å·®å¼‚ï¼Œä¹Ÿå¿…é¡»ä¿æŒæœ¯è¯­è¡¨è§„å®šçš„è¯‘æ³•

ã€ä¿®æ­£åŸåˆ™ã€‘
- å¦‚æœå›è¯‘ä¸åŸæ–‡çš„å·®å¼‚æ˜¯ç”±äºæœ¯è¯­ç¿»è¯‘ä¸ä¸€è‡´å¯¼è‡´çš„ï¼Œä¸è¦ä¿®æ­£è¯‘æ–‡ï¼Œå› ä¸ºæœ¯è¯­å·²ç»æ˜¯æ­£ç¡®çš„
- åªä¿®æ­£çœŸæ­£çš„è¯­ä¹‰é”™è¯¯ã€è¯­æ³•é”™è¯¯æˆ–æµç•…æ€§é—®é¢˜
- ä¿®æ­£æ—¶å¿…é¡»ä¿æŒæœ¯è¯­è¡¨è§„å®šçš„æ‰€æœ‰æœ¯è¯­ç¿»è¯‘ä¸å˜

ã€é‡è¦ã€‘è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- å¿…é¡»ä½¿ç”¨<textarea>æ ‡ç­¾åŒ…è£¹æ‰€æœ‰ä¿®æ­£åçš„è¯‘æ–‡
- æ¯è¡Œä¿®æ­£è¯‘æ–‡å‰å¿…é¡»åŠ ä¸Šåºå·ï¼ˆå¦‚1. 2. 3.ï¼‰
- ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„æ ‡é¢˜ã€å‰ç¼€æˆ–è¯´æ˜æ–‡å­—"""
        
        # æ„å»ºæ‰¹é‡ä¿®æ­£çš„è¾“å…¥
        refine_blocks = []
        for i, (src, trans, back) in enumerate(zip(to_refine_sources, to_refine_translations, to_refine_backs)):
            block = f"{i + 1}. åŸæ–‡: {src}\n   åŸè¯‘æ–‡: {trans}\n   å›è¯‘: {back}"
            refine_blocks.append(block)
        
        refine_text = "\n\n".join(refine_blocks)
        
        user_prompt = f"""###è¯·ä¿®æ­£ä»¥ä¸‹è¯‘æ–‡
{refine_text}

###ã€ä¸¥æ ¼è¦æ±‚ã€‘è¾“å‡ºæ ¼å¼
ä½ å¿…é¡»åªè¾“å‡ºä¿®æ­£åçš„çº¯è¯‘æ–‡ï¼Œä¸è¦è¾“å‡º"åŸæ–‡:"ã€"åŸè¯‘æ–‡:"ã€"å›è¯‘:"ã€"ä¿®æ­£åè¯‘æ–‡:"ç­‰æ ‡ç­¾ã€‚
æ ¼å¼å¦‚ä¸‹ï¼š
<textarea>
1. ï¼ˆç¬¬1è¡Œä¿®æ­£åçš„çº¯ä¸­æ–‡è¯‘æ–‡ï¼‰
2. ï¼ˆç¬¬2è¡Œä¿®æ­£åçš„çº¯ä¸­æ–‡è¯‘æ–‡ï¼‰
</textarea>

ä¾‹å¦‚ï¼Œå¦‚æœä¿®æ­£åè¯‘æ–‡æ˜¯"è¿™æ˜¯ç¿»è¯‘ç»“æœ"ï¼Œæ­£ç¡®è¾“å‡ºæ˜¯ï¼š
<textarea>
1. è¿™æ˜¯ç¿»è¯‘ç»“æœ
</textarea>

é”™è¯¯ç¤ºä¾‹ï¼ˆä¸è¦è¿™æ ·è¾“å‡ºï¼‰ï¼š
<textarea>
1. åŸæ–‡:xxx åŸè¯‘æ–‡:xxx å›è¯‘:xxx ä¿®æ­£åè¯‘æ–‡:è¿™æ˜¯ç¿»è¯‘ç»“æœ
</textarea>"""
        
        messages = [{"role": "user", "content": user_prompt}]
        
        try:
            if not self._wait_for_limiter(messages, system_prompt):
                return None
            
            platform_config = self.config.get_platform_configuration("translationReq") if self.config else {}
            skip, _, response_content, prompt_tokens, completion_tokens = self._llm_request_with_tracking(
                messages=messages,
                system_prompt=system_prompt,
                platform_config=platform_config
            )
            
            # ğŸ”¥ æ›´æ–°tokenç»Ÿè®¡
            self._update_token_stats(prompt_tokens, completion_tokens)
            
            if not skip and response_content:
                source_text_dict = {str(i): text for i, text in enumerate(to_refine_sources)}
                response_dict = ResponseExtractor.text_extraction(self, source_text_dict, response_content)
                response_dict = ResponseExtractor.remove_numbered_prefix(self, response_dict)
                
                if response_dict:
                    # å°†ä¿®æ­£ç»“æœå¡«å›åŸåˆ—è¡¨
                    refined_texts = translated_texts.copy()
                    for i, idx in enumerate(to_refine_indices):
                        key = str(i)
                        if key in response_dict and response_dict[key]:
                            refined_text = response_dict[key]
                            # ğŸ”¥ ã€å…³é”®ã€‘æ¸…ç†å¯èƒ½æ®‹ç•™çš„å‰ç¼€æ ‡ç­¾
                            refined_text = self._clean_refine_response(refined_text)
                            refined_texts[idx] = refined_text
                    
                    return refined_texts
        except Exception as e:
            self.error(f"æ‰¹é‡ä¿®æ­£å¤±è´¥: {e}", e)
        
        return None
    
    def _clean_refine_response(self, text: str) -> str:
        """
        æ¸…ç†ä¿®æ­£å“åº”ä¸­å¯èƒ½æ®‹ç•™çš„å‰ç¼€æ ‡ç­¾
        é˜²æ­¢"åŸæ–‡:xxx åŸè¯‘æ–‡:xxx å›è¯‘:xxx ä¿®æ­£åè¯‘æ–‡:xxx"è¿™ç§æ ¼å¼è¢«è¾“å‡º
        """
        if not text:
            return text
        
        # å¦‚æœåŒ…å«"ä¿®æ­£åè¯‘æ–‡:"ï¼Œæå–åé¢çš„å†…å®¹
        if "ä¿®æ­£åè¯‘æ–‡:" in text:
            text = text.split("ä¿®æ­£åè¯‘æ–‡:")[-1].strip()
        elif "ä¿®æ­£åè¯‘æ–‡ï¼š" in text:
            text = text.split("ä¿®æ­£åè¯‘æ–‡ï¼š")[-1].strip()
        
        # æ¸…ç†å¯èƒ½æ®‹ç•™çš„å…¶ä»–æ ‡ç­¾
        prefixes_to_remove = [
            "åŸæ–‡:", "åŸæ–‡ï¼š",
            "åŸè¯‘æ–‡:", "åŸè¯‘æ–‡ï¼š",
            "å›è¯‘:", "å›è¯‘ï¼š",
            "è¯‘æ–‡:", "è¯‘æ–‡ï¼š",
        ]
        
        # æ£€æŸ¥å¼€å¤´æ˜¯å¦æœ‰è¿™äº›å‰ç¼€
        for prefix in prefixes_to_remove:
            if text.startswith(prefix):
                text = text[len(prefix):].strip()
        
        # å¦‚æœæ•´è¡Œéƒ½æ˜¯"åŸæ–‡:xxx åŸè¯‘æ–‡:xxx"æ ¼å¼ï¼Œå°è¯•æå–æœ€åä¸€ä¸ªæœ‰æ„ä¹‰çš„éƒ¨åˆ†
        if any(p in text for p in ["åŸæ–‡:", "åŸè¯‘æ–‡:", "å›è¯‘:"]):
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²
            import re
            parts = re.split(r'(?:åŸæ–‡|åŸè¯‘æ–‡|å›è¯‘|ä¿®æ­£åè¯‘æ–‡)[ï¼š:]', text)
            if parts:
                # å–æœ€åä¸€ä¸ªéç©ºéƒ¨åˆ†
                for part in reversed(parts):
                    part = part.strip()
                    if part and not any(p in part for p in ["åŸæ–‡", "åŸè¯‘æ–‡", "å›è¯‘"]):
                        return part
        
        return text
    
    def _fallback_translate_one_by_one(self, source_texts: List[str], context_texts: List[str],
                                       strategy: str, terminology_db: Dict, memory_storage: Dict) -> List[str]:
        """
        Fallbackæœºåˆ¶ï¼šé€è¡Œç¿»è¯‘ï¼ˆå½“æ‰¹é‡ç¿»è¯‘å®Œå…¨å¤±è´¥æ—¶ï¼‰
        """
        self.warning(f"  â†’ å¼€å§‹é€è¡ŒFallbackç¿»è¯‘ï¼Œå…±{len(source_texts)}è¡Œ...")
        translated_texts = []
        
        for i, source_text in enumerate(source_texts):
            self.debug(f"    â†’ ç¿»è¯‘ç¬¬{i+1}/{len(source_texts)}è¡Œ...")
            translation = self._translate_single_line(
                source_text, context_texts, strategy, terminology_db, memory_storage
            )
            if translation:
                translated_texts.append(translation)
            else:
                # å¦‚æœå•è¡Œç¿»è¯‘ä¹Ÿå¤±è´¥ï¼Œä¿ç•™åŸæ–‡æ ‡è®°
                translated_texts.append(f"[ç¿»è¯‘å¤±è´¥]{source_text}")
                self.error(f"    âœ— ç¬¬{i+1}è¡Œç¿»è¯‘å¤±è´¥")
        
        success_count = sum(1 for t in translated_texts if not t.startswith("[ç¿»è¯‘å¤±è´¥]"))
        self.info(f"  âœ“ é€è¡Œç¿»è¯‘å®Œæˆ: {success_count}/{len(source_texts)} è¡ŒæˆåŠŸ")
        return translated_texts
    
    def _translate_single_line(self, source_text: str, context_texts: List[str],
                               strategy: str, terminology_db: Dict, memory_storage: Dict) -> Optional[str]:
        """
        ç¿»è¯‘å•è¡Œæ–‡æœ¬ï¼ˆç”¨äºfallbackæˆ–è¡¥å……ç¿»è¯‘ï¼‰
        """
        # æ„å»ºæœ¯è¯­æç¤º
        terminology_prompt = self._build_terminology_prompt(terminology_db, [source_text])
        
        # ğŸ”¥ æ£€æµ‹æ˜¯å¦ä¸ºå‚è€ƒæ–‡çŒ®
        is_reference = any(keyword in source_text.lower() for keyword in [
            'et al.', 'doi:', 'http://', 'https://', 'pubmed', 'pmid:', 
            'journal', 'proc.', 'vol.', 'pp.', 'issn'
        ]) or (len(source_text) > 500 and source_text.count(',') > 5)
        
        # ä¸ºå‚è€ƒæ–‡çŒ®æ·»åŠ ç‰¹æ®Šè¯´æ˜
        reference_instruction = ""
        if is_reference:
            reference_instruction = """
ã€å‚è€ƒæ–‡çŒ®ç¿»è¯‘è¦æ±‚ã€‘
âš ï¸ è¿™æ˜¯å‚è€ƒæ–‡çŒ®å†…å®¹ï¼Œéœ€è¦ç¿»è¯‘ï¼ä¸è¦ç›´æ¥è¾“å‡ºè‹±æ–‡åŸæ–‡ï¼
- å¿…é¡»ç¿»è¯‘ï¼šæ–‡ç« æ ‡é¢˜ã€æœŸåˆŠåç§°ã€ä¼šè®®åç§°
- ä¿ç•™ä¸å˜ï¼šä½œè€…å§“åã€å¹´ä»½ã€DOIã€URLã€å·å·é¡µç 
- ç¿»è¯‘ç¤ºä¾‹ï¼š
  åŸæ–‡: Brown, W.J. et al. (1995) Role for phosphatidylinositol 3-kinase in lysosomal enzyme transport. Nature 377, 525â€“528.
  è¯‘æ–‡: Brown, W.J. ç­‰äºº (1995) ç£·è„‚é…°è‚Œé†‡3-æ¿€é…¶åœ¨æº¶é…¶ä½“é…¶è¿è¾“ä¸­çš„ä½œç”¨ã€‚ã€Šè‡ªç„¶ã€‹377, 525â€“528ã€‚"""
        
        # ç®€åŒ–çš„system_promptï¼ˆå•è¡Œç¿»è¯‘ä¸éœ€è¦å¤æ‚çš„å¤šæ­¥éª¤å¼•å¯¼ï¼‰
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘ä¸“å®¶ã€‚
        
{terminology_prompt}
{reference_instruction}

ã€é‡è¦ã€‘è¾“å‡ºæ ¼å¼ï¼š
- ç›´æ¥è¾“å‡ºè¯‘æ–‡ï¼Œä¸è¦æ·»åŠ åºå·ã€æ ‡ç­¾æˆ–å…¶ä»–è¯´æ˜æ–‡å­—
- å¿…é¡»ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¸è¦ç›´æ¥è¾“å‡ºè‹±æ–‡åŸæ–‡"""
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_str = "\n".join(context_texts[-3:]) if context_texts else ""
        context_prefix = f"###ä¸Šæ–‡å†…å®¹\n{context_str}\n\n" if context_str else ""
        
        user_prompt = f"""{context_prefix}###å¾…ç¿»è¯‘æ–‡æœ¬
{source_text}

###è¯‘æ–‡"""
        
        messages = [{"role": "user", "content": user_prompt}]
        
        try:
            # ç­‰å¾…RequestLimiter
            if not self._wait_for_limiter(messages, system_prompt):
                return None
            
            platform_config = self.config.get_platform_configuration("translationReq") if self.config else {}
            
            skip, _, response_content, prompt_tokens, completion_tokens = self._llm_request_with_tracking(
                messages=messages,
                system_prompt=system_prompt,
                platform_config=platform_config
            )
            
            # ğŸ”¥ æ›´æ–°tokenç»Ÿè®¡
            self._update_token_stats(prompt_tokens, completion_tokens)
            
            if not skip and response_content:
                # ç®€å•æå–ï¼šå»é™¤å‰åç©ºç™½å’Œå¯èƒ½çš„å¼•å·
                translation = response_content.strip().strip('"').strip("'")
                return translation if translation else None
            else:
                return None
        except Exception as e:
            self.error(f"å•è¡Œç¿»è¯‘å¤±è´¥: {e}", e)
            return None
    
    def _strategy_based_batch_translation(self, source_texts: List[str], context_texts: List[str],
                                              strategy: str, terminology_db: Dict, memory_storage: Dict) -> Optional[List[str]]:
        """
        åŸºäºç­–ç•¥çš„æ‰¹é‡ç¿»è¯‘ï¼ˆåˆå¹¶åŸæ­¥éª¤1å’Œ2ï¼‰
        æ ¹æ®PlanningAgentåˆ†é…çš„ç­–ç•¥ï¼Œç›´æ¥æ‰§è¡Œå¯¹åº”çš„ç¿»è¯‘æ–¹å¼
        
        Args:
            source_texts: å¾…ç¿»è¯‘æ–‡æœ¬åˆ—è¡¨
            context_texts: ä¸Šä¸‹æ–‡æ–‡æœ¬åˆ—è¡¨
            strategy: ç¿»è¯‘ç­–ç•¥ ("literal" | "free" | "stylized")
            terminology_db: æœ¯è¯­åº“
            memory_storage: è®°å¿†å­˜å‚¨
        
        Returns:
            ç¿»è¯‘ç»“æœåˆ—è¡¨
        """
        # æ„å»ºæœ¯è¯­æç¤ºï¼ˆâœ… ä¼ é€’source_textsç”¨äºåŠ¨æ€ç­›é€‰ï¼‰
        terminology_prompt = self._build_terminology_prompt(terminology_db, source_texts)
        
        # æ ¹æ®ç­–ç•¥æ„å»ºä¸åŒçš„system_prompt
        if strategy == "literal":
            # ç›´è¯‘ç­–ç•¥ï¼šå¼ºè°ƒæœ¯è¯­å‡†ç¡®ã€ä¿æŒåŸæ–‡ç»“æ„
            strategy_instruction = """ç›´è¯‘ç­–ç•¥ï¼š
- ä¿æŒåŸæ–‡çš„å¥å­ç»“æ„å’Œè¡¨è¾¾æ–¹å¼
- **ä¸¥æ ¼éµå®ˆæœ¯è¯­åº“ä¸­çš„ä¸“ä¸šæœ¯è¯­å’Œå®ä½“ç¿»è¯‘ï¼Œä¸å¾—æ›´æ”¹**
- ä¼˜å…ˆä¿è¯å‡†ç¡®æ€§å’Œæœ¯è¯­ä¸€è‡´æ€§ï¼Œå…¶æ¬¡è€ƒè™‘æµç•…æ€§
- é€‚ç”¨äºæŠ€æœ¯æ–‡æ¡£ã€æ³•å¾‹æ–‡æœ¬ç­‰æ­£å¼å†…å®¹"""
        elif strategy == "stylized":
            # é£æ ¼åŒ–ç­–ç•¥ï¼šå¼ºè°ƒè‰ºæœ¯æ€§ã€éŸµå¾‹æ„Ÿ
            strategy_instruction = """é£æ ¼åŒ–ç­–ç•¥ï¼š
- æ³¨é‡è¯‘æ–‡çš„è‰ºæœ¯æ€§å’Œæ–‡å­¦ç¾æ„Ÿ
- ä¿æŒåŸæ–‡çš„éŸµå¾‹ã€èŠ‚å¥å’Œæƒ…æ„Ÿ
- **æœ¯è¯­åº“ä¸­çš„äººåã€åœ°åç­‰ä¸“æœ‰åè¯å¿…é¡»ä½¿ç”¨å›ºå®šç¿»è¯‘**
- å¯ä»¥é€‚å½“è°ƒæ•´å¥å¼ä»¥ç¬¦åˆç›®æ ‡è¯­è¨€ä¹ æƒ¯
- é€‚ç”¨äºæ–‡å­¦ä½œå“ã€è¯—æ­Œã€è¥é”€æ–‡æ¡ˆ"""
        else:  # free (é»˜è®¤)
            # æ„è¯‘ç­–ç•¥ï¼šå¼ºè°ƒè‡ªç„¶æµç•…
            strategy_instruction = """æ„è¯‘ç­–ç•¥ï¼š
- æ³¨é‡è¯‘æ–‡çš„è‡ªç„¶æµç•…æ€§
- ç¬¦åˆç›®æ ‡è¯­è¨€çš„è¡¨è¾¾ä¹ æƒ¯
- **æœ¯è¯­åº“ä¸­çš„ä¸“æœ‰åè¯å’Œå…³é”®æœ¯è¯­å¿…é¡»ä½¿ç”¨å›ºå®šç¿»è¯‘**
- å‡†ç¡®ä¼ è¾¾åŸæ–‡çš„æ„æ€ï¼Œå¯çµæ´»è°ƒæ•´è¡¨è¾¾æ–¹å¼
- é€‚ç”¨äºå¯¹è¯ã€å™è¿°æ€§æ–‡æœ¬ç­‰æ—¥å¸¸å†…å®¹"""
        
        # ğŸ”¥ æ£€æµ‹æ˜¯å¦åŒ…å«å‚è€ƒæ–‡çŒ®
        has_references = any(
            any(keyword in text.lower() for keyword in [
                'et al.', 'doi:', 'http://', 'https://', 'pubmed', 'pmid:',
                'journal', 'proc.', 'vol.', 'pp.', 'issn', 'references'
            ]) or (len(text) > 500 and text.count(',') > 5)
            for text in source_texts
        )
        
        reference_instruction = ""
        if has_references:
            reference_instruction = """
ã€å‚è€ƒæ–‡çŒ®ç¿»è¯‘è¦æ±‚ã€‘
âš ï¸ å¦‚æœæ–‡æœ¬ä¸­åŒ…å«å‚è€ƒæ–‡çŒ®ï¼Œå¿…é¡»ç¿»è¯‘ï¼ä¸è¦ç›´æ¥è¾“å‡ºè‹±æ–‡åŸæ–‡ï¼
- å¿…é¡»ç¿»è¯‘ï¼šæ–‡ç« æ ‡é¢˜ã€æœŸåˆŠåç§°ã€ä¼šè®®åç§°
- ä¿ç•™ä¸å˜ï¼šä½œè€…å§“åã€å¹´ä»½ã€DOIã€URLã€å·å·é¡µç 
- ç¿»è¯‘ç¤ºä¾‹ï¼š
  åŸæ–‡: Brown, W.J. et al. (1995) Role for phosphatidylinositol 3-kinase in lysosomal enzyme transport. Nature 377, 525â€“528.
  è¯‘æ–‡: Brown, W.J. ç­‰äºº (1995) ç£·è„‚é…°è‚Œé†‡3-æ¿€é…¶åœ¨æº¶é…¶ä½“é…¶è¿è¾“ä¸­çš„ä½œç”¨ã€‚ã€Šè‡ªç„¶ã€‹377, 525â€“528ã€‚
"""
        
        # æ„å»ºsystem_promptï¼ˆå¼ºåˆ¶è¦æ±‚éµå®ˆæœ¯è¯­è¡¨ï¼‰
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘ä¸“å®¶ï¼Œä½ çš„ä»»åŠ¡æ˜¯æŠŠåŸæ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼Œé€è¡Œç¿»è¯‘ï¼Œä¸è¦åˆå¹¶ï¼Œä¿æŒåŸæ¥çš„æ ¼å¼ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œç¿»è¯‘ï¼š
æ­¥éª¤1 - ç†è§£ï¼šåˆ†æåŸæ–‡çš„è¯­ä¹‰ã€è¯­å¢ƒå’Œé£æ ¼
æ­¥éª¤2 - åˆ†è§£ï¼šå¯¹äºé•¿éš¾å¥ï¼Œå…ˆè¯†åˆ«ä¸»å¹²æˆåˆ†å’Œä»å¥å±‚çº§
æ­¥éª¤3 - è½¬æ¢ï¼šå°†åŸæ–‡è½¬æ¢ä¸ºç›®æ ‡è¯­è¨€ï¼Œä¿æŒè¯­ä¹‰å‡†ç¡®
æ­¥éª¤4 - æ¶¦è‰²ï¼šä¼˜åŒ–è¯‘æ–‡ï¼Œç¡®ä¿æµç•…è‡ªç„¶

{strategy_instruction}

{terminology_prompt}

ğŸ”¥ã€å¼ºåˆ¶è¦æ±‚-æœ¯è¯­è¡¨éµå®ˆã€‘ğŸ”¥
- å¦‚æœåŸæ–‡ä¸­å‡ºç°æœ¯è¯­è¡¨ä¸­çš„ä»»ä½•æœ¯è¯­ï¼Œå¿…é¡»ä½¿ç”¨æœ¯è¯­è¡¨ä¸­æŒ‡å®šçš„ç¿»è¯‘
- ç»å¯¹ä¸å…è®¸ç”¨å…¶ä»–ç¿»è¯‘æ›¿ä»£æœ¯è¯­è¡¨ä¸­çš„æœ¯è¯­
- ä¾‹å¦‚ï¼šå¦‚æœæœ¯è¯­è¡¨è§„å®š"Beclin"å¿…é¡»ç¿»è¯‘ä¸º"è´å¯æ—"ï¼Œåˆ™ä¸èƒ½ç¿»è¯‘ä¸º"Beclin"ã€"è´å…‹æ—"æˆ–å…¶ä»–ä»»ä½•è¯‘æ³•
- ä¾‹å¦‚ï¼šå¦‚æœæœ¯è¯­è¡¨è§„å®š"phosphatidylinositol"å¿…é¡»ç¿»è¯‘ä¸º"ç£·è„‚é…°è‚Œé†‡"ï¼Œåˆ™ä¸èƒ½ç¿»è¯‘ä¸º"ç£·è„‚è‚Œé†‡"æˆ–å…¶ä»–ä»»ä½•è¯‘æ³•
{reference_instruction}
ã€é‡è¦ã€‘è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- é€è¡Œç¿»è¯‘ï¼Œä¸è¦åˆå¹¶ï¼ŒåŸæ–‡æœ‰{len(source_texts)}è¡Œï¼Œè¯‘æ–‡ä¹Ÿå¿…é¡»æœ‰{len(source_texts)}è¡Œ
- è¾“å‡ºçš„ç¿»è¯‘é¡ºåºæ ‡å·å¿…é¡»å’Œè¾“å…¥ä¸€ä¸€å¯¹åº”ï¼šè¾“å…¥1.å¯¹åº”è¾“å‡º1.ï¼Œè¾“å…¥2.å¯¹åº”è¾“å‡º2.ï¼Œä¾æ­¤ç±»æ¨
- å¿…é¡»ä½¿ç”¨<textarea>æ ‡ç­¾åŒ…è£¹æ‰€æœ‰è¯‘æ–‡
- æ¯è¡Œè¯‘æ–‡å‰å¿…é¡»åŠ ä¸Šåºå·ï¼ˆå¦‚1. 2. 3.ï¼‰
- åºå·å¿…é¡»ä»1åˆ°{len(source_texts)}è¿ç»­ï¼Œä¸è¦è·³è¿‡
- å³ä½¿æ˜¯å¾ˆçŸ­çš„è¡Œä¹Ÿä¸è¦ä¸å…¶ä»–è¡Œåˆå¹¶
- å¿…é¡»ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¸è¦ç›´æ¥è¾“å‡ºè‹±æ–‡åŸæ–‡
- ä¸è¦è‡ªåŠ¨æ·»åŠ ä¹¦åå·ã€Šã€‹ã€å¼•å·""æˆ–å…¶ä»–åŸæ–‡æ²¡æœ‰çš„æ ‡ç‚¹ç¬¦å·
- ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ€§æ–‡å­—ï¼Œå¦‚"ï¼ˆéŸ³è¯‘ä¸ºä¸»ï¼‰"ã€"ï¼ˆå¯åŠ æ³¨è¯´æ˜ï¼‰"ã€"ï¼ˆæ³¨ï¼š...ï¼‰"ç­‰
- åªè¾“å‡ºçº¯ç²¹çš„ç¿»è¯‘ç»“æœï¼Œä¸è¦åŠ ä»»ä½•æ³¨é‡Šæˆ–è¯´æ˜
- å¦‚æœåŸæ–‡æ˜¯"scientific reports"ï¼Œåªç¿»è¯‘ä¸º"ç§‘å­¦æŠ¥å‘Š"ï¼Œä¸è¦ç¿»è¯‘ä¸º"ã€Šç§‘å­¦æŠ¥å‘Šã€‹"

æ ¼å¼ç¤ºä¾‹ï¼š
<textarea>
1.ç¬¬ä¸€è¡Œè¯‘æ–‡
2.ç¬¬äºŒè¡Œè¯‘æ–‡
3.ç¬¬ä¸‰è¡Œè¯‘æ–‡
</textarea>"""
        
        # æ„å»ºsource_text_dictï¼ˆä½¿ç”¨ä¸åŸæ–¹æ³•ç›¸åŒçš„æ ¼å¼ï¼‰
        source_text_dict = {str(i): text for i, text in enumerate(source_texts)}
        
        # æ„å»ºå¾…ç¿»è¯‘æ–‡æœ¬ï¼ˆä½¿ç”¨ä¸åŸTranslatorTaskç›¸åŒçš„æ ¼å¼ï¼‰
        numbered_lines = []
        for index, line in enumerate(source_texts):
            if "\n" in line:
                sub_lines = line.split("\n")
                formatted_line = f"{index + 1}.{sub_lines[0]}"
                for sub_line in sub_lines[1:]:
                    formatted_line += f"\n{sub_line}"
                numbered_lines.append(formatted_line)
            else:
                numbered_lines.append(f"{index + 1}.{line}")
        
        source_text = "\n".join(numbered_lines)
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_str = "\n".join(context_texts[-3:]) if context_texts else ""
        
        # æ„å»ºuser_promptï¼ˆä¸åŸæ–¹æ³•ä¸€è‡´ï¼Œä½¿ç”¨textareaæ ‡ç­¾ï¼‰
        context_prefix = f"###ä¸Šæ–‡å†…å®¹\n{context_str}\n" if context_str else ""
        user_prompt = f"""{context_prefix}###å¾…ç¿»è¯‘æ–‡æœ¬ï¼ˆå…±{len(source_texts)}è¡Œï¼‰
<textarea>
{source_text}
</textarea>

###è¯‘æ–‡è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
âš ï¸ åŸæ–‡æœ‰{len(source_texts)}è¡Œï¼Œè¯‘æ–‡ä¹Ÿå¿…é¡»æœ‰{len(source_texts)}è¡Œï¼Œåºå·ä»1åˆ°{len(source_texts)}
âš ï¸ è¾“å‡ºçš„ç¿»è¯‘é¡ºåºæ ‡å·å¿…é¡»å’Œè¾“å…¥ä¸€ä¸€å¯¹åº”ï¼šè¾“å…¥ç¬¬1è¡Œå¯¹åº”è¾“å‡ºç¬¬1è¡Œï¼Œè¾“å…¥ç¬¬2è¡Œå¯¹åº”è¾“å‡ºç¬¬2è¡Œï¼Œä¾æ­¤ç±»æ¨
âš ï¸ ä¸è¦åˆå¹¶å¤šè¡Œï¼Œä¸è¦è·³è¿‡ä»»ä½•è¡Œï¼Œä¸è¦æ”¹å˜é¡ºåº
âš ï¸ ä¸è¦è‡ªåŠ¨æ·»åŠ ä¹¦åå·ã€Šã€‹æˆ–å…¶ä»–æ ‡ç‚¹ç¬¦å·
<textarea>
1. ï¼ˆç¬¬1è¡Œè¯‘æ–‡ï¼‰
2. ï¼ˆç¬¬2è¡Œè¯‘æ–‡ï¼‰
...
{len(source_texts)}. ï¼ˆç¬¬{len(source_texts)}è¡Œè¯‘æ–‡ï¼‰
</textarea>"""
        
        messages = [{"role": "user", "content": user_prompt}]
        
        try:
            # ç­‰å¾…RequestLimiterå…è®¸å‘é€è¯·æ±‚
            if not self._wait_for_limiter(messages, system_prompt):
                self.warning(f"  âš  RequestLimiteræ£€æŸ¥å¤±è´¥")
                return None
            
            platform_config = self.config.get_platform_configuration("translationReq") if self.config else {}
            
            skip, _, response_content, prompt_tokens, completion_tokens = self._llm_request_with_tracking(
                messages=messages,
                system_prompt=system_prompt,
                platform_config=platform_config
            )
            
            # ğŸ”¥ æ›´æ–°tokenç»Ÿè®¡
            self._update_token_stats(prompt_tokens, completion_tokens)
            
            if not skip and response_content:
                # ä½¿ç”¨ResponseExtractoræå–ç¿»è¯‘ç»“æœï¼ˆä¸åŸTranslatorTaskå®Œå…¨ç›¸åŒï¼‰
                response_dict = ResponseExtractor.text_extraction(self, source_text_dict, response_content)
                
                # å»é™¤æ•°å­—åºå·å‰ç¼€ï¼ˆä¸åŸæ–¹æ³•ç›¸åŒï¼‰
                response_dict = ResponseExtractor.remove_numbered_prefix(self, response_dict)
                
                # åªå–æˆ‘ä»¬éœ€è¦çš„é”®ï¼Œå¿½ç•¥å¤šä½™çš„é”®ï¼ˆä¸åŸæ–¹æ³•ä¸€è‡´ï¼‰
                if response_dict:
                    translated_texts = []
                    for i in range(len(source_texts)):
                        key = str(i)
                        if key in response_dict:
                            translated_texts.append(response_dict[key])
                        else:
                            translated_texts.append("")  # ç¼ºå¤±çš„é”®ç”¨ç©ºå­—ç¬¦ä¸²å¡«å……
                    
                    # åªè¦æœ‰éƒ¨åˆ†è¯‘æ–‡å°±è¿”å›
                    if any(translated_texts):
                        non_empty_count = sum(1 for t in translated_texts if t)
                        self.info(f"  âœ“ æ‰¹é‡{strategy}ç¿»è¯‘æˆåŠŸ: {non_empty_count}/{len(translated_texts)} è¡Œ")
                        return translated_texts
                    else:
                        self.warning(f"  âš  æ‰€æœ‰è¯‘æ–‡å‡ä¸ºç©º")
                        return None
                else:
                    self.warning(f"  âš  ResponseExtractoræœªèƒ½è§£æä»»ä½•ç»“æœ")
                    return None
            else:
                self.warning("  âš  LLMè¿”å›ä¸ºç©º")
                return None
        except Exception as e:
            self.error(f"æ‰¹é‡{strategy}ç¿»è¯‘å¤±è´¥: {e}", e)
            return None
    
    def _check_entity_consistency(self, source_texts: List[str], translated_texts: List[str],
                                  terminology_db: Dict, entity_database: Dict) -> List[str]:
        """
        æ£€æŸ¥å¹¶ä¿®æ­£å®ä½“ä¸€è‡´æ€§é—®é¢˜
        
        ç¡®ä¿ï¼š
        1. äººåã€åœ°åç­‰ä¸“æœ‰åè¯ç¿»è¯‘ä¸€è‡´
        2. æœ¯è¯­åº“ä¸­çš„æœ¯è¯­ç¿»è¯‘ä¸€è‡´
        3. è·¨æ‰¹æ¬¡çš„å®ä½“ç¿»è¯‘ä¿æŒç»Ÿä¸€
        
        Args:
            source_texts: åŸæ–‡åˆ—è¡¨
            translated_texts: è¯‘æ–‡åˆ—è¡¨
            terminology_db: æœ¯è¯­åº“
            entity_database: å®ä½“æ•°æ®åº“ï¼ˆä¼šè¢«æ›´æ–°ï¼‰
        
        Returns:
            ä¿®æ­£åçš„è¯‘æ–‡åˆ—è¡¨
        """
        # ğŸ”¥ å‘é€UIé˜¶æ®µæ›´æ–°ï¼šä¸€è‡´æ€§æ£€æŸ¥é˜¶æ®µï¼ˆä»…ç¬¬ä¸€æ¬¡ï¼‰
        if self._current_cache_project and not hasattr(self, '_entity_check_stage_sent'):
            # ä¸€è‡´æ€§æ£€æŸ¥ï¼šå•æ­¥æ“ä½œ
            self._update_stage_progress(self._current_cache_project, "entity_check", 0, 1)
            self._publish_stage_with_stats(self._current_cache_project, "entity_check", "æ£€æŸ¥ä¸­")
            self._entity_check_stage_sent = True
        
        import re

        def _find_actual_entity_rendering(entity: str, expected_translation: str, text: str) -> Dict[str, str]:
            """
            è¯•å›¾ä»è¯‘æ–‡ä¸­æ‰¾å‡ºâ€œå®ä½“å®é™…å‘ˆç°â€ä¸ºä½•ï¼Œä¾¿äºæ—¥å¿—å®šä½é—®é¢˜ï¼š
            1) ä¼˜å…ˆæ£€æµ‹æ˜¯å¦ä¿ç•™äº†åŸæ–‡å®ä½“ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
            2) å¦åˆ™ä»æœŸæœ›è¯‘æ–‡é‡Œæå–ä¸­æ–‡å­ä¸²ï¼ˆ2-5å­—ï¼‰å»è¯‘æ–‡é‡Œæ‰¾å‘½ä¸­çª—å£
            3) ä»æ‰¾ä¸åˆ°åˆ™è¿”å›æœªçŸ¥
            """
            if not text:
                return {"actual": "", "hint": "è¯‘æ–‡ä¸ºç©º"}

            # 1) æ˜¯å¦ä¿ç•™åŸæ–‡å®ä½“
            try:
                m = re.search(re.escape(entity), text, flags=re.IGNORECASE)
            except Exception:
                m = None
            if m:
                start = max(0, m.start() - 25)
                end = min(len(text), m.end() + 25)
                snippet = text[start:end]
                return {"actual": snippet, "hint": "ç–‘ä¼¼ä¿ç•™åŸæ–‡å®ä½“ï¼ˆæœªæŒ‰æœ¯è¯­è¡¨ç¿»è¯‘ï¼‰"}

            # 2) ä»æœŸæœ›è¯‘æ–‡é‡ŒæŠ½ä¸­æ–‡å­ä¸²åšâ€œå‘½ä¸­çª—å£â€
            zh = re.sub(r"[^\u4e00-\u9fff]+", "", expected_translation or "")
            if zh:
                # ç”Ÿæˆ2-5å­—å­ä¸²ï¼ŒæŒ‰é•¿åº¦ä¼˜å…ˆï¼Œé¿å…å¤ªçŸ­è¯¯å‘½ä¸­
                subs = []
                max_len = min(5, len(zh))
                min_len = 2 if len(zh) >= 2 else 1
                for L in range(max_len, min_len - 1, -1):
                    for i in range(0, len(zh) - L + 1):
                        subs.append(zh[i:i + L])
                seen = set()
                uniq_subs = []
                for s in subs:
                    if s not in seen:
                        seen.add(s)
                        uniq_subs.append(s)

                for s in uniq_subs:
                    idx = text.find(s)
                    if idx != -1:
                        start = max(0, idx - 25)
                        end = min(len(text), idx + len(s) + 25)
                        snippet = text[start:end]
                        return {"actual": snippet, "hint": f"å‘½ä¸­æœŸæœ›è¯‘æ–‡å…³é”®è¯ç‰‡æ®µ: {s}"}

            return {"actual": "", "hint": "æœªæ‰¾åˆ°æ˜æ˜¾å¯¹åº”ç‰‡æ®µï¼ˆå¯èƒ½è¢«æ”¹å†™/çœç•¥/åŒä¹‰æ›¿æ¢ï¼‰"}
        
        # ä»æœ¯è¯­åº“ä¸­æå–å®ä½“æ˜ å°„
        entity_mappings = {}
        # âœ… æ³¨æ„ï¼šterminology_db çš„ key å¯èƒ½å°±æ˜¯æœ¯è¯­æœ¬èº«ï¼›åŒæ—¶è¦è¿‡æ»¤ç©ºtermï¼Œé¿å… "" in text æ°¸è¿œä¸ºçœŸå¯¼è‡´è¯¯æŠ¥
        if isinstance(terminology_db, dict):
            for k, term_info in terminology_db.items():
                if not isinstance(term_info, dict):
                    continue
                raw_term = (term_info.get("term") or k or "")
                raw_translation = (term_info.get("translation") or "")
                term = str(raw_term).strip()
                translation = str(raw_translation).strip()
                if not term or not translation:
                    continue
                # é¢å¤–æ¸…ç†ï¼šé˜²æ­¢æœ¯è¯­åº“é‡Œæ®‹ç•™Markdownæ ‡è®°å½±å“åŒ¹é…
                translation = re.sub(r"\*{1,2}(.+?)\*{1,2}", r"\1", translation).strip()
                translation = re.sub(r"_{1,2}(.+?)_{1,2}", r"\1", translation).strip()
                if not translation:
                    continue
                entity_mappings[term] = translation
        
        # è¾“å‡ºå®ä½“/æœ¯è¯­ç»Ÿè®¡
        if entity_mappings:
            self.debug(f"  â†’ æ­£åœ¨æ£€æŸ¥ {len(entity_mappings)} ä¸ªå®ä½“/æœ¯è¯­çš„ä¸€è‡´æ€§...")
            # æ˜¾ç¤ºå‰5ä¸ªå®ä½“ä½œä¸ºç¤ºä¾‹
            sample_entities = list(entity_mappings.items())[:5]
            for entity, trans in sample_entities:
                self.debug(f"    â€¢ {entity} â†’ {trans}")
            if len(entity_mappings) > 5:
                self.debug(f"    ... ä»¥åŠå…¶ä»– {len(entity_mappings) - 5} ä¸ª")
        else:
            self.debug(f"  â†’ æœªå‘ç°å®ä½“/æœ¯è¯­ï¼Œè·³è¿‡ä¸€è‡´æ€§æ£€æŸ¥")
            return translated_texts
        
        # æ£€æŸ¥å¹¶ä¿®æ­£æ¯ä¸€è¡Œè¯‘æ–‡
        corrected_texts = []
        inconsistency_details = []  # å­˜å‚¨è¯¦ç»†çš„ä¸ä¸€è‡´ä¿¡æ¯
        entities_verified = 0  # ç»Ÿè®¡éªŒè¯é€šè¿‡çš„å®ä½“æ•°é‡
        entities_auto_fixed = 0  # ğŸ”¥ æ–°å¢ï¼šç»Ÿè®¡è‡ªåŠ¨ä¿®æ­£çš„å®ä½“æ•°é‡
        entity_check_log = []  # è®°å½•æ¯ä¸ªå®ä½“çš„æ£€æŸ¥æƒ…å†µ
        
        for line_idx, (source_text, translated_text) in enumerate(zip(source_texts, translated_texts)):
            if not translated_text:
                corrected_texts.append(translated_text)
                continue
            
            corrected_text = translated_text
            line_entities_found = []  # æœ¬è¡Œæ‰¾åˆ°çš„å®ä½“
            line_entities_replaced = []  # æœ¬è¡Œå¼ºåˆ¶æ›¿æ¢çš„å®ä½“
            line_entities_missing = []  # æœ¬è¡Œç¼ºå¤±ä½†æ— æ³•æ›¿æ¢çš„å®ä½“
            
            # ğŸ”¥ å¼ºåˆ¶æ›¿æ¢ï¼šæŒ‰ç…§æœ¯è¯­è¡¨å¼ºåˆ¶æ›¿æ¢å®ä½“ç¿»è¯‘
            for entity, expected_translation in entity_mappings.items():
                # å¦‚æœåŸæ–‡ä¸­æœ‰è¯¥å®ä½“ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
                if entity.lower() in source_text.lower():
                    # æ£€æŸ¥è¯‘æ–‡ä¸­æ˜¯å¦å·²æœ‰æ­£ç¡®ç¿»è¯‘ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
                    normalized_translation = re.sub(r'[\s\-â€“â€”]+', '', expected_translation.lower())
                    normalized_text = re.sub(r'[\s\-â€“â€”]+', '', corrected_text.lower())
                    
                    if normalized_translation in normalized_text or expected_translation.lower() in corrected_text.lower():
                        # è¯‘æ–‡ä¸­å·²åŒ…å«æ­£ç¡®çš„ç¿»è¯‘
                        entities_verified += 1
                        line_entities_found.append(f"{entity}â†’{expected_translation}âœ“")
                        
                        # æ›´æ–°å®ä½“æ•°æ®åº“
                        if entity not in entity_database:
                            entity_database[entity] = {
                                "translation": expected_translation,
                                "occurrences": 1,
                                "source": "terminology_db"
                            }
                        else:
                            entity_database[entity]["occurrences"] += 1
                    else:
                        # ğŸ”¥ ç­–ç•¥1ï¼šè‡ªåŠ¨ä¿®æ­£ä¿ç•™çš„åŸæ–‡å®ä½“
                        # ä»…å½“è¯‘æ–‡é‡Œä»å‡ºç°åŸæ–‡å®ä½“ï¼ˆå¦‚ Beclin / Autophagyï¼‰ä¸”æœŸæœ›è¯‘æ–‡ä¸åŒï¼Œæ‰ç›´æ¥æ›¿æ¢ä¸ºæœŸæœ›è¯‘æ–‡
                        if expected_translation and expected_translation != entity:
                            try:
                                before = corrected_text
                                corrected_text = re.sub(re.escape(entity), expected_translation, corrected_text, flags=re.IGNORECASE)
                                if corrected_text != before:
                                    entities_auto_fixed += 1  # ğŸ”¥ è®¡æ•°è‡ªåŠ¨ä¿®æ­£
                                    entities_verified += 1
                                    line_entities_replaced.append(f"{entity}â†’{expected_translation}âœ“(è‡ªåŠ¨ä¿®æ­£)")
                                    line_entities_found.append(f"{entity}â†’{expected_translation}âœ“(è‡ªåŠ¨ä¿®æ­£)")
                                    self.debug(f"    âœ… [è¡Œ{line_idx+1}] è‡ªåŠ¨ä¿®æ­£ä¿ç•™çš„åŸæ–‡å®ä½“: {entity} â†’ {expected_translation}")
                                    continue
                            except Exception:
                                pass

                        # ä»æ— æ³•ä¿®æ­£ï¼šè®°å½•ä¸ºéœ€è¦é‡æ–°ç¿»è¯‘/äººå·¥å…³æ³¨
                        
                        # å…ˆå°è¯•æ‰¾åˆ°åŸæ–‡ä¸­entityçš„ç¡®åˆ‡ä½ç½®æ¨¡å¼
                        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾entityåœ¨åŸæ–‡ä¸­çš„æ‰€æœ‰åŒ¹é…ä½ç½®
                        entity_pattern = re.compile(re.escape(entity), re.IGNORECASE)
                        matches = list(entity_pattern.finditer(source_text))
                        
                        if matches:
                            # å°è¯•æ™ºèƒ½æ›¿æ¢ï¼šæŸ¥æ‰¾è¯‘æ–‡ä¸­å¯¹åº”ä½ç½®çš„å¯èƒ½é”™è¯¯ç¿»è¯‘
                            # ç®€å•ç­–ç•¥ï¼šå¦‚æœè¯‘æ–‡é•¿åº¦æ¥è¿‘åŸæ–‡ï¼ŒæŒ‰æ¯”ä¾‹å®šä½
                            # å¤æ‚ç­–ç•¥ï¼šä½¿ç”¨LLMé‡æ–°ç¿»è¯‘è¿™ä¸€è¡Œï¼ˆæˆæœ¬è¾ƒé«˜ï¼‰
                            
                            # è¿™é‡Œä½¿ç”¨ç®€å•ç­–ç•¥ï¼šå…¨å±€æœç´¢å¯èƒ½çš„é”™è¯¯ç¿»è¯‘å¹¶æ›¿æ¢
                            # ä¾‹å¦‚ï¼šå¦‚æœentityæ˜¯"Beclin"ï¼Œexpectedæ˜¯"Beclin"ï¼Œä½†è¯‘æ–‡ä¸­æ˜¯"è´å…‹æ—"
                            # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°"è´å…‹æ—"å¹¶æ›¿æ¢ä¸º"Beclin"
                            
                            # ç”±äºä¸çŸ¥é“LLMå…·ä½“æŠŠentityç¿»è¯‘æˆäº†ä»€ä¹ˆï¼Œæˆ‘ä»¬é‡‡ç”¨å¼ºåˆ¶æ’å…¥ç­–ç•¥
                            # åœ¨ç¬¬ä¸€æ¬¡å‡ºç°ç›¸å…³å†…å®¹çš„åœ°æ–¹æ’å…¥æ­£ç¡®ç¿»è¯‘
                            
                            # æ›´ç®€å•çš„ç­–ç•¥ï¼šè®°å½•ä¸ºéœ€è¦é‡æ–°ç¿»è¯‘çš„è¡Œ
                            line_entities_missing.append(f"{entity}â†’{expected_translation}âŒ")
                            actual_info = _find_actual_entity_rendering(entity, expected_translation, corrected_text)
                            # æ§åˆ¶æ—¥å¿—é•¿åº¦ï¼Œé¿å…ä¸€æ¡è¿‡é•¿åˆ·å±
                            actual_snippet = (actual_info.get("actual") or "")
                            if len(actual_snippet) > 160:
                                actual_snippet = actual_snippet[:160] + "..."
                            inconsistency_details.append({
                                "line": line_idx + 1,
                                "entity": entity,
                                "expected": expected_translation,
                                "actual_entity": actual_snippet,
                                "actual_hint": actual_info.get("hint", ""),
                                "source": source_text[:80] + "..." if len(source_text) > 80 else source_text,
                                "translation": corrected_text[:80] + "..." if len(corrected_text) > 80 else corrected_text,
                                "action": "éœ€è¦é‡æ–°ç¿»è¯‘"
                            })
                        else:
                            # åŸæ–‡ä¸­æ²¡æ‰¾åˆ°entityï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰
                            pass
            
            # è®°å½•æœ¬è¡Œçš„æ£€æŸ¥ç»“æœ
            if line_entities_found or line_entities_missing or line_entities_replaced:
                entity_check_log.append({
                    "line": line_idx + 1,
                    "found": line_entities_found,
                    "missing": line_entities_missing,
                    "replaced": line_entities_replaced
                })
            
            corrected_texts.append(corrected_text)
        
        # ğŸ”¥ è¾“å‡ºå®Œæ•´çš„æ£€æŸ¥ç»“æœ
        inconsistencies_found = len(inconsistency_details)
        
        # æ˜¾ç¤ºè‡ªåŠ¨ä¿®æ­£ç»Ÿè®¡
        if entities_auto_fixed > 0:
            self.info(f"  âœ… è‡ªåŠ¨ä¿®æ­£äº† {entities_auto_fixed} å¤„ä¿ç•™çš„åŸæ–‡å®ä½“")
        
        if inconsistencies_found > 0:
            self.warning(f"  âš  å‘ç° {inconsistencies_found} å¤„æ— æ³•è‡ªåŠ¨ä¿®æ­£çš„å®ä½“ä¸€è‡´æ€§é—®é¢˜ï¼Œ"
                        f"{entities_verified} ä¸ªå®ä½“ç¿»è¯‘æ­£ç¡®ï¼ˆå« {entities_auto_fixed} ä¸ªè‡ªåŠ¨ä¿®æ­£ï¼‰")
            
            # ğŸ”¥ æ˜¾ç¤ºæ‰€æœ‰ä¸ä¸€è‡´çš„è¯¦ç»†ä¿¡æ¯ï¼ˆä¸é™åˆ¶æ•°é‡ï¼‰
            self.warning(f"  â†’ ã€æ— æ³•è‡ªåŠ¨ä¿®æ­£çš„é—®é¢˜åˆ—è¡¨ã€‘å…± {inconsistencies_found} å¤„ï¼š")
            for i, detail in enumerate(inconsistency_details, 1):
                self.warning(f"  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
                self.warning(f"    é—®é¢˜ {i}/{inconsistencies_found}")
                self.warning(f"    ã€è¡Œå·ã€‘: {detail['line']}")
                self.warning(f"    ã€åŸæ–‡å®ä½“ã€‘: '{detail['entity']}'")
                self.warning(f"    ã€æœŸæœ›è¯‘æ–‡ã€‘: '{detail['expected']}'")
                if detail.get("actual_entity") or detail.get("actual_hint"):
                    self.warning(f"    ã€è¯‘æ–‡ä¸­å®ä½“å‘ˆç°ã€‘: {detail.get('actual_entity', '')}")
                    self.warning(f"    ã€åˆ¤å®šä¾æ®ã€‘: {detail.get('actual_hint', '')}")
                self.warning(f"    ã€åŸæ–‡ç‰‡æ®µã€‘: {detail['source']}")
                self.warning(f"    ã€å®é™…è¯‘æ–‡ã€‘: {detail['translation']}")
                self.warning(f"    ã€å¤„ç†æ–¹å¼ã€‘: {detail.get('action', 'æœªå¤„ç†')}")
            self.warning(f"  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
            
            # ç»Ÿè®¡æœ€å¸¸å‡ºç°é—®é¢˜çš„å®ä½“
            problem_entities = {}
            for detail in inconsistency_details:
                entity = detail['entity']
                problem_entities[entity] = problem_entities.get(entity, 0) + 1
            
            self.warning(f"  â†’ ã€é—®é¢˜å®ä½“ç»Ÿè®¡ã€‘ï¼ˆå‡ºç°æ¬¡æ•°ï¼‰ï¼š")
            for entity, count in sorted(problem_entities.items(), key=lambda x: x[1], reverse=True):
                expected = next((d['expected'] for d in inconsistency_details if d['entity'] == entity), "?")
                self.warning(f"    â€¢ {entity} (æœŸæœ›: {expected}): {count}æ¬¡")
            
        else:
            if entities_verified > 0:
                auto_fix_info = f"ï¼ˆå« {entities_auto_fixed} ä¸ªè‡ªåŠ¨ä¿®æ­£ï¼‰" if entities_auto_fixed > 0 else ""
                self.info(f"  âœ“ å®ä½“ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡ï¼š{entities_verified} ä¸ªå®ä½“ç¿»è¯‘ä¸€è‡´{auto_fix_info}")
                # æ˜¾ç¤ºéªŒè¯é€šè¿‡çš„å®ä½“
                if entity_check_log:
                    self.debug(f"  â†’ éªŒè¯é€šè¿‡çš„å®ä½“è¯¦æƒ…ï¼š")
                    for log in entity_check_log[:10]:  # æ˜¾ç¤ºå‰10è¡Œ
                        if log["found"]:
                            self.debug(f"    ã€è¡Œ{log['line']}ã€‘{', '.join(log['found'][:5])}")
            else:
                self.debug(f"  âœ“ æœ¬æ‰¹æ¬¡æœªæ£€æµ‹åˆ°éœ€è¦éªŒè¯çš„å®ä½“")
        
        # ğŸ”¥ æ›´æ–°è¿›åº¦ï¼šæ£€æŸ¥å®Œæˆ
        if self._current_cache_project:
            self._update_stage_progress(self._current_cache_project, "entity_check", 1, 1)
        
        return corrected_texts
