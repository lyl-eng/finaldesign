"""
æœ¯è¯­ä¸å®ä½“Agent (Agent 1)
è´Ÿè´£æœ¯è¯­è¯†åˆ«ã€çŸ¥è¯†åº“é›†æˆå’Œå…¨å±€ä¸€è‡´æ€§ä¿éšœ
"""

import json
import os
import re
from typing import Dict, Any, List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from .BaseAgent import BaseAgent
from ModuleFolders.NERProcessor.NERProcessor import NERProcessor
from ModuleFolders.LLMRequester.LLMRequester import LLMRequester
from ModuleFolders.Cache.CacheProject import CacheProject
from ModuleFolders.ResponseExtractor.ResponseExtractor import ResponseExtractor


class TerminologyEntityAgent(BaseAgent):
    """
    Agent 1: æœ¯è¯­ä¸å®ä½“Agent
    åŠŸèƒ½ï¼š
    1. æ™ºèƒ½æœ¯è¯­è¯†åˆ«ï¼ˆNERã€é¢†åŸŸæœ¯è¯­ã€æ–‡åŒ–è´Ÿè½½è¯ï¼‰
    2. çŸ¥è¯†åº“é›†æˆï¼ˆRAGï¼‰åŠmemory
    3. å…¨å±€ä¸€è‡´æ€§æ§åˆ¶
    """
    
    def __init__(self, config=None):
        super().__init__(
            name="TerminologyEntityAgent",
            description="æœ¯è¯­è¯†åˆ«ä¸å…¨å±€ä¸€è‡´æ€§ä¿éšœAgent",
            config=config
        )
        
        self.ner_processor = NERProcessor()
        self.llm_requester = LLMRequester()
        self.response_extractor = ResponseExtractor()
        
        # æœ¯è¯­åº“å­˜å‚¨ï¼ˆé¡¹ç›®ä¸“å±ï¼‰
        self.terminology_db = {}  # {term: {translation, type, context, strategy}}
        
        # Memoryå­˜å‚¨
        self.memory_storage = {
            "translated_texts": [],
            "text_summaries": [],
            "reader_preferences": {},
            "translation_style_guide": {}
        }
        
        # ğŸ”¥ ç”¨äºtokenç»Ÿè®¡
        self._current_cache_project = None
        
        # ğŸ†• è¯­è¨€åˆ°NERæ¨¡å‹çš„æ˜ å°„
        self.language_model_map = {
            "japanese": "ja_core_news_md",
            "english": "en_core_web_sm",
            "chinese_simplified": "zh_core_web_sm",
            "chinese_traditional": "zh_core_web_sm",
            "korean": "ko_core_news_sm",
            "german": "de_core_news_sm",
            "french": "fr_core_news_sm",
            "spanish": "es_core_news_sm",
            "russian": "ru_core_news_sm"
        }
    
    def _update_token_stats(self, prompt_tokens: int, completion_tokens: int):
        """æ›´æ–°tokenç»Ÿè®¡å¹¶å‘é€UIæ›´æ–°äº‹ä»¶"""
        if not self._current_cache_project or not self._current_cache_project.stats_data:
            return
        
        from Base.Base import Base
        import time
        
        # ğŸ”¥ ä½¿ç”¨atomic_scopeç¡®ä¿çº¿ç¨‹å®‰å…¨
        with self._current_cache_project.stats_data.atomic_scope():
            # ğŸ”¥ æ›´æ–°æ€»tokenæ•°ï¼ˆprompt + completionï¼‰
            if prompt_tokens or completion_tokens:
                self._current_cache_project.stats_data.token += (prompt_tokens or 0) + (completion_tokens or 0)
            
            # ğŸ”¥ æ›´æ–°completion_tokensï¼ˆç”¨äºæˆæœ¬è®¡ç®—ï¼‰
            if completion_tokens:
                self._current_cache_project.stats_data.total_completion_tokens += completion_tokens
            
            # æ›´æ–°è¯·æ±‚è®¡æ•°
            self._current_cache_project.stats_data.total_requests += 1
            
            # ğŸ”¥ æ›´æ–°å·²æ¶ˆè€—æ—¶é—´ï¼ˆä¸åŸTaskExecutorä¿æŒä¸€è‡´ï¼‰
            self._current_cache_project.stats_data.time = time.time() - self._current_cache_project.stats_data.start_time
            
            # ğŸ”¥ ç«‹å³å‘é€UIæ›´æ–°äº‹ä»¶ï¼Œç¡®ä¿tokenç»Ÿè®¡å®æ—¶æ›´æ–°
            stats_dict = self._current_cache_project.stats_data.to_dict()
        
        # åœ¨atomic_scopeå¤–å‘é€äº‹ä»¶
        self.emit(Base.EVENT.TASK_UPDATE, stats_dict)
    
    def _llm_request_with_tracking(self, messages, system_prompt, platform_config):
        """
        åŒ…è£…LLMè¯·æ±‚ï¼Œè‡ªåŠ¨è¿½è¸ªæ´»è·ƒè°ƒç”¨æ•°
        
        Returns:
            (skip, response_think, response_content, prompt_tokens, completion_tokens)
        """
        if not self._current_cache_project or not self._current_cache_project.stats_data:
            return self.llm_requester.sent_request(messages, system_prompt, platform_config)
        
        from Base.Base import Base
        
        try:
            # ğŸ”¥ è°ƒç”¨å‰ï¼šå¢åŠ æ´»è·ƒLLMè°ƒç”¨è®¡æ•°å¹¶ç«‹å³å‘é€äº‹ä»¶
            with self._current_cache_project.stats_data.atomic_scope():
                self._current_cache_project.stats_data.active_llm_calls += 1
                stats_dict = self._current_cache_project.stats_data.to_dict()
            self.emit(Base.EVENT.TASK_UPDATE, stats_dict)
            
            # æ‰§è¡ŒLLMè¯·æ±‚
            result = self.llm_requester.sent_request(messages, system_prompt, platform_config)
            
            return result
        finally:
            # ğŸ”¥ è°ƒç”¨åï¼šå‡å°‘æ´»è·ƒLLMè°ƒç”¨è®¡æ•°å¹¶ç«‹å³å‘é€äº‹ä»¶
            with self._current_cache_project.stats_data.atomic_scope():
                self._current_cache_project.stats_data.active_llm_calls = max(0, self._current_cache_project.stats_data.active_llm_calls - 1)
                stats_dict = self._current_cache_project.stats_data.to_dict()
            self.emit(Base.EVENT.TASK_UPDATE, stats_dict)
    
    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œæœ¯è¯­è¯†åˆ«å’Œä¸€è‡´æ€§ä¿éšœä»»åŠ¡
        
        Args:
            input_data: åŒ…å«cache_projectå’Œmetadataçš„å­—å…¸
            
        Returns:
            åŒ…å«æœ¯è¯­åº“å’Œæ›´æ–°åçš„cache_projectçš„å­—å…¸
        """
        self.log_agent_action("å¼€å§‹æ‰§è¡Œæœ¯è¯­ä¸å®ä½“è¯†åˆ«")
        
        cache_project: CacheProject = input_data.get("cache_project")
        metadata = input_data.get("metadata", {})
        
        # ğŸ”¥ ä¿å­˜cache_projectå¼•ç”¨ï¼Œç”¨äºtokenç»Ÿè®¡
        self._current_cache_project = cache_project
        
        if not cache_project:
            self.error("æœªæ‰¾åˆ°cache_projectæ•°æ®")
            return {"success": False, "error": "ç¼ºå°‘cache_project"}
        
        self.info("=" * 60)
        self.info("é˜¶æ®µ2: æœ¯è¯­ä¸å®ä½“è¯†åˆ«")
        self.info("=" * 60)
        
        # ğŸ†• æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æœ¯è¯­åº“ï¼ˆæ”¯æŒå¤ç”¨ï¼‰
        existing_terminology = cache_project.extra.get("terminology_database", {})
        existing_memory = cache_project.extra.get("memory_storage", {})
        
        if existing_terminology and len(existing_terminology) > 0:
            self.info(f"âœ… æ£€æµ‹åˆ°å·²æœ‰æœ¯è¯­åº“ï¼ˆ{len(existing_terminology)} ä¸ªæœ¯è¯­ï¼‰ï¼Œç›´æ¥å¤ç”¨")
            self.terminology_db = existing_terminology
            
            if existing_memory and len(existing_memory) > 0:
                self.memory_storage = existing_memory
                self.info(f"âœ… æ£€æµ‹åˆ°å·²æœ‰Memoryå­˜å‚¨ï¼Œç›´æ¥å¤ç”¨")
            
            self.info("=" * 60 + "\n")
            self.log_agent_action("æœ¯è¯­åº“å¤ç”¨", f"å¤ç”¨äº† {len(self.terminology_db)} ä¸ªæœ¯è¯­")
            
            return {
                "success": True,
                "cache_project": cache_project,
                "terminology_database": self.terminology_db,
                "memory_storage": self.memory_storage
            }
        
        # å¦‚æœæ²¡æœ‰å·²æœ‰æœ¯è¯­åº“ï¼Œæ‰§è¡Œæ­£å¸¸çš„è¯†åˆ«æµç¨‹
        self.info("æœªæ£€æµ‹åˆ°æœ¯è¯­åº“ï¼Œå¼€å§‹æ™ºèƒ½è¯†åˆ«...")
        
        # 1. æ™ºèƒ½æœ¯è¯­è¯†åˆ«
        self.info("â†’ æ‰§è¡Œæ™ºèƒ½æœ¯è¯­è¯†åˆ«ï¼ˆNERã€é¢†åŸŸæœ¯è¯­ã€æ–‡åŒ–è´Ÿè½½è¯ï¼‰...")
        terminology_results = self._identify_terminology(cache_project, metadata)
        self.info(f"âœ“ è¯†åˆ«åˆ° {len(terminology_results)} ä¸ªæ½œåœ¨æœ¯è¯­")
        
        # 2. çŸ¥è¯†åº“é›†æˆï¼ˆRAGï¼‰å’ŒæŸ¥è¯
        self.info("â†’ æ‰§è¡ŒçŸ¥è¯†åº“é›†æˆä¸æŸ¥è¯...")
        verified_terminology = self._verify_and_enrich_terminology(terminology_results)
        self.info(f"âœ“ æŸ¥è¯å®Œæˆï¼Œç¡®è®¤ {len(verified_terminology)} ä¸ªæœ¯è¯­")
        
        # 3. æ„å»ºé¡¹ç›®ä¸“å±æœ¯è¯­åº“
        self.info("â†’ æ„å»ºé¡¹ç›®ä¸“å±æœ¯è¯­åº“...")
        self._build_terminology_database(verified_terminology)
        self.info(f"âœ“ æœ¯è¯­åº“æ„å»ºå®Œæˆï¼Œå…± {len(self.terminology_db)} ä¸ªæœ¯è¯­")
        
        # 4. æ›´æ–°Memory
        self.info("â†’ æ›´æ–°Memoryå­˜å‚¨...")
        self._update_memory(cache_project, metadata)
        self.info(f"âœ“ Memoryæ›´æ–°å®Œæˆ")
        self.info("=" * 60 + "\n")
        
        # 5. å°†æœ¯è¯­åº“ä¿å­˜åˆ°é¡¹ç›®
        cache_project.extra["terminology_database"] = self.terminology_db
        cache_project.extra["memory_storage"] = self.memory_storage
        
        self.log_agent_action("æœ¯è¯­è¯†åˆ«å®Œæˆ", f"è¯†åˆ«åˆ° {len(self.terminology_db)} ä¸ªæœ¯è¯­")
        
        return {
            "success": True,
            "cache_project": cache_project,
            "terminology_database": self.terminology_db,
            "memory_storage": self.memory_storage
        }
    
    def _identify_terminology(self, cache_project: CacheProject, metadata: Dict) -> List[Dict]:
        """
        æ™ºèƒ½æœ¯è¯­è¯†åˆ«
        è¯†åˆ«ä¸‰ç±»å…³é”®è¯­è¨€å•ä½ï¼š
        1. å‘½åå®ä½“ï¼ˆNERï¼‰
        2. é¢†åŸŸæœ¯è¯­
        3. æ–‡åŒ–è´Ÿè½½è¯ä¸ä¹ è¯­
        """
        self.log_agent_action("æ‰§è¡Œæ™ºèƒ½æœ¯è¯­è¯†åˆ«")
        
        all_results = []
        
        # æ”¶é›†æ‰€æœ‰æ–‡æœ¬æ•°æ®
        items_data = []
        for file_path, cache_file in cache_project.files.items():
            for item in cache_file.items:
                if item.source_text and item.source_text.strip():
                    items_data.append({
                        "source_text": item.source_text,
                        "file_path": file_path
                    })
        
        # ğŸ†• 1. ä½¿ç”¨NERå¤„ç†å™¨è¯†åˆ«å‘½åå®ä½“ï¼ˆè‡ªåŠ¨æ ¹æ®æºè¯­è¨€é€‰æ‹©æ¨¡å‹ï¼‰
        ner_model = self._select_ner_model()
        if ner_model:
            self.info(f"â†’ ä½¿ç”¨NERæ¨¡å‹è¯†åˆ«å‘½åå®ä½“: {ner_model}")
            entity_types = ["PERSON", "ORG", "GPE", "LOC", "PRODUCT", "EVENT", "WORK_OF_ART"]
            ner_results = self.ner_processor.extract_terms(
                items_data=items_data,
                model_name=ner_model,
                entity_types=entity_types
            )
            
            for result in ner_results:
                result["category"] = "named_entity"
                result["priority"] = "high"  # å‘½åå®ä½“ä¼˜å…ˆçº§é«˜
            all_results.extend(ner_results)
            self.info(f"âœ“ NERè¯†åˆ«å®Œæˆï¼Œè¯†åˆ«åˆ° {len(ner_results)} ä¸ªå‘½åå®ä½“")
        else:
            self.info("â†’ æœªæ‰¾åˆ°åˆé€‚çš„NERæ¨¡å‹ï¼Œè·³è¿‡NERè¯†åˆ«")
        
        # 2. ä½¿ç”¨LLMè¯†åˆ«é¢†åŸŸæœ¯è¯­å’Œæ–‡åŒ–è´Ÿè½½è¯
        domain = metadata.get("domain", "general")
        llm_terminology = self._identify_terminology_with_llm(items_data, domain)
        all_results.extend(llm_terminology)
        
        return all_results
    
    def _select_ner_model(self) -> Optional[str]:
        """
        ğŸ†• æ ¹æ®é…ç½®çš„æºè¯­è¨€è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„NERæ¨¡å‹
        
        Returns:
            æ¨¡å‹åç§°ï¼Œå¦‚æœæ²¡æœ‰åˆé€‚çš„æ¨¡å‹åˆ™è¿”å›None
        """
        if not self.config:
            return None
        
        # è·å–æºè¯­è¨€é…ç½®
        source_language = getattr(self.config, 'source_language', 'auto')
        
        # å¦‚æœæ˜¯è‡ªåŠ¨æ£€æµ‹ï¼Œå°è¯•ä»é¡¹ç›®ä¸­æ£€æµ‹ä¸»è¦è¯­è¨€
        if source_language == 'auto':
            self.debug("æºè¯­è¨€ä¸ºè‡ªåŠ¨æ£€æµ‹ï¼Œæš‚æ—¶è·³è¿‡NERè¯†åˆ«ï¼ˆéœ€è¦å®é™…æ–‡æœ¬æ‰èƒ½æ£€æµ‹è¯­è¨€ï¼‰")
            # TODO: å¯ä»¥åœ¨è¿™é‡Œè°ƒç”¨è¯­è¨€æ£€æµ‹é€»è¾‘
            return None
        
        # æ ¹æ®è¯­è¨€æ˜ å°„é€‰æ‹©æ¨¡å‹
        model_name = self.language_model_map.get(source_language)
        if not model_name:
            self.debug(f"è¯­è¨€ '{source_language}' æ²¡æœ‰å¯¹åº”çš„NERæ¨¡å‹æ˜ å°„")
            return None
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        model_path = os.path.join('.', 'Resource', 'Models', 'ner', model_name)
        if not os.path.exists(model_path):
            self.warning(f"NERæ¨¡å‹ä¸å­˜åœ¨: {model_path}ï¼Œè·³è¿‡NERè¯†åˆ«")
            self.info(f"ğŸ’¡ æç¤º: å¯ä»¥ä» https://spacy.io/models ä¸‹è½½ {model_name} å¹¶è§£å‹åˆ° {model_path}")
            return None
        
        return model_name
    
    def _identify_terminology_with_llm(self, items_data: List[Dict], domain: str) -> List[Dict]:
        """
        ğŸ”¥ ä½¿ç”¨LLMè¯†åˆ«é¢†åŸŸæœ¯è¯­å’Œæ–‡åŒ–è´Ÿè½½è¯ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰
        ç›´æ¥å¤ç”¨æ™ºèƒ½åˆ†å—å·¥å…·æ–¹æ³•
        """
        self.log_agent_action("ä½¿ç”¨LLMè¯†åˆ«é¢†åŸŸæœ¯è¯­å’Œæ–‡åŒ–è´Ÿè½½è¯")
        
        # ğŸ”¥ ç›´æ¥ä½¿ç”¨æ™ºèƒ½åˆ†å—å·¥å…·æ–¹æ³•ï¼ˆä¸ç¿»è¯‘agentå®Œå…¨ç›¸åŒçš„é€»è¾‘ï¼‰
        chunks = self._smart_chunk_by_chars(items_data, max_chars=6000, get_text_func=lambda x: x["source_text"])
        
        if len(chunks) > 1:
            self.info(f"  æ–‡æœ¬è¾ƒå¤šï¼ˆ{len(items_data)}æ¡ï¼‰ï¼Œæ™ºèƒ½åˆ†å—ä¸º {len(chunks)} æ‰¹ï¼Œå¹¶è¡Œè¯†åˆ«...")
        
        # ğŸ”¥ ã€å¹¶è¡Œå¤„ç†ã€‘ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè¯†åˆ«æ‰€æœ‰æ‰¹æ¬¡
        all_terms = []
        
        if len(chunks) == 1:
            # åªæœ‰1æ‰¹ï¼Œç›´æ¥ä¸²è¡Œå¤„ç†
            all_terms = self._identify_chunk_terms(chunks[0], 1, len(chunks), domain)
        else:
            # å¤šæ‰¹å¹¶è¡Œå¤„ç†
            with ThreadPoolExecutor(max_workers=min(len(chunks), 5)) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_chunk = {
                    executor.submit(self._identify_chunk_terms, chunk, idx, len(chunks), domain): idx
                    for idx, chunk in enumerate(chunks, 1)
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_chunk):
                    chunk_idx = future_to_chunk[future]
                    try:
                        chunk_terms = future.result()
                        all_terms.extend(chunk_terms)
                    except Exception as e:
                        self.error(f"ç¬¬ {chunk_idx} æ‰¹æœ¯è¯­è¯†åˆ«å¤±è´¥: {e}", e)
        
        # å»é‡ï¼ˆåŸºäºæœ¯è¯­åç§°ï¼‰
        unique_terms = {}
        for term in all_terms:
            term_name = term.get("term", "").lower()
            if term_name and term_name not in unique_terms:
                unique_terms[term_name] = term
        
        final_terms = list(unique_terms.values())
        if len(chunks) > 1:
            self.info(f"âœ“ å¹¶è¡Œè¯†åˆ«å®Œæˆï¼Œæ€»è®¡è¯†åˆ«åˆ° {len(final_terms)} ä¸ªç‹¬ç«‹æœ¯è¯­ï¼ˆå»é‡åï¼‰")
        
        return final_terms
    
    def _identify_chunk_terms(self, chunk: List[Dict], chunk_idx: int, total_chunks: int, domain: str) -> List[Dict]:
        """
        ğŸ†• è¯†åˆ«å•ä¸ªchunkçš„æœ¯è¯­ï¼ˆç”¨äºå¹¶è¡Œå¤„ç†ï¼‰
        
        Args:
            chunk: å¾…è¯†åˆ«çš„æ–‡æœ¬chunk
            chunk_idx: å½“å‰chunkç´¢å¼•
            total_chunks: æ€»chunkæ•°é‡
            domain: é¢†åŸŸ
            
        Returns:
            è¯†åˆ«åˆ°çš„æœ¯è¯­åˆ—è¡¨
        """
        # æ„å»ºæç¤ºè¯ï¼ˆå–æ¯ä¸ªitemçš„å‰200å­—ç¬¦ï¼‰
        sample_texts = [item["source_text"][:200] for item in chunk]
        sample_text = "\n---\n".join(sample_texts)
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æœ¯è¯­è¯†åˆ«ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­è¯†åˆ«ï¼š
1. é¢†åŸŸæœ¯è¯­ï¼šä¸“ä¸šæˆ–é¢†åŸŸç‰¹æœ‰çš„è¯æ±‡å’ŒçŸ­è¯­ï¼ˆå¦‚"{domain}"é¢†åŸŸçš„ä¸“ä¸šæœ¯è¯­ï¼‰
2. æ–‡åŒ–è´Ÿè½½è¯ï¼šç¼ºä¹ç›´æ¥å¯¹ç­‰è¡¨è¾¾çš„è¯æ±‡å’Œä¹ è¯­

æ³¨æ„ï¼š
- åªè¯†åˆ«çœŸæ­£éœ€è¦å›ºå®šç¿»è¯‘çš„æœ¯è¯­ï¼ˆå¦‚ä¸“æœ‰åè¯ã€ä¸“ä¸šæœ¯è¯­ï¼‰
- ä¸è¦è¯†åˆ«æ™®é€šè¯æ±‡
- ä¼˜å…ˆè¯†åˆ«å‡ºç°é¢‘ç‡é«˜çš„æœ¯è¯­

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯†åˆ«ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "terms": [
        {{
            "term": "æœ¯è¯­åŸæ–‡",
            "category": "domain_term" æˆ– "cultural_expression",
            "context": "å‡ºç°ä¸Šä¸‹æ–‡",
            "meaning": "è¯­ä¹‰è§£é‡Š",
            "translation_strategy": "ç›´è¯‘/æ„è¯‘/è¯­ä¹‰è¡¥å¿"
        }}
    ]
}}"""
        
        messages = [{
            "role": "user",
            "content": f"è¯·è¯†åˆ«ä»¥ä¸‹æ–‡æœ¬ä¸­çš„é¢†åŸŸæœ¯è¯­å’Œæ–‡åŒ–è´Ÿè½½è¯ï¼š\n\n{sample_text}"
        }]
        
        # è°ƒç”¨LLM
        try:
            platform_config = self.config.get_platform_configuration("translationReq") if self.config else {}
            skip, _, response_content, prompt_tokens, completion_tokens = self._llm_request_with_tracking(
                messages=messages,
                system_prompt=system_prompt,
                platform_config=platform_config
            )
            
            # ğŸ”¥ æ›´æ–°tokenç»Ÿè®¡
            self._update_token_stats(prompt_tokens, completion_tokens)
            
            if not skip and response_content:
                # è§£æJSONå“åº”
                try:
                    json_start = response_content.find("{")
                    json_end = response_content.rfind("}") + 1
                    if json_start >= 0 and json_end > json_start:
                        json_str = response_content[json_start:json_end]
                        result = json.loads(json_str)
                        
                        chunk_terms = []
                        for term_info in result.get("terms", []):
                            chunk_terms.append({
                                "term": term_info.get("term"),
                                "type": term_info.get("category", "unknown"),
                                "context": term_info.get("context", ""),
                                "meaning": term_info.get("meaning", ""),
                                "translation_strategy": term_info.get("translation_strategy", ""),
                                "category": term_info.get("category", "domain_term"),
                                "priority": "medium"
                            })
                        
                        if total_chunks > 1:
                            self.info(f"  âœ“ ç¬¬ {chunk_idx}/{total_chunks} æ‰¹è¯†åˆ«åˆ° {len(chunk_terms)} ä¸ªæœ¯è¯­")
                        
                        return chunk_terms
                        
                except json.JSONDecodeError:
                    self.warning(f"ç¬¬ {chunk_idx} æ‰¹LLMè¿”å›çš„JSONæ ¼å¼ä¸æ­£ç¡®")
        except Exception as e:
            self.error(f"ç¬¬ {chunk_idx} æ‰¹LLMæœ¯è¯­è¯†åˆ«å¤±è´¥: {e}", e)
        
        return []
    
    
    def _smart_chunk_by_chars(self, items: List, max_chars: int, get_text_func) -> List[List]:
        """
        ğŸ”¥ ã€é€šç”¨æ™ºèƒ½åˆ†å—å·¥å…·ã€‘- æŒ‰å­—ç¬¦æ•°æ™ºèƒ½åˆ†å—
        å¯ç”¨äºä»»ä½•éœ€è¦åˆ†å—çš„åœºæ™¯ï¼ˆæœ¯è¯­è¯†åˆ«ã€ç¿»è¯‘ã€æŸ¥è¯ç­‰ï¼‰
        
        è¿™æ˜¯ä»ç¿»è¯‘agentæå–çš„æ ¸å¿ƒåˆ†å—é€»è¾‘ï¼Œä¿è¯æ‰€æœ‰agentä½¿ç”¨å®Œå…¨ç›¸åŒçš„åˆ†å—ç­–ç•¥
        
        Args:
            items: å¾…åˆ†å—çš„åˆ—è¡¨ï¼ˆå¯ä»¥æ˜¯ä»»ä½•ç±»å‹ï¼‰
            max_chars: å•ä¸ªchunkçš„æœ€å¤§å­—ç¬¦æ•°
            get_text_func: ä»itemä¸­æå–æ–‡æœ¬çš„å‡½æ•°ï¼ˆä¾‹å¦‚ï¼šlambda x: x["source_text"]ï¼‰
            
        Returns:
            chunks: åˆ†å—åçš„åˆ—è¡¨
            
        Example:
            # åˆ†å—Dictåˆ—è¡¨
            chunks = _smart_chunk_by_chars(items_data, 6000, lambda x: x["source_text"])
            
            # åˆ†å—æœ¯è¯­åˆ—è¡¨ï¼ˆæœ¯è¯­è¾ƒçŸ­ï¼Œå¯ä»¥è®¾ç½®æ›´å¤§çš„batchï¼‰
            batches = _smart_chunk_by_chars(terms_list, 3000, lambda x: x.get("term", ""))
        """
        chunks = []
        current_chunk = []
        chunk_chars = 0
        
        for item in items:
            text = get_text_func(item)
            text_length = len(text)
            
            # ğŸ”¥ æç«¯è¶…é•¿æ–‡æœ¬ï¼ˆè¶…è¿‡max_charsï¼‰å•ç‹¬æˆchunk
            if text_length > max_chars:
                # å…ˆæäº¤å½“å‰chunkï¼ˆå¦‚æœæœ‰ï¼‰
                if current_chunk:
                    chunks.append(current_chunk)
                    current_chunk = []
                    chunk_chars = 0
                
                # æç«¯è¶…é•¿æ–‡æœ¬å•ç‹¬æˆchunk
                chunks.append([item])
                continue
            
            # ğŸ”¥ æ™ºèƒ½æ‰“åŒ…ï¼šæŒ‰æ€»å­—ç¬¦æ•°é™åˆ¶
            # å¦‚æœåŠ å…¥å½“å‰itemä¼šè¶…è¿‡max_charsï¼Œå…ˆæäº¤å½“å‰chunk
            if current_chunk and (chunk_chars + text_length > max_chars):
                chunks.append(current_chunk)
                current_chunk = []
                chunk_chars = 0
            
            # æ·»åŠ å½“å‰itemåˆ°chunk
            current_chunk.append(item)
            chunk_chars += text_length
        
        # å¤„ç†æœ€åä¸€ä¸ªchunk
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def _verify_and_enrich_terminology(self, terminology_results: List[Dict]) -> List[Dict]:
        """
        çŸ¥è¯†åº“é›†æˆï¼ˆRAGï¼‰åŠæŸ¥è¯
        åˆ©ç”¨å¤–éƒ¨èµ„æºå¯¹è¯†åˆ«å‡ºçš„å®ä½“å’Œæœ¯è¯­è¿›è¡ŒæŸ¥è¯
        ğŸ†• æ‰¹é‡å¤„ç†ä¼˜åŒ–ï¼šä¸€æ¬¡LLMè°ƒç”¨æŸ¥è¯æ‰€æœ‰æ–°æœ¯è¯­
        """
        self.log_agent_action("æ‰§è¡Œæœ¯è¯­æŸ¥è¯å’ŒçŸ¥è¯†åº“é›†æˆ")
        
        verified_results = []
        new_terms_to_enrich = []  # éœ€è¦LLMæŸ¥è¯çš„æ–°æœ¯è¯­
        new_terms_indices = []    # è®°å½•æ–°æœ¯è¯­åœ¨ç»“æœåˆ—è¡¨ä¸­çš„ä½ç½®
        
        # ç¬¬ä¸€éï¼šåˆ†ç¦»å·²æœ‰æœ¯è¯­å’Œæ–°æœ¯è¯­
        for idx, term_info in enumerate(terminology_results):
            term = term_info.get("term")
            
            # 1. æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨äºæœ¯è¯­åº“
            if term in self.terminology_db:
                verified_info = self.terminology_db[term].copy()
                verified_info.update(term_info)
                verified_results.append(verified_info)
            else:
                # 2. æ”¶é›†éœ€è¦æŸ¥è¯çš„æ–°æœ¯è¯­
                new_terms_to_enrich.append(term_info)
                new_terms_indices.append(len(verified_results))
                verified_results.append(None)  # å ä½
        
        # æ‰¹é‡æŸ¥è¯æ–°æœ¯è¯­ï¼ˆå¦‚æœæœ‰ï¼‰
        if new_terms_to_enrich:
            self.info(f"â†’ æ‰¹é‡æŸ¥è¯ {len(new_terms_to_enrich)} ä¸ªæ–°æœ¯è¯­...")
            
            # ğŸ”¥ ç›´æ¥ä½¿ç”¨æ™ºèƒ½åˆ†å—å·¥å…·æ–¹æ³•ï¼ˆä¸é¢†åŸŸè¯†åˆ«ã€ç¿»è¯‘å®Œå…¨ç›¸åŒçš„é€»è¾‘ï¼‰
            # æœ¯è¯­ç›¸å¯¹è¾ƒçŸ­ï¼Œå¯ä»¥è®¾ç½®è¾ƒå¤§çš„batchä»¥å‡å°‘LLMè°ƒç”¨æ¬¡æ•°
            batches = self._smart_chunk_by_chars(new_terms_to_enrich, max_chars=3000, get_text_func=lambda x: x.get("term", ""))
            all_enriched = []
            
            for batch_num, batch in enumerate(batches, 1):
                if len(batches) > 1:
                    self.info(f"  å¤„ç†ç¬¬ {batch_num}/{len(batches)} æ‰¹ï¼ˆ{len(batch)} ä¸ªæœ¯è¯­ï¼‰")
                
                enriched_batch = self._batch_enrich_terms_with_llm(batch)
                all_enriched.extend(enriched_batch)
            
            # å¡«å……æŸ¥è¯ç»“æœ
            for idx, enriched_info in zip(new_terms_indices, all_enriched):
                verified_results[idx] = enriched_info
        
        return verified_results
    
    def _batch_enrich_terms_with_llm(self, terms_list: List[Dict]) -> List[Dict]:
        """
        ğŸ”¥ æ‰¹é‡ä½¿ç”¨LLMæŸ¥è¯æœ¯è¯­ç¿»è¯‘
        é‡‡ç”¨ä¸ç¿»è¯‘agentç›¸åŒçš„ <textarea> æ ¼å¼ + ResponseExtractor è§£æ
        
        Args:
            terms_list: å¾…æŸ¥è¯çš„æœ¯è¯­åˆ—è¡¨
            
        Returns:
            æŸ¥è¯åçš„æœ¯è¯­åˆ—è¡¨
        """
        if not terms_list:
            return []
        
        # ğŸ”¥ æ„å»ºæ‰¹é‡ç¿»è¯‘çš„promptï¼ˆä½¿ç”¨ textarea æ ¼å¼ï¼‰
        terms_text = []
        for idx, term_info in enumerate(terms_list, 1):
            term = term_info.get("term")
            category = term_info.get("category", "unknown")
            context = term_info.get("context", "")[:50]  # åªå–å‰50å­—ç¬¦
            terms_text.append(f"{idx}. {term}")
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æœ¯è¯­ç¿»è¯‘ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸ºä»¥ä¸‹æœ¯è¯­æä¾›å‡†ç¡®çš„ä¸­æ–‡ç¿»è¯‘ã€‚

ã€ç¿»è¯‘è¦æ±‚ã€‘
1. æ ¹æ®æœ¯è¯­çš„ç±»å‹é€‰æ‹©åˆé€‚çš„ç¿»è¯‘ç­–ç•¥ï¼š
   - ä¸“æœ‰åè¯ï¼ˆäººåã€åœ°åï¼‰ï¼šéŸ³è¯‘ä¸ºä¸»
   - ç”Ÿç‰©/åŒ–å­¦æœ¯è¯­ï¼šä½¿ç”¨æ ‡å‡†å­¦æœ¯è¯‘å
   - æ™®é€šæœ¯è¯­ï¼šæ„è¯‘ï¼Œç¬¦åˆä¸­æ–‡ä¹ æƒ¯
2. ç¿»è¯‘å¿…é¡»å‡†ç¡®ã€è§„èŒƒï¼Œç¬¦åˆä¸“ä¸šé¢†åŸŸçš„æƒ¯ä¾‹
3. ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–æ³¨é‡Š

ã€è¾“å‡ºæ ¼å¼è¦æ±‚ã€‘
- å¿…é¡»ä½¿ç”¨<textarea>æ ‡ç­¾åŒ…è£¹æ‰€æœ‰è¯‘æ–‡
- é€è¡Œç¿»è¯‘ï¼ŒåŸæ–‡æœ‰{len(terms_list)}è¡Œï¼Œè¯‘æ–‡ä¹Ÿå¿…é¡»æœ‰{len(terms_list)}è¡Œ
- æ¯è¡Œæ ¼å¼ï¼šåºå·. è¯‘æ–‡
- åºå·å¿…é¡»ä»1åˆ°{len(terms_list)}è¿ç»­ï¼Œä¸è¦è·³è¿‡
- ä¸è¦åˆå¹¶è¡Œï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜

æ ¼å¼ç¤ºä¾‹ï¼š
<textarea>
1.ç¬¬ä¸€ä¸ªæœ¯è¯­çš„è¯‘æ–‡
2.ç¬¬äºŒä¸ªæœ¯è¯­çš„è¯‘æ–‡
3.ç¬¬ä¸‰ä¸ªæœ¯è¯­çš„è¯‘æ–‡
</textarea>"""
        
        # æ„å»ºç”¨æˆ·æ¶ˆæ¯
        user_content = "è¯·ä¸ºä»¥ä¸‹æœ¯è¯­æä¾›å‡†ç¡®çš„ä¸­æ–‡ç¿»è¯‘ï¼š\n\n<textarea>\n" + "\n".join(terms_text) + "\n</textarea>"
        
        messages = [{
            "role": "user",
            "content": user_content
        }]
        
        try:
            platform_config = self.config.get_platform_configuration("translationReq") if self.config else {}
            skip, _, response_content, prompt_tokens, completion_tokens = self._llm_request_with_tracking(
                messages=messages,
                system_prompt=system_prompt,
                platform_config=platform_config
            )
            
            # ğŸ”¥ æ›´æ–°tokenç»Ÿè®¡
            self._update_token_stats(prompt_tokens, completion_tokens)
            
            if not skip and response_content:
                # ğŸ”¥ ä½¿ç”¨ ResponseExtractor è§£æï¼ˆä¸ç¿»è¯‘agentç›¸åŒï¼‰
                source_text_dict = {str(i): term_info.get("term") for i, term_info in enumerate(terms_list)}
                translation_dict = self.response_extractor.extract_translation(source_text_dict, response_content)
                
                if translation_dict:
                    # å°†ç¿»è¯‘ç»“æœåˆå¹¶åˆ°æœ¯è¯­ä¿¡æ¯ä¸­
                    success_count = 0
                    for idx, term_info in enumerate(terms_list):
                        translation = translation_dict.get(str(idx), "")
                        
                        # æ¸…ç†åºå·å‰ç¼€ï¼ˆå¦‚ "1." æˆ– "1. "ï¼‰
                        translation = re.sub(r'^\d+\.\s*', '', translation).strip()
                        
                        # æ¸…ç†Markdownæ ‡è®°
                        translation = re.sub(r'\*\*(.+?)\*\*', r'\1', translation)
                        translation = re.sub(r'\*(.+?)\*', r'\1', translation)
                        translation = translation.strip('*_').strip()
                        
                        if translation:
                            term_info["translation_suggestions"] = [translation]
                            term_info["llm_verification"] = "æ‰¹é‡æŸ¥è¯å®Œæˆ"
                            success_count += 1
                    
                    self.info(f"âœ“ æ‰¹é‡æŸ¥è¯å®Œæˆï¼ŒæˆåŠŸå¤„ç† {success_count}/{len(terms_list)} ä¸ªæœ¯è¯­")
                    return terms_list
                else:
                    self.warning("ResponseExtractoræœªèƒ½è§£æå‡ºç¿»è¯‘ç»“æœ")
                    
        except Exception as e:
            self.error(f"æ‰¹é‡æœ¯è¯­æŸ¥è¯å¤±è´¥: {e}", e)
        
        # å¦‚æœæ‰¹é‡æŸ¥è¯å¤±è´¥ï¼Œè¿”å›åŸå§‹ä¿¡æ¯
        return terms_list
    
    def _enrich_term_with_llm(self, term_info: Dict) -> Dict:
        """
        âš ï¸ å·²å¼ƒç”¨ï¼šä½¿ç”¨LLMä¸°å¯Œå•ä¸ªæœ¯è¯­ä¿¡æ¯
        ç°åœ¨ä½¿ç”¨ _batch_enrich_terms_with_llm è¿›è¡Œæ‰¹é‡å¤„ç†
        ä¿ç•™æ­¤æ–¹æ³•ä»¥é˜²åç»­éœ€è¦å•ç‹¬æŸ¥è¯æŸäº›æœ¯è¯­
        """
        term = term_info.get("term")
        category = term_info.get("category", "unknown")
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æœ¯è¯­ç¿»è¯‘ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹æœ¯è¯­æä¾›ï¼š
1. å‡†ç¡®çš„ç¿»è¯‘å»ºè®®
2. ä½¿ç”¨åœºæ™¯è¯´æ˜
3. ç¿»è¯‘ç­–ç•¥å»ºè®®

æœ¯è¯­ï¼š{term}
ç±»åˆ«ï¼š{category}"""
        
        messages = [{
            "role": "user",
            "content": f"è¯·ä¸ºæœ¯è¯­'{term}'æä¾›ç¿»è¯‘å»ºè®®å’Œè¯´æ˜ã€‚"
        }]
        
        try:
            platform_config = self.config.get_platform_configuration("translationReq") if self.config else {}
            skip, _, response_content, prompt_tokens, completion_tokens = self._llm_request_with_tracking(
                messages=messages,
                system_prompt=system_prompt,
                platform_config=platform_config
            )
            
            # ğŸ”¥ æ›´æ–°tokenç»Ÿè®¡
            self._update_token_stats(prompt_tokens, completion_tokens)
            
            if not skip and response_content:
                # è§£æå“åº”ï¼Œæå–ç¿»è¯‘å»ºè®®
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥æ›´æ™ºèƒ½åœ°è§£æ
                term_info["llm_verification"] = response_content
                term_info["translation_suggestions"] = self._extract_translation_suggestions(response_content)
        except Exception as e:
            self.error(f"æœ¯è¯­æŸ¥è¯å¤±è´¥ {term}: {e}", e)
        
        return term_info
    
    def _extract_translation_suggestions(self, llm_response: str) -> List[str]:
        """
        ä»LLMå“åº”ä¸­æå–ç¿»è¯‘å»ºè®®
        âœ… æ¸…ç†æ‰€æœ‰Markdownæ ¼å¼æ ‡è®°ï¼ˆ**ï¼Œ__ï¼Œ*ï¼Œ_ç­‰ï¼‰
        """
        import re
        suggestions = []
        # ç®€å•çš„æå–é€»è¾‘
        lines = llm_response.split("\n")
        for line in lines:
            if "ç¿»è¯‘" in line or "è¯‘" in line:
                # å°è¯•æå–å¯èƒ½çš„ç¿»è¯‘
                parts = line.split("ï¼š") or line.split(":")
                if len(parts) > 1:
                    translation = parts[1].strip()
                    # âœ… æ¸…ç†Markdownæ ¼å¼æ ‡è®°
                    # ç§»é™¤ç²—ä½“ï¼š**text** æˆ– __text__
                    translation = re.sub(r'\*\*(.+?)\*\*', r'\1', translation)
                    translation = re.sub(r'__(.+?)__', r'\1', translation)
                    # ç§»é™¤æ–œä½“ï¼š*text* æˆ– _text_
                    translation = re.sub(r'\*(.+?)\*', r'\1', translation)
                    translation = re.sub(r'_(.+?)_', r'\1', translation)
                    # ç§»é™¤è¡Œé¦–è¡Œå°¾çš„å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
                    translation = translation.strip('*_').strip()
                    if translation:
                        suggestions.append(translation)
        return suggestions[:3]  # è¿”å›å‰3ä¸ªå»ºè®®
    
    def _build_terminology_database(self, verified_terminology: List[Dict]) -> None:
        """
        æ„å»ºé¡¹ç›®ä¸“å±æœ¯è¯­åº“ï¼ˆç»“æ„åŒ–èµ„æºï¼‰
        å¹¶å°†æœ¯è¯­åŒæ­¥åˆ° ElasticSearch (Phase 2)
        """
        self.log_agent_action("æ„å»ºé¡¹ç›®ä¸“å±æœ¯è¯­åº“å¹¶åŒæ­¥åˆ°DB")
        
        # 1. å†…å­˜æ›´æ–°
        for term_info in verified_terminology:
            term = term_info.get("term")
            if not term:
                continue
            
            # æ„å»ºæœ¯è¯­åº“æ¡ç›®
            self.terminology_db[term] = {
                "term": term,
                "type": term_info.get("type", "unknown"),
                "category": term_info.get("category", "unknown"),
                "translation": term_info.get("translation_suggestions", [""])[0] if term_info.get("translation_suggestions") else "",
                "context": term_info.get("context", ""),
                "meaning": term_info.get("meaning", ""),
                "translation_strategy": term_info.get("translation_strategy", "ç›´è¯‘"),
                "priority": term_info.get("priority", "medium"),
                "verified": True
            }

        # 2. æ•°æ®åº“åŒæ­¥ (ES) - å†™å…¥å®Œæ•´è¯æ±‡ä¿¡æ¯
        try:
            from ModuleFolders.Cache.DatabaseManager import DatabaseManager
            db_manager = DatabaseManager()
            
            # è·å–å½“å‰ work_id (é»˜è®¤ 0)
            work_id = getattr(self._current_cache_project, 'db_work_id', 0)
            
            # æ‰¹é‡å†™å…¥æœ¯è¯­åˆ° ESï¼ˆåŒ…å«å®Œæ•´è¯æ±‡ä¿¡æ¯ï¼‰
            for term, info in self.terminology_db.items():
                # ç¡®å®šè¯æ±‡ç±»å‹
                term_type = info.get("type", "term")
                word_type_map = {
                    "named_entity": "entity",
                    "terminology": "term",
                    "cultural_expression": "idiom",
                    "unknown": "term"
                }
                word_type = word_type_map.get(term_type, "term")
                
                # æ„å»ºå€™é€‰è¯‘æ³•åˆ—è¡¨
                translations_list = []
                main_translation = info.get("translation", "")
                if main_translation:
                    translations_list.append({
                        "translation": main_translation,
                        "source": "LLM",
                        "confidence": info.get("confidence", 1.0),
                        "rank": 1,
                        "rationale": info.get("translation_strategy", "")
                    })
                
                # æ”¶é›†ç›¸å…³åŸå­IDï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                atom_ids = []
                if hasattr(self._current_cache_project, 'db_atom_map'):
                    # éå†æ‰€æœ‰æ–‡ä»¶çš„ atom_mapï¼Œæ‰¾å‡ºåŒ…å«è¿™ä¸ªæœ¯è¯­çš„åŸå­
                    for file_path, atom_map in self._current_cache_project.db_atom_map.items():
                        for row_idx, a_id in atom_map.items():
                            atom_ids.append(a_id)
                    # é™åˆ¶æ•°é‡é¿å…è¿‡å¤§
                    atom_ids = atom_ids[:10] if len(atom_ids) > 10 else atom_ids
                
                db_manager.upsert_term(
                    entry_key=term,
                    entry_val=main_translation,
                    work_id=work_id,
                    word_type=word_type,
                    domain=self.memory_storage.get("domain", "general"),
                    variants=[],  # TODO: è¿˜æ²¡æå–å˜ä½“
                    example_sentences=[info.get("context", "")] if info.get("context") else [],
                    translations=translations_list,
                    atom_ids=atom_ids,
                    confidence=info.get("confidence", 1.0),
                    agent_notes=f"ç±»å‹: {info.get('category', '')}, å«ä¹‰: {info.get('meaning', '')}",
                    is_confirmed=info.get("verified", False)
                )
            
            self.info(f"[DB] æœ¯è¯­å·²åŒæ­¥åˆ° ElasticSearch: {len(self.terminology_db)} ä¸ªæ¡ç›® (Project ID: {work_id})")
            
        except Exception as e:
            self.error(f"[DB] æœ¯è¯­åº“åŒæ­¥å¤±è´¥: {e}")
    
    def _update_memory(self, cache_project: CacheProject, metadata: Dict) -> None:
        """
        æ›´æ–°Memoryå­˜å‚¨
        å­˜å‚¨å·²ç¿»è¯‘æ–‡æœ¬ã€æ‘˜è¦ã€è¯»è€…å€¾å‘ã€ç¿»è¯‘é£æ ¼æŒ‡å—ç­‰
        """
        self.log_agent_action("æ›´æ–°Memoryå­˜å‚¨")
        
        # å­˜å‚¨å…ƒæ•°æ®
        self.memory_storage["domain"] = metadata.get("domain", "general")
        self.memory_storage["style"] = metadata.get("style", "neutral")
        
        # å­˜å‚¨å·²ç¿»è¯‘æ–‡æœ¬æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
        translated_texts = []
        for cache_file in cache_project.files.values():
            for item in cache_file.items:
                if hasattr(item, 'translated_text') and item.translated_text:
                    translated_texts.append({
                        "source": item.source_text[:100],  # åªå­˜å‚¨å‰100å­—ç¬¦
                        "translated": item.translated_text[:100]
                    })
        
        if translated_texts:
            self.memory_storage["translated_texts"] = translated_texts[-50:]  # åªä¿ç•™æœ€è¿‘50æ¡
    
    def get_terminology_prompt(self) -> str:
        """
        ç”Ÿæˆæœ¯è¯­è¡¨æç¤ºè¯ï¼Œç”¨äºå¼ºåˆ¶æ¨¡å‹ä½¿ç”¨è§„èŒƒè¯‘æ³•
        """
        if not self.terminology_db:
            return ""
        
        prompt = "\n\nã€æœ¯è¯­è¡¨ã€‘è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æœ¯è¯­è¡¨è¿›è¡Œç¿»è¯‘ï¼Œç¡®ä¿å…¨æ–‡ä¸€è‡´æ€§ï¼š\n"
        for term, info in self.terminology_db.items():
            translation = info.get("translation", "")
            if translation:
                prompt += f"- {term} â†’ {translation}\n"
        
        return prompt
    
    def get_memory_context(self) -> str:
        """
        è·å–Memoryä¸Šä¸‹æ–‡ï¼Œç”¨äºåŠ¨æ€åŠ è½½åˆ°promptä¸­
        """
        context_parts = []
        
        # é¢†åŸŸå’Œé£æ ¼ä¿¡æ¯
        domain = self.memory_storage.get("domain", "general")
        style = self.memory_storage.get("style", "neutral")
        if domain != "general":
            context_parts.append(f"æ–‡æœ¬é¢†åŸŸï¼š{domain}")
        if style != "neutral":
            context_parts.append(f"æ–‡æœ¬é£æ ¼ï¼š{style}")
        
        # ç¿»è¯‘é£æ ¼æŒ‡å—
        style_guide = self.memory_storage.get("translation_style_guide", {})
        if style_guide:
            context_parts.append(f"ç¿»è¯‘é£æ ¼æŒ‡å—ï¼š{json.dumps(style_guide, ensure_ascii=False)}")
        
        return "\n".join(context_parts) if context_parts else ""
